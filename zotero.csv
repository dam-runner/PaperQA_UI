Key,Item Type,Publication Year,Author,Title,Publication Title,ISBN,ISSN,DOI,Url,Abstract Note,Date,Date Added,Date Modified,Access Date,Pages,Issue,Volume,Journal Abbreviation,Publisher,Language,Rights,Library Catalog,Extra,Notes,File Attachments,Link Attachments,Manual Tags,Automatic Tags
PRE9LEDD,preprint,2023,"Nguyen, Khai-Nguyen; Tang, Zixin; Mali, Ankur; Kelly, Alex",Like a bilingual baby: The advantage of visually grounding a bilingual language model,,,,,http://arxiv.org/abs/2210.05487,"Unlike most neural language models, humans learn language in a rich, multi-sensory and, often, multi-lingual environment. Current language models typically fail to fully capture the complexities of multilingual language use. We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES. We find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity. However, we find no significant advantage of visual grounding for abstract words. Our results provide additional evidence of the advantages of visually grounded language models and point to the need for more naturalistic language data from multilingual speakers and multilingual datasets with perceptual grounding.",2/13/2023,4/11/2024 23:01,5/29/2025 10:01,4/11/2024 23:01,,,,,arXiv,,,arXiv.org,arXiv:2210.05487 [cs],,C:\Users\damru\Zotero\storage\BZZRRDKL\Nguyen et al. - 2023 - Like a bilingual baby The advantage of visually g.pdf; ; C:\Users\damru\Zotero\storage\HMFUV32P\2210.html; ,https://arxiv.org/pdf/2210.05487.pdf; https://arxiv.org/abs/2210.05487,,Computer Science - Computation and Language
3RX9MMPJ,preprint,2022,"Lin, Xi Victoria; Mihaylov, Todor; Artetxe, Mikel; Wang, Tianlu; Chen, Shuohui; Simig, Daniel; Ott, Myle; Goyal, Naman; Bhosale, Shruti; Du, Jingfei; Pasunuru, Ramakanth; Shleifer, Sam; Koura, Punit Singh; Chaudhary, Vishrav; O'Horo, Brian; Wang, Jeff; Zettlemoyer, Luke; Kozareva, Zornitsa; Diab, Mona; Stoyanov, Veselin; Li, Xian",Few-shot Learning with Multilingual Language Models,,,,10.48550/arXiv.2112.10668,http://arxiv.org/abs/2112.10668,"Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.",11/10/2022,4/11/2024 22:51,5/29/2025 10:02,4/11/2024 22:51,,,,,arXiv,,,arXiv.org,arXiv:2112.10668 [cs],,C:\Users\damru\Zotero\storage\3Q5X2Q4W\Lin et al. - 2022 - Few-shot Learning with Multilingual Language Model.pdf; ; C:\Users\damru\Zotero\storage\PN3B5K8N\2112.html; ,https://arxiv.org/pdf/2112.10668.pdf; https://arxiv.org/abs/2112.10668,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language
2C9R4JYJ,journalArticle,2023,"Kwon, Yoohwan; Chung, Soo-Whan",MoLE : Mixture Of Language Experts For Multi-Lingual Automatic Speech Recognition,"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,,10.1109/ICASSP49357.2023.10096227,https://ieeexplore.ieee.org/document/10096227/,"Multi-lingual speech recognition aims to distinguish linguistic expressions in different languages and integrate acoustic processing simultaneously. In contrast, current multilingual speech recognition research follows a language-aware paradigm, mainly targeted to improve recognition performance rather than discriminate language characteristics. In this paper, we present a multi-lingual speech recognition network named Mixture-of-Language-Experts (MoLE), which digests speech in a variety of languages. Specifically, MoLE analyzes linguistic expression from input speech in arbitrary languages, activating a language-specific expert with a lightweight language gating network. The gating network not only activates experts, but also estimates the reliability of the activation. Based on the reliability, the activated expert and the language-agnostic expert are aggregated to represent language-conditioned embedding for efficient speech recognition. Our proposed model is evaluated in 5 languages scenario, and the experimental results show that our structure is advantageous on multi-lingual recognition, especially for speech in low-resource language.",6/4/2023,4/10/2024 16:07,5/29/2025 10:02,4/10/2024 16:07,5-Jan,,,,,,https://doi.org/10.15223/policy-029,Semantic Scholar,"Conference Name: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ISBN: 9781728163277 Place: Rhodes Island, Greece Publisher: IEEE",,; C:\Users\damru\Zotero\storage\9FJ3QM6Q\Kwon and Chung - 2023 - MoLE  Mixture Of Language Experts For Multi-Lingu.pdf; ,https://www.semanticscholar.org/paper/MoLE-%3A-Mixture-Of-Language-Experts-For-Automatic-Kwon-Chung/250373008e4441678006a6ba7dbb186449c7169f; https://arxiv.org/pdf/2302.13750,,
LQVN72SQ,bookSection,2020,"Gretter, Roberto; Matassoni, Marco; Falavigna, Giuseppe Daniele; Keelan, Evanini; Leong, Chee Wee",Overview of the interspeech tlt2020 shared task onasr for non-native children’s speech,Proceedings of Interspeech 2020,,,,https://cris.fbk.eu/handle/11582/324906,,2020,4/9/2024 4:39,5/29/2025 11:50,4/9/2024 4:39,245–249,,,,,,,Google Scholar,,,C:\Users\damru\Zotero\storage\LJ5M7A2T\Gretter et al. - 2020 - Overview of the interspeech tlt2020 shared task on.pdf; ,https://cris.fbk.eu/bitstream/11582/324906/1/is2020_challenge.pdf,,
9HZDXE5J,conferencePaper,2024,"Rolland, Thomas; Abad, Alberto",Improved Children’s Automatic Speech Recognition Combining Adapters and Synthetic Data Augmentation,"ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,,,https://ieeexplore.ieee.org/abstract/document/10446889/,,2024,4/9/2024 4:39,5/29/2025 12:58,4/9/2024 4:39,12757–12761,,,,IEEE,,,Google Scholar,,,,,,
ZL52Q6PI,journalArticle,2022,"Cowan, Tiana; Paroby, Caroline; Leibold, Lori J.; Buss, Emily; Rodriguez, Barbara; Calandruccio, Lauren",Masked-Speech Recognition for Linguistically Diverse Populations: A Focused Review and Suggestions for the Future,"Journal of Speech, Language, and Hearing Research",,"1092-4388, 1558-9102",10.1044/2022_JSLHR-22-00011,http://pubs.asha.org/doi/10.1044/2022_JSLHR-22-00011,"Purpose:               Twenty years ago, von Hapsburg and Peña (2002) wrote a tutorial that reviewed the literature on speech audiometry and bilingualism and outlined valuable recommendations to increase the rigor of the evidence base. This review article returns to that seminal tutorial to reflect on how that advice was applied over the last 20 years and to provide updated recommendations for future inquiry.                                         Method:               We conducted a focused review of the literature on masked-speech recognition for bilingual children and adults. First, we evaluated how studies published since 2002 described bilingual participants. Second, we reviewed the literature on native language masked-speech recognition. Third, we discussed theoretically motivated experimental work. Fourth, we outlined how recent research in bilingual speech recognition can be used to improve clinical practice.                                         Results:               Research conducted since 2002 commonly describes bilingual samples in terms of their language status, competency, and history. Bilingualism was not consistently associated with poor masked-speech recognition. For example, bilinguals who were exposed to English prior to age 7 years and who were dominant in English performed comparably to monolinguals for masked-sentence recognition tasks. To the best of our knowledge, there are no data to document the masked-speech recognition ability of these bilinguals in their other language compared to a second monolingual group, which is an important next step. Nonetheless, individual factors that commonly vary within bilingual populations were associated with masked-speech recognition and included language dominance, competency, and age of acquisition. We identified methodological issues in sampling strategies that could, in part, be responsible for inconsistent findings between studies. For instance, disparities in socioeconomic status (SES) between recruited bilingual and monolingual groups could cause confounding bias within the research design.                                         Conclusions:               Dimensions of the bilingual linguistic profile should be considered in clinical practice to inform counseling and (re)habilitation strategies since susceptibility to masking is elevated in at least one language for most bilinguals. Future research should continue to report language status, competency, and history but should also report language stability and demand for use data. In addition, potential confounds (e.g., SES, educational attainment) when making group comparisons between monolinguals and bilinguals must be considered.",8/17/2022,4/9/2024 4:39,5/29/2025 11:52,4/9/2024 4:39,3195-3216,8,65,J Speech Lang Hear Res,,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\2FXLTBTJ\PMC9911100.html; ; C:\Users\damru\Zotero\storage\AYEQLKJT\Cowan et al. - 2022 - Masked-Speech Recognition for Linguistically Diver.pdf; ,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9911100/; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9911100/pdf/JSLHR-65-3195.pdf,,
9MLEL7L4,conferencePaper,2020,"Bhardwaj, Vivek; Bala, Sashi; Kadyan, Virender; Kukreja, Vinay",Development of robust automatic speech recognition system for children's using kaldi toolkit,2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA),,,,https://ieeexplore.ieee.org/abstract/document/9182941/,,2020,4/9/2024 4:37,5/29/2025 12:59,4/9/2024 4:37,10–13,,,,IEEE,,,Google Scholar,,,C:\Users\damru\Zotero\storage\6UAVEQS7\Bhardwaj et al. - 2020 - Development of robust automatic speech recognition.pdf; ,https://www.researchgate.net/profile/Vivek-Bhardwaj-4/publication/344056935_Development_of_Robust_Automatic_Speech_Recognition_System_for_Children%27s_using_Kaldi_Toolkit/links/5fc4f5f7458515b7978abb2f/Development-of-Robust-Automatic-Speech-Recognition-System-for-Childrens-using-Kaldi-Toolkit.pdf,,
5RSK48Y3,conferencePaper,2021,"Gretter, Roberto; Matassoni, Marco; Falavigna, Daniele; Misra, A.; Leong, Chee Wee; Knill, Katherine; Wang, Linlin",Etlt 2021: Shared task on automatic speech recognition for non-native children’s speech,,,,,https://www.repository.cam.ac.uk/items/ad96db23-a702-4568-88ae-972fc5dc1af9,,2021,4/9/2024 4:37,5/29/2025 11:50,4/9/2024 4:37,,,,,ISCA,,,Google Scholar,,,C:\Users\damru\Zotero\storage\AMM4MMCJ\Gretter et al. - 2021 - Etlt 2021 Shared task on automatic speech recogni.pdf; ,https://www.repository.cam.ac.uk/bitstreams/10893a3e-0b58-4597-81bd-4552e190c98d/download,,
9S9WFXRS,conferencePaper,2021,"Li, Xinjian; Mortensen, David R.; Metze, Florian; Black, Alan W.",Multilingual phonetic dataset for low resource speech recognition,"ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,,,https://ieeexplore.ieee.org/abstract/document/9413720/,,2021,4/9/2024 4:35,5/29/2025 10:02,4/9/2024 4:35,6958–6962,,,,IEEE,,,Google Scholar,,,C:\Users\damru\Zotero\storage\N9DLCDJY\Li et al. - 2021 - Multilingual phonetic dataset for low resource spe.pdf; ,https://www.cs.cmu.edu/~awb/papers/ICASSP21_Multilingual_Phonetic_Dataset.pdf,,
SGX72PWR,preprint,2020,"Madhumani, Gurunath Reddy; Shah, Sanket; Abraham, Basil; Joshi, Vikas; Sitaram, Sunayana",Learning not to Discriminate: Task Agnostic Learning for Improving Monolingual and Code-switched Speech Recognition,,,,,http://arxiv.org/abs/2006.05257,"Recognizing code-switched speech is challenging for Automatic Speech Recognition (ASR) for a variety of reasons, including the lack of code-switched training data. Recently, we showed that monolingual ASR systems fine-tuned on code-switched data deteriorate in performance on monolingual speech recognition, which is not desirable as ASR systems deployed in multilingual scenarios should recognize both monolingual and code-switched speech with high accuracy. Our experiments indicated that this loss in performance could be mitigated by using certain strategies for fine-tuning and regularization, leading to improvements in both monolingual and code-switched ASR. In this work, we present further improvements over our previous work by using domain adversarial learning to train task agnostic models. We evaluate the classification accuracy of an adversarial discriminator and show that it can learn shared layer parameters that are task agnostic. We train end-to-end ASR systems starting with a pooled model that uses monolingual and code-switched data along with the adversarial discriminator. Our proposed technique leads to reductions in Word Error Rates (WER) in monolingual and code-switched test sets across three language pairs.",6/9/2020,4/9/2024 4:34,4/9/2024 4:34,4/9/2024 4:33,,,,,arXiv,,,arXiv.org,"arXiv:2006.05257 [cs, eess]",,C:\Users\damru\Zotero\storage\KF7FDKZJ\Madhumani et al. - 2020 - Learning not to Discriminate Task Agnostic Learni.pdf; C:\Users\damru\Zotero\storage\WEKSNXD7\2006.html,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
54NBFCU9,preprint,2021,"Chan, William; Park, Daniel; Lee, Chris; Zhang, Yu; Le, Quoc; Norouzi, Mohammad",SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network,,,,,http://arxiv.org/abs/2104.02133,"We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\% WER on AMI-IHM, 4.7\% WER on Switchboard, 8.3\% WER on CallHome, and 1.3\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\% WER without a language model, which compares to 38.6\% WER to a strong HMM baseline with a language model.",4/27/2021,4/9/2024 4:32,5/29/2025 12:59,4/9/2024 4:32,,,,,arXiv,,,arXiv.org,arXiv:2104.02133 [cs],,C:\Users\damru\Zotero\storage\UQXHYW9T\Chan et al. - 2021 - SpeechStew Simply Mix All Available Speech Recogn.pdf; ; C:\Users\damru\Zotero\storage\T6JX8H2Q\2104.html; ,https://arxiv.org/pdf/2104.02133.pdf; https://arxiv.org/abs/2104.02133,,Computer Science - Computation and Language; Computer Science - Machine Learning
MXMZH555,preprint,2021,"Babu, Arun; Wang, Changhan; Tjandra, Andros; Lakhotia, Kushal; Xu, Qiantong; Goyal, Naman; Singh, Kritika; von Platen, Patrick; Saraf, Yatharth; Pino, Juan; Baevski, Alexei; Conneau, Alexis; Auli, Michael",XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,,,,,http://arxiv.org/abs/2111.09296,"This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.",12/16/2021,4/9/2024 4:32,5/29/2025 12:59,4/9/2024 4:32,,,,,arXiv,,,arXiv.org,"arXiv:2111.09296 [cs, eess]",,C:\Users\damru\Zotero\storage\V3T5YDZ5\Babu et al. - 2021 - XLS-R Self-supervised Cross-lingual Speech Repres.pdf; ; C:\Users\damru\Zotero\storage\RIURI7F3\2111.html; ,https://arxiv.org/pdf/2111.09296.pdf; https://arxiv.org/abs/2111.09296,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
MMCTDYWI,preprint,2021,"Chen, Guoguo; Chai, Shuzhou; Wang, Guanbo; Du, Jiayu; Zhang, Wei-Qiang; Weng, Chao; Su, Dan; Povey, Daniel; Trmal, Jan; Zhang, Junbo; Jin, Mingjie; Khudanpur, Sanjeev; Watanabe, Shinji; Zhao, Shuaijiang; Zou, Wei; Li, Xiangang; Yao, Xuchen; Wang, Yongqing; Wang, Yujun; You, Zhao; Yan, Zhiyong","GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio",,,,,http://arxiv.org/abs/2106.06909,"This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.",6/13/2021,4/9/2024 4:32,5/29/2025 12:59,4/9/2024 4:32,,,,,arXiv,,,arXiv.org,"arXiv:2106.06909 [cs, eess]",,"C:\Users\damru\Zotero\storage\CGPTVHFJ\Chen et al. - 2021 - GigaSpeech An Evolving, Multi-domain ASR Corpus w.pdf; C:\Users\damru\Zotero\storage\34WHY7SW\2106.html",,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
GCD5JL79,conferencePaper,2020,"Shahnawazuddin, S.; Adiga, Nagaraj; Kumar, Kunal; Poddar, Aayushi; Ahmad, Waquar",Voice Conversion Based Data Augmentation to Improve Children’s Speech Recognition in Limited Data Scenario,Interspeech 2020,,,10.21437/Interspeech.2020-1112,https://www.isca-archive.org/interspeech_2020/shahnawazuddin20_interspeech.html,"Automatic recognition of children’s speech is a challenging research problem due to several reasons. One among those is unavailability of large amounts of speech data from child speakers to develop automatic speech recognition (ASR) systems employing deep learning architectures.Using a limited amount of training data limits the power of the learned system. To overcome this issue, we have explored means to effectively make use of adults’ speech data for training an ASR system. For that purpose, generative adversarial network (GAN) based voice conversion (VC) is exploited to modify the acoustic attributes of adults’ speech making it perceptually similar to that of children’s speech. The original and converted speech samples from adult speakers are then pooled together to learn the statistical model parameters. Signiﬁcantly improved recognition rate for children’s speech is noted due to VC-based data augmentation. To further enhance the recognition rate, a limited amount of children’s speech data is also pooled into training. Large reduction in error rate is observed in this case as well. It is worth mentioning that GAN-based VC does not change the speakingrate. To demonstrate the need to deal with speaking-rate differences we report the results of time-scale modiﬁcation of childrens speech test data.",10/25/2020,4/9/2024 4:09,5/29/2025 12:58,4/9/2024 4:09,4382-4386,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\QT9LT739\Shahnawazuddin et al. - 2020 - Voice Conversion Based Data Augmentation to Improv.pdf; ,http://www.interspeech2020.org/uploadfile/pdf/Thu-2-8-10.pdf,,
NC2DBRI2,journalArticle,2020,"Gurunath Shivakumar, Prashanth; Georgiou, Panayiotis","Transfer learning from adult to children for speech recognition: Evaluation, analysis and recommendations",Computer Speech & Language,,0885-2308,10.1016/j.csl.2020.101077,https://www.sciencedirect.com/science/article/pii/S0885230820300103,"Children speech recognition is challenging mainly due to the inherent high variability in children’s physical and articulatory characteristics and expressions. This variability manifests in both acoustic constructs and linguistic usage due to the rapidly changing developmental stage in children’s life. Part of the challenge is due to the lack of large amounts of available children speech data for efficient modeling. This work attempts to address the key challenges using transfer learning from adult’s models to children’s models in a Deep Neural Network (DNN) framework for children’s Automatic Speech Recognition (ASR) task evaluating on multiple children’s speech corpora with a large vocabulary. The paper presents a systematic and an extensive analysis of the proposed transfer learning technique considering the key factors affecting children’s speech recognition from prior literature. Evaluations are presented on (i) comparisons of earlier GMM-HMM and the newer DNN Models, (ii) effectiveness of standard adaptation techniques versus transfer learning, (iii) various adaptation configurations in tackling the variabilities present in children speech, in terms of (a) acoustic spectral variability, and (b) pronunciation variability and linguistic constraints. Our Analysis spans over (i) number of DNN model parameters (for adaptation), (ii) amount of adaptation data, (iii) ages of children, (iv) age dependent-independent adaptation. Finally, we provide Recommendations on (i) the favorable strategies over various aforementioned - analyzed parameters, and (ii) potential future research directions and relevant challenges/problems persisting in DNN based ASR for children’s speech.",9/1/2020,4/9/2024 4:07,5/29/2025 11:50,4/9/2024 4:07,101077,,63,Computer Speech & Language,,,,ScienceDirect,,,C:\Users\damru\Zotero\storage\GCLYNNX2\Gurunath Shivakumar and Georgiou - 2020 - Transfer learning from adult to children for speec.pdf; ; C:\Users\damru\Zotero\storage\6A7WKV43\S0885230820300103.html,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7199459/,,Analysis of children’s speech; Automatic speech recognition; Children speech recognition; Deep learning; Deep neural network; Transfer learning
L7P2M535,journalArticle,2022,"Gurunath Shivakumar, Prashanth; Narayanan, Shrikanth",End-to-end neural systems for automatic children speech recognition: An empirical study,Computer Speech & Language,,0885-2308,10.1016/j.csl.2021.101289,https://www.sciencedirect.com/science/article/pii/S0885230821000905,"A key desiderata for inclusive and accessible speech recognition technology is ensuring its robust performance to children’s speech. Notably, this includes the rapidly advancing neural network based end-to-end speech recognition systems. Children speech recognition is more challenging due to the larger intra-inter speaker variability in terms of acoustic and linguistic characteristics compared to adult speech. Furthermore, the lack of adequate and appropriate children speech resources adds to the challenge of designing robust end-to-end neural architectures. This study provides a critical assessment of automatic children speech recognition through an empirical study of contemporary state-of-the-art end-to-end speech recognition systems. Insights are provided on the aspects of training data requirements, adaptation on children data, and the effect of children age, utterance lengths, different architectures and loss functions for end-to-end systems and role of language models on the speech recognition performance.",3/1/2022,4/9/2024 4:05,5/29/2025 12:59,4/9/2024 4:05,101289,,72,Computer Speech & Language,,,,ScienceDirect,,,C:\Users\damru\Zotero\storage\XYKQ3HDF\S0885230821000905.html; ; C:\Users\damru\Zotero\storage\6YFGUFUF\Gurunath Shivakumar and Narayanan - 2022 - End-to-end neural systems for automatic children s.pdf; ,https://www.sciencedirect.com/science/article/abs/pii/S0885230821000905; https://arxiv.org/pdf/2102.09918,,Children speech recognition; End-to-end speech recognition; Residual network; Time depth separable convolutional network; Transformer
HWSFWXYL,webpage,,,[PDF] Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children's Speech | Semantic Scholar,,,,,https://www.semanticscholar.org/paper/Spectral-Modification-Based-Data-Augmentation-For-Singh-Sailor/c33de1a3b7a7e55b7d6c7dc6d543ac7cf33c3be8,,,7/19/2024 22:13,5/29/2025 13:00,7/19/2024 22:13,,,,,,,,,,,,,,
4SX337UI,journalArticle,2023,"Barcovschi, Andrei; Jain, Rishabh; Corcoran, Peter","A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition",2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD),,,10.1109/SpeD59241.2023.10314867,https://ieeexplore.ieee.org/document/10314867/,"Automatic Speech Recognition (ASR) systems have progressed significantly in their performance on adult speech data; however, transcribing child speech remains challenging due to the acoustic differences in the characteristics of child and adult voices. This work aims to explore the potential of adapting state-of-the-art Conformer-transducer models to child speech to improve child speech recognition performance. Furthermore, the results are compared with those of self-supervised wav2vec2 models and semi-supervised multi-domain Whisper models that were previously finetuned on the same data. We demonstrate that finetuning Conformer-transducer models on child speech yields significant improvements in ASR performance on child speech, compared to the non-finetuned models. We also show Whisper and wav2vec2 adaptation on different child speech datasets. Our detailed comparative analysis shows that wav2vec2 provides the most consistent performance improvements among the three methods studied.",10/25/2023,7/19/2024 22:12,5/29/2025 12:59,7/19/2024 22:12,42-47,,,,,,https://doi.org/10.15223/policy-029,Semantic Scholar,"Conference Name: 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD) ISBN: 9798350327977 Place: Bucharest, Romania Publisher: IEEE",,; C:\Users\damru\Zotero\storage\4MM5YGB2\Barcovschi et al. - 2023 - A comparative analysis between Conformer-Transduce.pdf,https://www.semanticscholar.org/paper/A-comparative-analysis-between-Whisper%2C-and-for-the-Barcovschi-Jain/949bef66273661eab439b6bd9b9a4015de9a5f4d,,
WXU7XHH3,conferencePaper,2024,"Liu, Dancheng; Xiong, Jinjun",FASA: a Flexible and Automatic Speech Aligner for Extracting High-quality Aligned Children Speech Data,,,,,https://www.semanticscholar.org/paper/FASA%3A-a-Flexible-and-Automatic-Speech-Aligner-for-Liu-Xiong/62a721694bbb2c9de1899374bb68ae7ecb3ef419,"Automatic Speech Recognition (ASR) for adults' speeches has made significant progress by employing deep neural network (DNN) models recently, but improvement in children's speech is still unsatisfactory due to children's speech's distinct characteristics. DNN models pre-trained on adult data often struggle in generalizing children's speeches with fine tuning because of the lack of high-quality aligned children's speeches. When generating datasets, human annotations are not scalable, and existing forced-alignment tools are not usable as they make impractical assumptions about the quality of the input transcriptions. To address these challenges, we propose a new forced-alignment tool, FASA, as a flexible and automatic speech aligner to extract high-quality aligned children's speech data from many of the existing noisy children's speech data. We demonstrate its usage on the CHILDES dataset and show that FASA can improve data quality by 13.6$\times$ over human annotations.",6/25/2024,7/19/2024 22:11,5/29/2025 10:02,7/19/2024 22:11,,,,,,,,Semantic Scholar,,,C:\Users\damru\Zotero\storage\GI3T3U88\Liu and Xiong - 2024 - FASA a Flexible and Automatic Speech Aligner for .pdf; ; ,https://arxiv.org/pdf/2406.17926.pdf; https://www.semanticscholar.org/paper/FASA%3A-a-Flexible-and-Automatic-Speech-Aligner-for-Liu-Xiong/62a721694bbb2c9de1899374bb68ae7ecb3ef419,,
SZZAQAJV,journalArticle,2024,"Rolland, Thomas; Abad, Alberto",Exploring Adapters with Conformers for Children’s Automatic Speech Recognition,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,,10.1109/ICASSP48485.2024.10447091,https://ieeexplore.ieee.org/document/10447091/,"The high variability in acoustic, pronunciation, and linguistic characteristics of children’s speech makes of children’s automatic speech recognition (ASR) a complex task. Training a dedicated ASR model from scratch for children remains challenging, mainly due to the limited availability of children’s data. To tackle this limitation, a common strategy involves fine-tuning a pre-trained ASR model. However, this approach faces challenges due to the diversity of speakers and data scarcity, especially when dealing with large ASR models like the Conformer. In this study, we explore an alternative approach known as Adapter transfer. Adapter transfer requires training fewer parameters and can be more effective in adapting large ASR models for children’s speech. In this paper, we assess various Adapter configurations in the literature and introduce a novel configuration called Two Serial Adapter (TSA). The experimental results indicate that Adapter transfer consistently outperforms traditional fine-tuning across various configurations for the Conformer model.",4/14/2024,7/19/2024 22:11,5/29/2025 12:58,7/19/2024 22:11,12747-12751,,,,,,https://doi.org/10.15223/policy-029,Semantic Scholar,"Conference Name: ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) ISBN: 9798350344851 Place: Seoul, Korea, Republic of Publisher: IEEE",,,https://www.semanticscholar.org/paper/Exploring-Adapters-with-Conformers-for-Children%E2%80%99s-Rolland-Abad/d21661500948db9df426089508de1faeb15ef43c,,
ILDBXNK5,journalArticle,2024,"Jain, Rishabh; Barcovschi, Andrei; Yiwere, Mariam Yahayah; Corcoran, Peter; Cucu, Horia",Exploring Native and Non-Native English Child Speech Recognition With Whisper,IEEE Access,,2169-3536,10.1109/ACCESS.2024.3378738,https://ieeexplore.ieee.org/document/10474352/,"Modern end-to-end Automatic Speech Recognition (ASR) systems struggle to recognise children’s speech. This challenge is due to the high acoustic variability in children’s voices and the scarcity of child speech training data, particularly for accented or low-resource languages. This study focuses on improving the performance of ASR on native and non-native English child speech using publicly available datasets. We evaluate how the large-scale whisper models (trained with a large amount of adult speech data) perform with child speech. In addition, we perform finetuning experiments using different child speech datasets to investigate the performance of whisper ASR on non-native English-speaking children’s speech. Our findings indicate relative Word Error Rate (WER) improvements ranging from 29% to 89% over previous benchmarks on the same datasets. Notably, these gains were achieved by finetuning with only a 10% sample of unseen non-native datasets. These results demonstrate the potential of whisper for improving ASR in a low-resource scenario for non-native child speech.",2024,4/9/2024 4:37,5/29/2025 10:03,7/19/2024 22:11,41601-41610,,12,IEEE Access,,,https://creativecommons.org/licenses/by/4.0/legalcode,Semantic Scholar,,,C:\Users\damru\Zotero\storage\JY5VKIS8\Jain et al. - 2024 - Exploring Native and Non-Native English Child Spee.pdf,,,Data models; Training; Automatic speech recognition; Speech recognition; Adaptation models; Child automatic speech recognition; CMU_Kids; Decoding; large-scale supervision; MyST; non-native child speech; Pediatrics; PFSTAR; speechocean762; Testing; Transformers; whisper
XBY4IVWP,journalArticle,,,[PDF] Reducing Multilingual Context Confusion for End-to-end Code-switching Automatic Speech Recognition | Semantic Scholar,,,,,https://www.semanticscholar.org/paper/Reducing-Multilingual-Context-Confusion-for-Speech-Zhang-Yi/92ede035f77eead25c3ec8c84565cb98efb4ef99,"This work proposes a language-related attention mechanism to reduce multilingual context confusion for the E2E code-switching ASR model based on the Equivalence Constraint (EC) Theory, and efficiently transfers language knowledge from rich monolingual data to improve the performance of the code- Switched AsR model. Code-switching deals with alternative languages in communication process. Training end-to-end (E2E) automatic speech recognition (ASR) systems for code-switching is especially challenging as code-switching training data are always insufficient to combat the increased multilingual context confusion due to the presence of more than one language. We propose a language-related attention mechanism to reduce multilingual context confusion for the E2E code-switching ASR model based on the Equivalence Constraint (EC) Theory. The linguistic theory requires that any monolingual fragment that occurs in the code-switching sentence must occur in one of the monolingual sentences. The theory establishes a bridge between monolingual data and code-switching data. We leverage this linguistics theory to design the code-switching E2E ASR model. The proposed model efficiently transfers language knowledge from rich monolingual data to improve the performance of the code-switching ASR model. We evaluate our model on ASRU 2019 Mandarin-English code-switching challenge dataset. Compared to the baseline model, our proposed model achieves a 17.12% relative error reduction.",,7/19/2024 22:09,5/29/2025 12:58,7/19/2024 22:09,,,,,,en,,www.semanticscholar.org,,,,,,
DS83MICB,conferencePaper,2020,"Booth, Eric; Carns, Jake; Kennington, Casey; Rafla, Nader",Evaluating and improving child-directed automatic speech recognition,Proceedings of the Twelfth Language Resources and Evaluation Conference,,,,https://aclanthology.org/2020.lrec-1.778/,,2020,7/19/2024 18:48,5/29/2025 12:59,7/19/2024 18:48,6340–6345,,,,,,,Google Scholar,,,C:\Users\damru\Zotero\storage\9ASKWPLM\Booth et al. - 2020 - Evaluating and improving child-directed automatic .pdf; ,https://aclanthology.org/2020.lrec-1.778.pdf,,
UN656EGC,conferencePaper,2009,"Gerosa, Matteo; Giuliani, Diego; Narayanan, Shrikanth; Potamianos, Alexandros",A review of ASR technologies for children's speech,"Proceedings of the 2nd Workshop on Child, Computer and Interaction",978-1-60558-690-8,,10.1145/1640377.1640384,https://dl.acm.org/doi/10.1145/1640377.1640384,,11/5/2009,7/19/2024 18:48,5/29/2025 11:50,7/19/2024 18:48,8-Jan,,,,ACM,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\UY9EMQXY\Gerosa et al. - 2009 - A review of ASR technologies for children's speech.pdf; ,https://sail.usc.edu/publications/files/gerosawocci2009.pdf,,
BEHRDBGX,journalArticle,2017,"Serizel, Romain; Giuliani, Diego",Deep-neural network approaches for speech recognition with heterogeneous groups of speakers including children,Natural Language Engineering,,,,https://www.cambridge.org/core/journals/natural-language-engineering/article/deepneural-network-approaches-for-speech-recognition-with-heterogeneous-groups-of-speakers-including-children/49315B363FEC168B838B820DB6D6AC9F,,2017,7/19/2024 18:48,5/29/2025 12:58,7/19/2024 18:48,325–350,3,23,,,,,Google Scholar,Publisher: Cambridge University Press,,C:\Users\damru\Zotero\storage\CY9SV85R\Serizel and Giuliani - 2017 - Deep-neural network approaches for speech recognit.pdf; ,https://inria.hal.science/hal-01390905/file/15-1.pdf,,
XMYXM2FX,conferencePaper,2022,"Lastow, Fredrik; Ekberg, Edwin; Nugues, Pierre",Language-agnostic age and gender classification of voice using self-supervised pre-training,2022 Swedish Artificial Intelligence Society Workshop (SAIS),,,,https://ieeexplore.ieee.org/abstract/document/9833071/,,2022,7/19/2024 18:48,5/29/2025 10:02,7/19/2024 18:48,1–9,,,,IEEE,,,Google Scholar,,,,,,
SH93GZC5,journalArticle,2009,"Gerosa, Matteo; Giuliani, Diego; Brugnara, Fabio",Towards age-independent acoustic modeling,Speech Communication,,,,https://www.sciencedirect.com/science/article/pii/S0167639309000089,,2009,7/19/2024 18:48,5/29/2025 11:50,7/19/2024 18:48,499–509,6,51,,,,,Google Scholar,Publisher: Elsevier,,C:\Users\damru\Zotero\storage\AFMVMYGH\Gerosa et al. - 2009 - Towards age-independent acoustic modeling.pdf; ,https://hal.science/hal-00524121/file/PEER_stage2_10.1016%252Fj.specom.2009.01.006.pdf,,
NSSW9YY4,journalArticle,1985,"MacWhinney, Brian; Snow, Catherine",The child language data exchange system,Journal of child language,,,,https://www.cambridge.org/core/journals/journal-of-child-language/article/child-language-data-exchange-system/914F419D804ECA0C726F0C08E7E3C094,,1985,7/19/2024 18:44,5/29/2025 12:59,7/19/2024 18:44,271–295,2,12,,,,,Google Scholar,Publisher: Cambridge University Press,,C:\Users\damru\Zotero\storage\IAW6LDG5\MacWhinney and Snow - 1985 - The child language data exchange system.pdf; ,https://www.academia.edu/download/78100178/The_child_language_data_exchange_system20220105-16984-1akuf6w.pdf,,
IVK9EDWW,journalArticle,2017,"Gilkerson, Jill; Richards, Jeffrey A.; Warren, Steven F.; Montgomery, Judith K.; Greenwood, Charles R.; Kimbrough Oller, D.; Hansen, John H. L.; Paul, Terrance D.",Mapping the Early Language Environment Using All-Day Recordings and Automated Analysis,American Journal of Speech-Language Pathology,,"1058-0360, 1558-9110",10.1044/2016_AJSLP-15-0169,http://pubs.asha.org/doi/10.1044/2016_AJSLP-15-0169,"Purpose               This research provided a first-generation standardization of automated language environment estimates, validated these estimates against standard language assessments, and extended on previous research reporting language behavior differences across socioeconomic groups.                                         Method               Typically developing children between 2 to 48 months of age completed monthly, daylong recordings in their natural language environments over a span of approximately 6–38 months. The resulting data set contained 3,213 12-hr recordings automatically analyzed by using the Language Environment Analysis (LENA) System to generate estimates of (a) the number of adult words in the child's environment, (b) the amount of caregiver–child interaction, and (c) the frequency of child vocal output.                                         Results               Child vocalization frequency and turn-taking increased with age, whereas adult word counts were age independent after early infancy. Child vocalization and conversational turn estimates predicted 7%–16% of the variance observed in child language assessment scores. Lower socioeconomic status (SES) children produced fewer vocalizations, engaged in fewer adult–child interactions, and were exposed to fewer daily adult words compared with their higher socioeconomic status peers, but within-group variability was high.                                         Conclusions               The results offer new insight into the landscape of the early language environment, with clinical implications for identification of children at-risk for impoverished language environments.",5/17/2017,7/19/2024 18:43,5/29/2025 11:50,7/19/2024 18:43,248-265,2,26,Am J Speech Lang Pathol,,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\Z8R5ZRNV\2016_AJSLP-15-0169.html; ; C:\Users\damru\Zotero\storage\JJBRYWW5\Gilkerson et al. - 2017 - Mapping the Early Language Environment Using All-D.pdf; ,https://pubs.asha.org/doi/full/10.1044/2016_AJSLP-15-0169; https://europepmc.org/articles/pmc6195063?pdf=render,,
6P9669KR,journalArticle,2013,"McCabe, Allyssa; Bornstein, Marc H.; Guerra, Alison Wishard; Kuchirko, Yana; Páez, Mariela; Tamis‐LeMonda, Catherine S.; Cates, Carolyn Brockmeyer; Hirsh‐Pasek, Kathy; Melzi, Gigliana; Song, Lulu; Golinkoff, Roberta; Hoff, Erika; Mendelsohn, Alan",Multilingual Children: Beyond Myths and Toward Best Practices and commentaries,Social Policy Report,,"1075-7031, 2379-3988",10.1002/j.2379-3988.2013.tb00077.x,https://srcd.onlinelibrary.wiley.com/doi/10.1002/j.2379-3988.2013.tb00077.x,,2013-12,6/27/2024 4:34,5/29/2025 12:59,6/27/2024 4:34,Jan-37,4,27,Social Policy Report,,en,http://onlinelibrary.wiley.com/termsAndConditions#vor,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\QIG2ZZIQ\McCabe et al. - 2013 - Multilingual Children Beyond Myths and Toward Bes.pdf; ,https://files.eric.ed.gov/fulltext/ED595539.pdf,,
L49J7K2M,journalArticle,2022,"Singh, Leher; Cheng, QiQi; Tan, Seok Hui; Tan, Agnes; Low, Yen Ling",Language acquisition in a multilingual society: English vocabulary norms and predictors in Singaporean children,Child Development,,1467-8624,10.1111/cdev.13676,,"In this study, infant vocabulary development was tracked in a multilingual society (Singapore) within a socioeconomically diverse sample. The sample comprised 1316 infants from 17.4 to 27.7 months (669 females, 647 males; 88% Chinese race, 4% Malay, 4% Indian, and 0.004% mixed-race [4% declined to provide race information]). Children varied in English language exposure and socioeconomic status. Analyses focused on identifying demographic predictors of English vocabulary size in multilingually exposed infants. Adaptations of the Macarthur-Bates Communicative Development Inventory for English, Mandarin, and Malay are provided as well as English vocabulary norms that account for variation in English exposure. This manuscript reports the first set of English language norms-calibrated to English exposure-for multilingual infants in a non-Western setting.",2022-01,6/27/2024 4:02,5/29/2025 13:00,,288-305,1,93,Child Dev,,eng,,PubMed,PMID: 34672368,,,http://www.ncbi.nlm.nih.gov/pubmed/34672368,,Child; Child Language; Female; Humans; Infant; Language; Language Development; Language Tests; Male; Multilingualism; Vocabulary
K6WELIH9,bookSection,2023,"Amalika, S.; Putri, C. D. M.; Susanto, D.",Multilingual children's language use: The case of “the return of Superman” show,Reimagining Innovation in Education and Social Sciences,978-1-003-36668-3,,,,"This research has conducted descriptive qualitative method and Barron-Hauwaert's theory about multilingual children's language pattern usage, in order to investigate multilingual children's language use in the community through the influence of their closest environment represented in ‘The Return of Superman’ variety shows in Korea. The subject of the study is ‘Park Naeun’ as well as her multicultural family. The researchers would use the online documentation technique to collect data from selected videos on YouTube channels, as described in the three research questions. The results from the findings demonstrate that Park Naeun was capable of speaking using both the majority and minority languages as a multilingual child. The influence of parents’ language on the children's language is a signal that contributes to the success of the children's language-learning process. When parents can maintain their children's first language, they could also maintain their children's next language acquisition. This analysis revealed that Park Naeun changed their language use for many reasons: politeness, meaningful conversations, and similar perceptions.",2023,6/27/2024 3:52,5/29/2025 12:59,,,,,,Routledge,,,,Num Pages: 9,,C:\Users\damru\Zotero\storage\M5GWBCZG\Amalika et al. - 2023 - Multilingual children's language use The case of .pdf; ,https://s3-euw1-ap-pe-df-pch-content-store-p.s3.eu-west-1.amazonaws.com/9781003366683/772dc5da-5ce5-43c8-a9c8-02eecb8772f8/chapters/chapter14.pdf?AWSAccessKeyId=ASIAQFVOSJ57QWPA6M4X&Expires=1719460374&Signature=ErjlLmWDTwDrqx7CsBk15MZx0C4%3D&response-content-disposition=attachment%3B%20filename%3D%229781003366683_chapterpdf.pdf%22&x-amz-security-token=IQoJb3JpZ2luX2VjEDkaCXVzLWVhc3QtMSJGMEQCIG%2BbavPhgwvcZS9Vaq%2B4LMT0bXJCVn7KlaZ3YHZDu0fVAiAVS25cqVJVIwrtHFI2zrd01TKgQFbaFuaFtgRpoWQ1jyqWBAji%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAQaDDAxMjE3NzI2NDUxMSIMUVdhIOPbYtEayuJWKuoDDwBUl7tV5GSTJnMZfPWxSm2hZLoKvi%2FeTOKa3HQxnj0hSLImZf%2BsV3Yn4FSeiUilH77QMLnVvJ7mvXD6SYHNb4kupP2Xkx63n%2Fk6mD9L2G%2BmggeXucKo4kPEHIJq4paPIFXLBPk9y8BBYPVbzEFbixQAnOo9bM0bnFq387v2Z0suou%2FTxCaNmw9pHFTwDvpOS6GUBGIISUHPM14lhXTfYl9ZD6yrM10DIwrJrTS2rrRkd64%2BVDPzczdY8AFq1GNjviLoMYJ9Eny%2BxZyeDlKu4nmVCfU5sb1068Akrfhphati1Fn%2B7IH0yXaPlAg3QOKCZjoWTjzA%2B0UZbMgzckESljgf11rjXZR3E1ZAhJ0ly14H%2FrDttZ0qJJr3j3LbwwsgB4M8DOZR3EDXdaLEGEHtnnTTtTarFyC9Gi9glZ2i8lUFaBb9tfKHzPLHwZMqFVahnopptws4ffHc9TvGo%2BvScmcQoLVu8kjHnJIQmanuJmFPlLh9TWiVJlfauNocJMCDPoeGdL6UXN0tA8BOC53WPOvw0xQDY5KLgeMEZkj%2B6W14IFH62aYUYpb0Hxi%2FSjhATB7lA0SigsYJh30UO%2FIYVGzxnQzWS%2FWr9NGVjzdT%2BzEL2so8lEy41sGYSywhWGPpFB4%2BIBLFlMX8OzD58fKzBjqmAaG%2FJJklSCe8He3wCEmJhHwWcOWzgSh%2FcF6qeWgzLkkyvwaOit2gxT2D%2BhApttAkVw1WYRTrA9B%2FJKM9or6ljVdmtcJrnPShj%2FzXjerSbokOFoZCs%2BShkChhBvNhhmoaGTeIsmpePBKBO8JjE9BkwasxZyMqNtBrxnpAspIveTbpsxaEULitHhe7ynqiSVigBfsn6HLR6jM09J2noIY%2BJjM1bzrMMcs%3D,,
YA6GFZXU,journalArticle,,"Hua, Zhu; Wei, Li",Bi- and Multilingual Language Acquisition,,,,,,,,6/27/2024 3:42,5/29/2025 12:59,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\KBDNUJF6\Hua and Wei - Bi- and Multilingual Language Acquisition.pdf; ,https://d1wqtxts1xzle7.cloudfront.net/7559919/Bi_and_multilingual_acquisition0001-libre.pdf?1390852224=&response-content-disposition=inline%3B+filename%3DBi_and_Multilingual_Language_Acquisition.pdf&Expires=1719463337&Signature=gEFlAx47GJ0GiYGoHE03dr9S322QH7OWTJvH820UX0I6YIZqwab2~Rcc~3gK6zdR~Atzk3GuwFoTWeXCxYkA1t6zrh4YYesRw3hBKLHzQyS6aveSii18tEaAwSi9og84SLmnUrGZNG2ZI10QgJwyWk7nWaUDVdTGAmYd6votkTbVuH4pQ4RQ4aj7kmMmLtlnPvgroTtDqhXFq7VKieAFuIHvDqrjmfMdOfbeR0xAYsxYrEiGi-Y9LyTark02q~cI~4LVPQv7QCv2k4H4gOelT1LA4SKcqLWEgjt6WvHxSQb50d-R5fnIwpfjPReevAY9TTESRwvcmvOoI5GsCLSNlg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA,,
QGUBNMIH,journalArticle,2019,"Poeste, Meike; Müller, Natascha; Arnaus Gil, Laia","Code-mixing and language dominance: bilingual, trilingual and multilingual children compared",International Journal of Multilingualism,,"1479-0718, 1747-7530",10.1080/14790718.2019.1569017,https://www.tandfonline.com/doi/full/10.1080/14790718.2019.1569017,,10/2/2019,6/27/2024 3:35,5/29/2025 10:01,6/27/2024 3:35,459-491,4,16,International Journal of Multilingualism,,en,,DOI.org (Crossref),,,"C:\Users\damru\Zotero\storage\7875BKLK\Poeste et al. - 2019 - Code-mixing and language dominance bilingual, tri.pdf; ",https://www.romanistik.uni-wuppertal.de/fileadmin/romanistik/pdf/poeste_m-mueller_n-arnausgil_l19.pdf,,
6HIX4VDL,bookSection,2012,"Bernhardt, B. May; Stemberger, Joseph P.",19. Translation to Practice: Transcription of the Speech of Multilingual Children,19. Translation to Practice: Transcription of the Speech of Multilingual Children,978-1-84769-514-7,,,https://www.degruyter.com/document/doi/10.21832/9781847695147-023/pdf?licenseType=restricted,19. Translation to Practice: Transcription of the Speech of Multilingual Children was published in Multilingual Aspects of Speech Sound Disorders in Children on page 182.,2/20/2012,6/27/2024 3:35,5/29/2025 11:51,6/27/2024 3:35,182-190,,,,Multilingual Matters,en,De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.,www.degruyter.com,DOI: 10.21832/9781847695147-023,,,,,
XF6I5UZX,journalArticle,,"Rolland, Thomas; Abad, Alberto; Cucchiarini, Catia; Strik, Helmer",Multilingual Transfer Learning for Children Automatic Speech Recognition,,,,,,"Despite recent advances in automatic speech recognition (ASR), the recognition of children’s speech still remains a significant challenge. This is mainly due to the high acoustic variability and the limited amount of available training data. The latter problem is particularly evident in languages other than English, which are usually less-resourced. In the current paper, we address children ASR in a number of less-resourced languages by combining several small-sized children speech corpora from these languages. In particular, we address the following research question: Does a novel two-step training strategy in which multilingual learning is followed by language-specific transfer learning outperform conventional single language/task training for children speech, as well as multilingual and transfer learning alone? Based on previous experimental results with English, we hypothesize that multilingual learning provides a better generalization of the underlying characteristics of children’s speech. Our results provide a positive answer to our research question, by showing that using transfer learning on top of a multilingual model for an unseen language outperforms conventional single language-specific learning.",,6/27/2024 3:32,6/27/2024 3:32,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\GWFFPDYS\Rolland et al. - Multilingual Transfer Learning for Children Automa.pdf,,,
MGEF2RJR,journalArticle,2021,"Sullivan, Jessica; Mei, Michelle; Perfors, Andrew; Wojcik, Erica; Frank, Michael","SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant’s Perspective",Open Mind,,,10.1162/opmi_a_00039,,"We introduce a new resource: the SAYCam corpus. Infants aged 6–32 months wore a head-mounted camera for approximately 2 hr per week, over the course of approximately two-and-a-half years. The result is a large, naturalistic, longitudinal dataset of infant- and child-perspective videos. Over 200,000 words of naturalistic speech have already been transcribed. Similarly, the dataset is searchable using a number of criteria (e.g., age of participant, location, setting, objects present). The resulting dataset will be of broad use to psychologists, linguists, and computer scientists.",3/5/2021,6/27/2024 2:51,5/29/2025 12:58,,10-Jan,,5,Open Mind,,,,ResearchGate,,,"C:\Users\damru\Zotero\storage\A9VZN3TX\Sullivan et al. - 2021 - SAYCam A Large, Longitudinal Audiovisual Dataset .pdf; ; ",https://www.researchgate.net/publication/349840130_SAYCam_A_Large_Longitudinal_Audiovisual_Dataset_Recorded_From_the_Infant's_Perspective/fulltext/60948af5a6fdccaebd11e5f4/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset-Recorded-From-the-Infants-Perspective.pdf; https://www.researchgate.net/publication/349840130_SAYCam_A_Large_Longitudinal_Audiovisual_Dataset_Recorded_From_the_Infant%27s_Perspective,,
F4TTW7GK,conferencePaper,2023,"Shi, Jiatong; Berrebbi, Dan; Chen, William; Hu, En-Pei; Huang, Wei-Ping; Chung, Ho-Lam; Chang, Xuankai; Li, Shang-Wen; Mohamed, Abdelrahman; Lee, Hung-yi; Watanabe, Shinji",ML-SUPERB: Multilingual Speech Universal PERformance Benchmark,INTERSPEECH 2023,,,10.21437/Interspeech.2023-1316,https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.html,"Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of SelfSupervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from highresource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.",8/20/2023,6/26/2024 0:08,5/29/2025 12:58,6/26/2024 0:08,884-888,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\7DLA9P9S\shi23g_interspeech.pdf; ,https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.pdf,,
YQK9IGIS,preprint,2024,"Fan, Ruchao; Shankar, Natarajan Balaji; Alwan, Abeer",Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models,,,,,http://arxiv.org/abs/2406.10507,"Speech foundation models (SFMs) have achieved state-ofthe-art results for various speech tasks in supervised (e.g. Whisper) or self-supervised systems (e.g. WavLM). However, the performance of SFMs for child ASR has not been systematically studied. In addition, there is no benchmark for child ASR with standard evaluations, making the comparisons of novel ideas difficult. In this paper, we initiate and present a comprehensive benchmark on several child speech databases based on various SFMs (Whisper, Wav2vec2.0, HuBERT, and WavLM). Moreover, we investigate finetuning strategies by comparing various data augmentation and parameter-efficient finetuning (PEFT) methods. We observe that the behaviors of these methods are different when the model size increases. For example, PEFT matches the performance of full finetuning for large models but worse for small models. To stabilize finetuning using augmented data, we propose a perturbation invariant finetuning (PIF) loss as a regularization.1.",6/15/2024,6/24/2024 16:57,5/29/2025 12:59,6/24/2024 16:57,,,,,arXiv,en,,arXiv.org,"arXiv:2406.10507 [cs, eess]",,C:\Users\damru\Zotero\storage\GK3WZAGT\Fan et al. - 2024 - Benchmarking Children's ASR with Supervised and Se.pdf,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
FSIBF7EG,book,2000,"Shobaki, Khaldoun; Hosom, John-Paul; Cole, Ronald",The OGI kids’ speech corpus and recognizers,,,,,,"We describe a corpus of children's speech, called the OGI Kids' Speech corpus, and a speaker- and vocabulary-independent recognition system trained and evaluated with these data. The corpus is composed of both prompted and spontaneous speech from 1100 children from kindergarten through grade 10. The prompted speech was presented as text appearing below an animated character (Baldi) that produced accurate visible speech synchronized with recorded prompts. The speech and text consists of isolated words, sentences, and digit strings. A phonetic recognizer was trained using an HMM/ANN framework, with training data taken from intervals of speech associated with phonetic segments in the isolated words in the corpus. Phonetic segments were derived using automatic phonetic alignment. To find out how well the recognizer is able to generalize to new words not found in the training set, we performed two test-set evaluations: one using a new set of utterances from the set of 205 words spoken in isolation (similar to the data used to train the recognizer) and one using words from the prompted sentences. Results were dramatically different (97.5% for isolated vs. 37.9% for words in sentences), and we explore methods that may be used to improve the recognizer's ability to generalize to new words.",10/16/2000,6/24/2024 16:54,5/29/2025 12:58,,,,,,,,,ResearchGate,Pages: 261 DOI: 10.21437/ICSLP.2000-800,,C:\Users\damru\Zotero\storage\BKE32NLX\Shobaki et al. - 2000 - The OGI kids’ speech corpus and recognizers.pdf; ; ,https://www.researchgate.net/profile/Ronald-Cole/publication/221488491_The_OGI_kids'_speech_corpus_and_recognizers/links/53f5f4a10cf2888a7491fd21/The-OGI-kids-speech-corpus-and-recognizers.pdf; https://www.researchgate.net/publication/221488491_The_OGI_kids%27_speech_corpus_and_recognizers,,
43PR6W33,conferencePaper,2023,"Radford, Alec; Kim, Jong Wook; Xu, Tao; Brockman, Greg; Mcleavey, Christine; Sutskever, Ilya",Robust Speech Recognition via Large-Scale Weak Supervision,Proceedings of the 40th International Conference on Machine Learning,,,,https://proceedings.mlr.press/v202/radford23a.html,"We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",7/3/2023,6/24/2024 16:44,6/24/2024 16:44,6/24/2024 16:44,28492-28518,,,,PMLR,en,,proceedings.mlr.press,ISSN: 2640-3498,,C:\Users\damru\Zotero\storage\TEGD6S3N\Radford et al. - 2023 - Robust Speech Recognition via Large-Scale Weak Sup.pdf,,,
8SI7Q9WH,conferencePaper,2021,"Li, Bo; Pang, Ruoming; Sainath, Tara N.; Gulati, Anmol; Zhang, Yu; Qin, James; Haghani, Parisa; Huang, W. Ronny; Ma, Min; Bai, Junwen",Scaling End-to-End Models for Large-Scale Multilingual ASR,2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU),,,10.1109/ASRU51503.2021.9687871,https://ieeexplore.ieee.org/abstract/document/9687871,"Building ASR models across many languages is a challenging multi-task learning problem due to large variations and heavily unbalanced data. Existing work has shown positive transfer from high resource to low resource languages. However, degradations on high resource languages are commonly observed due to interference from the heterogeneous multilingual data and reduction in per-language capacity. We conduct a capacity study on a 15-language task, with the amount of data per language varying from 7.6K to 53.5K hours. We adopt GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that (1) scaling the number of model parameters is an effective way to solve the capacity bottleneck - our 500M-param model already outperforms monolingual baselines and scaling it to 1B and 10B brought further quality gains; (2) larger models are not only more data efficient, but also more efficient in terms of training cost as measured in TPU days - the 1B-param model reaches the same accuracy at 34% of training time as the 500M-param model; (3) given a fixed capacity budget, adding depth works better than width and large encoders do better than large decoders; (4) with continuous training, they can be adapted to new languages and domains.",2021-12,6/24/2024 16:14,5/29/2025 10:02,6/24/2024 16:14,1011-1018,,,,,,,IEEE Xplore,,,C:\Users\damru\Zotero\storage\5A2U3Z3U\9687871.html; ; C:\Users\damru\Zotero\storage\WA88V2XE\Li et al. - 2021 - Scaling End-to-End Models for Large-Scale Multilin.pdf; ,https://ieeexplore.ieee.org/abstract/document/9687871; https://arxiv.org/pdf/2104.14830,,Data models; Training; Adaptation models; Buildings; Costs; Interference; large-scale; multilingual speech recognition; Performance gain
772SAVNP,bookSection,2021,"Rose, Yvan; Hedlund, Gregory","The Phonbank Database within Talkbank, and a Practical Overview of the Phon Program",,978-0-429-32090-3,,,,,2/18/2021,6/24/2024 15:42,5/29/2025 12:58,,228-246,,,,,,,ResearchGate,DOI: 10.4324/9780429320903-19,,"C:\Users\damru\Zotero\storage\K8KSTQLD\Rose and Hedlund - 2021 - The Phonbank Database within Talkbank, and a Pract.pdf; ; ",https://www.researchgate.net/profile/Yvan-Rose/publication/349435799_The_Phonbank_Database_within_Talkbank_and_a_Practical_Overview_of_the_Phon_Program/links/60898bed458515d315e2efe7/The-Phonbank-Database-within-Talkbank-and-a-Practical-Overview-of-the-Phon-Program.pdf; https://www.researchgate.net/profile/Yvan-Rose/publication/349435799_The_Phonbank_Database_within_Talkbank_and_a_Practical_Overview_of_the_Phon_Program/links/60898bed458515d315e2efe7/The-Phonbank-Database-within-Talkbank-and-a-Practical-Overview-of-the-Phon-Program.pdf,,
3L8NNDQ8,webpage,,,Cristia: Accuracy of the language environment analysis... - Google Scholar,,,,,https://scholar.google.com/scholar_lookup?&title=Accuracy%20of%20the%20Language%20Environment%20Analysis%20System%20Segmentation%20and%20Metrics%3A%20A%20Systematic%20Review&journal=Journal%20of%20Speech%2C%20Language%2C%20and%20Hearing%20Research&doi=10.1044%2F2020_JSLHR-19-00017&volume=63&issue=4&pages=1093-1105&publication_year=2020&author=Cristia%2CA&author=Bulgarelli%2CF&author=Bergelson%2CE,,,6/24/2024 15:32,5/29/2025 12:58,6/24/2024 15:32,,,,,,,,,,,C:\Users\damru\Zotero\storage\43DV8LF4\scholar_lookup.html; ,https://scholar.google.com/scholar_lookup?&title=Accuracy%20of%20the%20Language%20Environment%20Analysis%20System%20Segmentation%20and%20Metrics%3A%20A%20Systematic%20Review&journal=Journal%20of%20Speech%2C%20Language%2C%20and%20Hearing%20Research&doi=10.1044%2F2020_JSLHR-19-00017&volume=63&issue=4&pages=1093-1105&publication_year=2020&author=Cristia%2CA&author=Bulgarelli%2CF&author=Bergelson%2CE,,
YGWLLBUI,journalArticle,2021,"Cristia, Alejandrina; Lavechin, Marvin; Scaff, Camila; Soderstrom, Melanie; Rowland, Caroline; Räsänen, Okko; Bunce, John; Bergelson, Elika",A thorough evaluation of the Language Environment Analysis (LENA) system,Behavior Research Methods,,1554-3528,10.3758/s13428-020-01393-5,https://doi.org/10.3758/s13428-020-01393-5,"In the previous decade, dozens of studies involving thousands of children across several research disciplines have made use of a combined daylong audio-recorder and automated algorithmic analysis called the LENAⓇ system, which aims to assess children’s language environment. While the system’s prevalence in the language acquisition domain is steadily growing, there are only scattered validation efforts on only some of its key characteristics. Here, we assess the LENAⓇ system’s accuracy across all of its key measures: speaker classification, Child Vocalization Counts (CVC), Conversational Turn Counts (CTC), and Adult Word Counts (AWC). Our assessment is based on manual annotation of clips that have been randomly or periodically sampled out of daylong recordings, collected from (a) populations similar to the system’s original training data (North American English-learning children aged 3-36 months), (b) children learning another dialect of English (UK), and (c) slightly older children growing up in a different linguistic and socio-cultural setting (Tsimane’ learners in rural Bolivia). We find reasonably high accuracy in some measures (AWC, CVC), with more problematic levels of performance in others (CTC, precision of male adults and other children). Statistical analyses do not support the view that performance is worse for children who are dissimilar from the LENAⓇ original training set. Whether LENAⓇ results are accurate enough for a given research, educational, or clinical application depends largely on the specifics at hand. We therefore conclude with a set of recommendations to help researchers make this determination for their goals.",4/1/2021,6/24/2024 15:30,5/29/2025 11:52,6/24/2024 15:30,467-486,2,53,Behav Res,,en,,Springer Link,,,C:\Users\damru\Zotero\storage\B2LIYICS\Cristia et al. - 2021 - A thorough evaluation of the Language Environment .pdf; ,https://link.springer.com/content/pdf/10.3758%2Fs13428-020-01393-5.pdf,,Adult Word Count; Agreement; Child Vocalization Count; Conversational Turn Count; English; Human transcription; LENA; Measurement error; Method comparison; Reliability; Speech technology; Tsimane’
9RXM4PKZ,preprint,2022,"Johnson, Alexander; Fan, Ruchao; Morris, Robin; Alwan, Abeer",LPC Augment: An LPC-Based ASR Data Augmentation Algorithm for Low and Zero-Resource Children's Dialects,,,,10.48550/arXiv.2202.09529,http://arxiv.org/abs/2202.09529,"This paper proposes a novel linear prediction coding-based data aug-mentation method for children's low and zero resource dialect ASR. The data augmentation procedure consists of perturbing the formant peaks of the LPC spectrum during LPC analysis and reconstruction. The method is evaluated on two novel children's speech datasets with one containing California English from the Southern CaliforniaArea and the other containing a mix of Southern American English and African American English from the Atlanta, Georgia area. We test the proposed method in training both an HMM-DNN system and an end-to-end system to show model-robustness and demonstrate that the algorithm improves ASR performance, especially for zero resource dialect children's task, as compared to common data augmentation methods such as VTLP, Speed Perturbation, and SpecAugment.",2/21/2022,6/23/2024 21:08,5/29/2025 10:03,6/23/2024 21:08,,,,,arXiv,,,arXiv.org,"arXiv:2202.09529 [cs, eess]",,C:\Users\damru\Zotero\storage\3ZTHYQXK\Johnson et al. - 2022 - LPC Augment An LPC-Based ASR Data Augmentation Al.pdf; ; C:\Users\damru\Zotero\storage\PJQB8I3N\2202.html; ,https://arxiv.org/pdf/2202.09529.pdf; https://arxiv.org/abs/2202.09529,,"Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; 14J60 (Primary) 14F05, 14J26 (Secondary); F.2.2; I.2.7"
KR9HCNQN,journalArticle,,"Ehlert, Hanna; Beaulac, Edith; Wallbaum, Maren; Gebauer, Christopher; Rumberg, Lars; Ostermann, Jörn; Lüdtke, Ulrike",COLLECTING AND ANNOTATING NATURAL CHILD SPEECH DATA – CHALLENGES AND INTERDISCIPLINARY PERSPECTIVES,,,,,,"In this paper we share experiences on collecting and annotating child speech data from our speech language therapy background and the TALC-project (Tools for Analyzing Language and Communication) where we explore the application of machine learning models (focus ASR) for linguistic and speech therapy purposes in an interdisciplinary team. We will reﬂect on the importance of collecting natural speech data for ASR model training and will summarize recommended methods for eliciting such spontaneous child speech at different ages. For annotating recorded data such as transcribing them and marking relevant parts for subsequent analysis, we will focus on possible ways to ensure communication between different researchers. Throughout, we will elaborate on the interdisciplinary collaboration in our project in order to ensure that requirements of model developers and end-users are met.",,6/23/2024 18:10,5/29/2025 12:59,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\WN79Y87J\Ehlert et al. - COLLECTING AND ANNOTATING NATURAL CHILD SPEECH DAT.pdf,,,
LZP2GN7P,conferencePaper,2018,"Sailor, Hardik; Patil, Ankur; Patil, Hemant",Advances in Low Resource ASR: A Deep Learning Perspective,6th Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU 2018),,,10.21437/SLTU.2018-4,https://www.isca-archive.org/sltu_2018/sailor18b_sltu.html,"Recently, developing Automatic Speech Recognition (ASR) systems for Low Resource (LR) languages is an active research area. The research in ASR is signiﬁcantly advanced using deep learning approaches producing state-of-the-art results compared to the conventional approaches. However, it is still challenging to use such approaches for LR languages since it requires a huge amount of training data. Recently, data augmentation, multilingual and cross-lingual approaches, transfer learning, etc. enable training deep learning architectures. This paper presents an overview of deep learning-based approaches for building ASR for LR languages. Recent projects and events organized to support the development of ASR and related applications in this direction are also discussed. This paper could be a good motivation for the researchers interested to work towards low resource ASR using deep learning techniques. The approaches described here could be useful in other related applications, such as audio search.",8/29/2018,6/23/2024 16:28,5/29/2025 12:58,6/23/2024 16:28,15-19,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\8Y4NW6JQ\Sailor et al. - 2018 - Advances in Low Resource ASR A Deep Learning Pers.pdf; ,https://www.isca-archive.org/sltu_2018/sailor18b_sltu.pdf,,
IH9WEVVB,conferencePaper,2019,"Prasad, Manasa; Esch, Daan Van; Ritchie, Sandy; Mortensen, Jonas Fromseier",Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data,Interspeech 2019,,,10.21437/Interspeech.2019-1775,https://www.isca-archive.org/interspeech_2019/prasad19_interspeech.html,"When building automatic speech recognition (ASR) systems, typically some amount of audio and text data in the target language is needed. While text data can be obtained relatively easily across many languages, transcribed audio data is challenging to obtain. This presents a barrier to making voice technologies available in more languages of the world. In this paper, we present a way to build an ASR system system for a language even in the absence of any audio training data in that language at all. We do this by simply re-using an existing acoustic model from a phonologically similar language, without any kind of modiﬁcation or adaptation towards the target language. The basic insight is that, if two languages are sufﬁciently similar in terms of their phonological system, an acoustic model should hold up relatively well when used for another language. We describe how we tailor our pronunciation models to enable such re-use, and show experimental results across a number of languages from various language families. We also provide a theoretical analysis of situations in which this approach is likely to work. Our results show that it is possible to achieve less than 20% word error rate (WER) using this method.",9/15/2019,6/23/2024 16:23,5/29/2025 10:01,6/23/2024 16:23,271-275,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\MARITUI6\Prasad et al. - 2019 - Building Large-Vocabulary ASR Systems for Language.pdf; ,https://www.isca-archive.org/interspeech_2019/prasad19_interspeech.pdf,,
2BUT5QKW,conferencePaper,2022,"Li, Chengfei; Deng, Shuhao; Wang, Yaoping; Wang, Guangjing; Gong, Yaguang; Chen, Changbin; Bai, Jinfeng",TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline,Interspeech 2022,,,10.21437/Interspeech.2022-877,https://www.isca-archive.org/interspeech_2022/li22j_interspeech.html,"This paper introduces a new corpus of Mandarin-English code-switching speech recognition—TALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license1. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable.",9/18/2022,6/21/2024 23:30,5/29/2025 10:02,6/21/2024 23:30,1741-1745,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\TGJ7J6XT\Li et al. - 2022 - TALCS An open-source Mandarin-English code-switch.pdf; ,https://arxiv.org/pdf/2206.13135,,
PCU4YUF5,conferencePaper,2005,"Batliner, Anton; Blomberg, Mats; D'Arcy, Shona; Elenius, Daniel; Giuliani, Diego; Gerosa, Matteo; Hacker, Christian; Russell, Martin; Steidl, Stefan; Wong, Michael",The PF_STAR children's speech corpus,Interspeech 2005,,,10.21437/Interspeech.2005-705,https://www.isca-archive.org/interspeech_2005/batliner05b_interspeech.html,"This paper describes the corpus of recordings of children’s speech which was collected as part of the EU FP5 PF STAR project. The corpus contains more than 60 hours of speech, including read and imitated native-language speech in British English, German and Swedish, read and imitated non-nativelanguage English speech from German, Italian and Swedish children, and native-language spontaneous and emotional speech in English and German.",9/4/2005,6/20/2024 1:01,5/29/2025 12:59,6/20/2024 1:01,2761-2764,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\MUYBNS32\Batliner et al. - 2005 - The PF_STAR children's speech corpus.pdf; ,https://www.isca-archive.org/interspeech_2005/batliner05b_interspeech.pdf,,
QM5MSBP4,preprint,2018,"Sanchez, Alessandro; Meylan, Stephan; Braginsky, Mika; MacDonald, Kyle; Yurovsky, Daniel; Frank, Michael C.",childes-db: a flexible and reproducible interface to the Child Language Data Exchange System,,,,10.31234/osf.io/93mwx,https://osf.io/93mwx,"The Child Language Data Exchange System (CHILDES) has played a critical role in research on child language development, particularly in characterizing the early language learning environment. Access to these data can be both complex for novices and difficult to automate for advanced users, however. To address these issues, we introduce childes-db, a database-formatted mirror of CHILDES that improves data accessibility and usability by offering novel interfaces, including browsable web applications and an R application programming interface (API). Along with versioned infrastructure that facilitates reproducibility of past analyses, these interfaces lower barriers to analyzing naturalistic parent-child language, allowing for a wider range of researchers in language and cognitive development to easily leverage CHILDES in their work.",4/23/2018,6/19/2024 2:40,5/29/2025 12:58,6/19/2024 2:40,,,,,OSF,en-us,,OSF Preprints,,,C:\Users\damru\Zotero\storage\RZZ88FXU\Sanchez et al. - 2018 - childes-db a flexible and reproducible interface .pdf; ,https://psyarxiv.com/93mwx/download,,
AWMQTAVW,journalArticle,2020,"Anh, Nguyen Tuan; Linh, Tran Thi Ngoc; Hien, Dang Thi",Automatic Multilingual Speech Recognition,,,,,,"Automatic Speech Recognition (ASR) for multi-languages is currently attracting more and more attention; however, development is still hampered by the need for language experts. End-toEnd ASR simplifies their work by directly predicting the output character based on the acoustic input. This study presents the improvement of LIS-Net model for End-to-End Vietnamese and Chinese ASR system. In this study, an efficient yet accurate end-to-end multilingual multi-speaker ASR model has developed, allowing direct conversion of raw speech audio signals into text of multiple languages. This study proposes a new method of coding labels specifically for multiple languages by pagination labels by language. The results of this study are significantly improved compared to that of baseline models.",2020,6/14/2024 19:39,5/29/2025 12:59,,,5,7,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\RSPA3SVW\Anh et al. - 2020 - Automatic Multilingual Speech Recognition.pdf; ,https://www.ijeas.org/download_data/IJEAS0705013.pdf,,
R9Y95CPL,journalArticle,,"Černiavski, Rafal",Cross-lingual and Multilingual Automatic Speech Recognition for Scandinavian Languages,,,,,,,,6/14/2024 19:39,5/29/2025 12:59,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\NQCRQUZ7\Černiavski - Cross-lingual and Multilingual Automatic Speech Re.pdf; ,https://uu.diva-portal.org/smash/get/diva2:1675877/FULLTEXT01.pdf,,
Q4QAXEG5,conferencePaper,2019,"Pires, Telmo; Schlinger, Eva; Garrette, Dan",How Multilingual is Multilingual BERT?,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,,,10.18653/v1/P19-1493,https://aclanthology.org/P19-1493,"In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",2019-07,6/12/2024 16:25,5/29/2025 10:01,6/12/2024 16:25,4996–5001,,,,Association for Computational Linguistics,,,ACLWeb,,,C:\Users\damru\Zotero\storage\F9UGB4KG\Pires et al. - 2019 - How Multilingual is Multilingual BERT.pdf; ,https://aclanthology.org/P19-1493.pdf,,
8ACI3VJT,journalArticle,1999,"Neufeld, Eric",Book Reviews: Statistical Methods for Speech Recognition,Computational Linguistics,,,,https://aclanthology.org/J99-2009,,1999,6/11/2024 21:19,5/29/2025 10:01,6/11/2024 21:19,,2,25,,,,,ACLWeb,"Place: Cambridge, MA Publisher: MIT Press",,C:\Users\damru\Zotero\storage\QUXRJDF3\Neufeld - 1999 - Book Reviews Statistical Methods for Speech Recog.pdf; ,https://aclanthology.org/J99-2009.pdf,,
6XB4IRAW,conferencePaper,2021,"Conneau, Alexis; Baevski, Alexei; Collobert, Ronan; Mohamed, Abdelrahman; Auli, Michael",Unsupervised Cross-Lingual Representation Learning for Speech Recognition,,,,10.21437/Interspeech.2021-329,https://www.isca-archive.org/interspeech_2021/conneau21_interspeech.html,,2021,6/10/2024 21:20,5/29/2025 11:52,6/10/2024 21:20,2426-2430,,,,,,,www.isca-archive.org,,,C:\Users\damru\Zotero\storage\KV67N6WJ\Conneau et al. - 2021 - Unsupervised Cross-Lingual Representation Learning.pdf; ,https://arxiv.org/pdf/2006.13979,,
456C6452,conferencePaper,2020,"Datta, Arindrima; Ramabhadran, Bhuvana; Emond, Jesse; Kannan, Anjuli; Roark, Brian",Language-agnostic multilingual modeling,"ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",,,,https://ieeexplore.ieee.org/abstract/document/9053443/,,2020,4/9/2024 4:34,5/29/2025 12:59,4/9/2024 4:33,8239–8243,,,,IEEE,,,Google Scholar,,,C:\Users\damru\Zotero\storage\2SD9R7HC\Datta et al. - 2020 - Language-agnostic multilingual modeling.pdf; ,https://arxiv.org/pdf/2004.09571,,
A7Y8L3L6,journalArticle,2024,"Sobti, Rajni; Guleria, Kalpna; Kadyan, Virender","Comprehensive literature review on children automatic speech recognition system, acoustic linguistic mismatch approaches and challenges",Multimedia Tools and Applications,,1573-7721,10.1007/s11042-024-18753-4,https://link.springer.com/10.1007/s11042-024-18753-4,,3/11/2024,4/9/2024 4:37,5/29/2025 12:58,4/9/2024 4:37,,,,Multimed Tools Appl,,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\TRJ68QI8\Sobti et al. - 2024 - Comprehensive literature review on children automa.pdf; ,https://link.springer.com/content/pdf/10.1007%2Fs11042-024-18753-4.pdf,,Acoustic and Linguistic Variations; Applications of Child ASRs; Automatic Speech Recognition; Data Augmentation; Low Resource Language; Mismatch ASR
9BHQLF9S,journalArticle,2022,"Sobti, Rajni; Kadyan, Virender; Guleria, Kalpna",Challenges for Designing of Children Speech Corpora: A State-of-the-Art Review,ECS Transactions,,1938-5862,10.1149/10701.9053ecst,https://iopscience.iop.org/article/10.1149/10701.9053ecst/meta,"Challenges for Designing of Children Speech Corpora: A State-of-the-Art Review, Rajni Sobti, Virender Kadyan, Kalpna Guleria",4/24/2022,4/9/2024 4:06,5/29/2025 12:58,4/9/2024 4:06,9053,1,107,ECS Trans.,,en,,iopscience.iop.org,Publisher: IOP Publishing,,C:\Users\damru\Zotero\storage\IFVMIGWU\Sobti et al. - 2022 - Challenges for Designing of Children Speech Corpor.pdf; ,https://iopscience.iop.org/article/10.1149/10701.9053ecst/pdf,,
MAINLZJ7,journalArticle,2022,"Hidayatullah, Ahmad Fathan; Qazi, Atika; Lai, Daphne Teck Ching; Apong, Rosyzie Anna","A systematic review on language identification of code-mixed text: techniques, data availability, challenges, and framework development",IEEE access,,,,https://ieeexplore.ieee.org/abstract/document/9956817/,,2022,4/11/2024 23:14,5/29/2025 13:00,4/11/2024 23:14,122812–122831,,10,,,,,Google Scholar,Publisher: IEEE,,"C:\Users\damru\Zotero\storage\9TTYZ4R8\2006 - (PDF) A Systematic Review on Language Identification of Code-Mixed Text Techniques, Data Availabili.pdf",,,
GI4VFKMI,preprint,2020,"Sitaram, Sunayana; Chandu, Khyathi Raghavi; Rallabandi, Sai Krishna; Black, Alan W.",A Survey of Code-switched Speech and Language Processing,,,,,http://arxiv.org/abs/1904.00784,"Code-switching, the alternation of languages within a conversation or utterance, is a common communicative phenomenon that occurs in multilingual communities across the world. This survey reviews computational approaches for code-switched Speech and Natural Language Processing. We motivate why processing code-switched text and speech is essential for building intelligent agents and systems that interact with users in multilingual communities. As code-switching data and resources are scarce, we list what is available in various code-switched language pairs with the language processing tasks they can be used for. We review code-switching research in various Speech and NLP applications, including language processing tools and end-to-end systems. We conclude with future directions and open problems in the field.",7/22/2020,4/11/2024 23:14,5/29/2025 12:58,4/11/2024 23:13,,,,,arXiv,,,arXiv.org,"arXiv:1904.00784 [cs, stat]",,C:\Users\damru\Zotero\storage\B48JHEYQ\Sitaram et al. - 2020 - A Survey of Code-switched Speech and Language Proc.pdf; ; C:\Users\damru\Zotero\storage\8MHRR4WK\1904.html; ,https://arxiv.org/pdf/1904.00784.pdf; https://arxiv.org/abs/1904.00784,,Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning
E9CEJFEH,journalArticle,2014,"Crowe, Kathryn; McLeod, Sharynne",A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss,International Journal of Bilingual Education and Bilingualism,,"1367-0050, 1747-7522",10.1080/13670050.2012.758686,http://www.tandfonline.com/doi/abs/10.1080/13670050.2012.758686,,5/4/2014,4/19/2024 21:09,5/29/2025 11:52,4/19/2024 21:09,287-309,3,17,International Journal of Bilingual Education and Bilingualism,,en,,DOI.org (Crossref),,,,,,
DTAQ3YNL,bookSection,2012,"Butler, Yuko Goto",Bilingualism/Multilingualism and Second‐Language Acquisition,The Handbook of Bilingualism and Multilingualism,978-1-4443-3490-6 978-1-118-33238-2,,,https://onlinelibrary.wiley.com/doi/10.1002/9781118332382.ch5,,11/7/2012,4/19/2024 21:09,5/29/2025 12:59,4/19/2024 21:09,109-136,,,,Wiley,en,http://doi.wiley.com/10.1002/tdm_license_1.1,DOI.org (Crossref),DOI: 10.1002/9781118332382.ch5,,C:\Users\damru\Zotero\storage\HSI3YW98\Butler - 2012 - BilingualismMultilingualism and Second‐Language A.pdf; ,https://www.researchgate.net/profile/Yuko-Butler/publication/287446419_BilingualismMultilingualism_and_Second-Language_Acquisition/links/5ad4e2feaca272fdaf7c02f0/Bilingualism-Multilingualism-and-Second-Language-Acquisition.pdf,,
SLHF5CTI,bookSection,2017,"De Houwer, Annick",Bilingual Language Acquisition,The Handbook of Child Language,978-0-631-20312-4 978-1-4051-6631-7,,,https://onlinelibrary.wiley.com/doi/10.1111/b.9780631203124.1996.00009.x,,8/18/2017,4/19/2024 21:07,5/29/2025 13:00,4/19/2024 21:07,219-250,,,,Wiley,en,,DOI.org (Crossref),DOI: 10.1111/b.9780631203124.1996.00009.x,,,,,
A6PIC3K2,conferencePaper,2021,"Diwan, Anuj; Vaideeswaran, Rakesh; Shah, Sanket; Singh, Ankita; Raghavan, Srinivasa; Khare, Shreya; Unni, Vinit; Vyas, Saurabh; Rajpuria, Akash; Yarra, Chiranjeevi; Mittal, Ashish; Ghosh, Prasanta Kumar; Jyothi, Preethi; Bali, Kalika; Seshadri, Vivek; Sitaram, Sunayana; Bharadwaj, Samarth; Nanavati, Jai; Nanavati, Raoul; Sankaranarayanan, Karthik; Seeram, Tejaswi; Abraham, Basil",Multilingual and code-switching ASR challenges for low resource Indian languages,Interspeech 2021,,,10.21437/Interspeech.2021-1339,http://arxiv.org/abs/2104.00235,"Recently, there is increasing interest in multilingual automatic speech recognition (ASR) where a speech recognition system caters to multiple low resource languages by taking advantage of low amounts of labeled corpora in multiple languages. With multilingualism becoming common in today's world, there has been increasing interest in code-switching ASR as well. In code-switching, multiple languages are freely interchanged within a single sentence or between sentences. The success of low-resource multilingual and code-switching ASR often depends on the variety of languages in terms of their acoustics, linguistic characteristics as well as the amount of data available and how these are carefully considered in building the ASR system. In this challenge, we would like to focus on building multilingual and code-switching ASR systems through two different subtasks related to a total of seven Indian languages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati and Bengali. For this purpose, we provide a total of ~600 hours of transcribed speech data, comprising train and test sets, in these languages including two code-switched language pairs, Hindi-English and Bengali-English. We also provide a baseline recipe for both the tasks with a WER of 30.73% and 32.45% on the test sets of multilingual and code-switching subtasks, respectively.",8/30/2021,4/19/2024 20:47,5/29/2025 12:59,4/19/2024 20:47,2446-2450,,,,,,,arXiv.org,"arXiv:2104.00235 [cs, eess]",,C:\Users\damru\Zotero\storage\3MTLXVNU\Diwan et al. - 2021 - Multilingual and code-switching ASR challenges for.pdf; ; C:\Users\damru\Zotero\storage\HEX34STU\2104.html; ,https://arxiv.org/pdf/2104.00235.pdf; https://arxiv.org/abs/2104.00235,,Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
62PSYWRN,conferencePaper,2019,"Park, Daniel S.; Chan, William; Zhang, Yu; Chiu, Chung-Cheng; Zoph, Barret; Cubuk, Ekin D.; Le, Quoc V.",SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,Interspeech 2019,,,10.21437/Interspeech.2019-2680,http://arxiv.org/abs/1904.08779,"We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",9/15/2019,4/19/2024 20:37,5/29/2025 10:01,4/19/2024 20:37,2613-2617,,,,,,,arXiv.org,"arXiv:1904.08779 [cs, eess, stat]",,C:\Users\damru\Zotero\storage\CYH3ALM3\Park et al. - 2019 - SpecAugment A Simple Data Augmentation Method for.pdf; ; ; C:\Users\damru\Zotero\storage\UX9BECDT\1904.html; ; ,https://arxiv.org/pdf/1904.08779.pdf; https://arxiv.org/pdf/1904.08779.pdf; https://arxiv.org/abs/1904.08779; https://arxiv.org/abs/1904.08779,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language; Computer Science - Machine Learning; Statistics - Machine Learning
SZ8LJD22,journalArticle,,"Chung, Yu-An",Self-Supervised Learning for Speech Processing,,,,,,"Deep neural networks trained with supervised learning algorithms on large amounts of labeled speech data have achieved remarkable performance on various spoken language processing applications, often being the state of the arts on the corresponding leaderboards. However, the fact that training these systems relies on large amounts of annotated speech poses a scalability bottleneck for the continued advancement of state-of-the-art performance, and an even more fundamental barrier for deployment of deep neural networks in speech domains where labeled data are intrinsically rare, costly, or time-consuming to collect.",,4/12/2024 21:41,5/29/2025 12:59,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\FH3IE7LQ\Chung - Self-Supervised Learning for Speech Processing.pdf,,,
DCB3FCMX,preprint,2023,"Adebara, Ife; Elmadany, AbdelRahim; Abdul-Mageed, Muhammad; Inciarte, Alcides Alcoba",SERENGETI: Massively Multilingual Language Models for Africa,,,,,http://arxiv.org/abs/2212.10785,"Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.\footnote{\href{https://github.com/UBC-NLP/serengeti}{https://github.com/UBC-NLP/serengeti}}",5/26/2023,4/12/2024 21:35,5/29/2025 12:59,4/12/2024 21:35,,,,,arXiv,,,arXiv.org,arXiv:2212.10785 [cs],,C:\Users\damru\Zotero\storage\G29V8QY3\Adebara et al. - 2023 - SERENGETI Massively Multilingual Language Models .pdf; ; C:\Users\damru\Zotero\storage\8DBETHI5\2212.html; ,https://arxiv.org/pdf/2212.10785.pdf; https://arxiv.org/abs/2212.10785,,Computer Science - Artificial Intelligence; Computer Science - Computation and Language
PV9ZJUJR,journalArticle,2019,"Gale, Robert; Chen, Liu; Dolata, Jill; van Santen, Jan; Asgari, Meysam",Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques,Interspeech,,2308-457X,10.21437/Interspeech.2019-3161,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575194/,"This study explores building and improving an automatic speech recognition (ASR) system for children aged 6–9 years and diagnosed with autism spectrum disorder (ASD), language impairment (LI), or both. Working with only 1.5 hours of target data in which children perform the Clinical Evaluation of Language Fundamentals Recalling Sentences task, we apply deep neural network (DNN) weight transfer techniques to adapt a large DNN model trained on the LibriSpeech corpus of adult speech. To begin, we aim to find the best proportional training rates of the DNN layers. Our best configuration yields a 29.38% word error rate (WER). Using this configuration, we explore the effects of quantity and similarity of data augmentation in transfer learning. We augment our training with portions of the OGI Kids’ Corpus, adding 4.6 hours of typically developing speakers aged kindergarten through 3rd grade. We find that 2nd grade data alone — approximately the mean age of the target data — outperforms other grades and all the sets combined. Doubling the data for 1st, 2nd, and 3rd grade, we again compare each grade as well as pairs of grades. We find the combination of 1st and 2nd grade performs best at a 26.21% WER.",2019-09,9/26/2024 18:11,5/29/2025 12:59,9/26/2024 18:11,15-Nov,,2019,Interspeech,,,,PubMed Central,PMID: 33088838 PMCID: PMC7575194,,C:\Users\damru\Zotero\storage\JJGBTYAU\Gale et al. - 2019 - Improving ASR Systems for Children with Autism and.pdf; ; ,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575194/pdf/nihms-1052986.pdf; https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575194/,,
GWEU8DKR,book,2019,"Ramteke, Pravin; Supanekar, Sujata; Hegde, Pradyoth; Nelson, Hanna; Aithal, Venkataraja; Koolagudi, Shashidhar",NITK Kids’ Speech Corpus,,,,,,,9/15/2019,9/26/2024 18:04,5/29/2025 12:59,,,,,,,,,ResearchGate,Pages: 335 DOI: 10.21437/Interspeech.2019-2061,,C:\Users\damru\Zotero\storage\W3TGIPI2\Ramteke et al. - 2019 - NITK Kids’ Speech Corpus.pdf; ; ,https://www.researchgate.net/profile/Pradyoth-Hegde/publication/335829087_NITK_Kids'_Speech_Corpus/links/5db84cdba6fdcc2128eb86a3/NITK-Kids-Speech-Corpus.pdf; https://www.researchgate.net/publication/335829087_NITK_Kids%27_Speech_Corpus,,
3FTKPNY4,journalArticle,2011,"Kraleva, Radoslava",Design and development a children’s speech database,Natural Science,,,,,"The report presents the process of planning, designing and the development of a database of spoken children’s speech whose native language is Bulgarian. The proposed model is designed for children between the age of 4 and 6 without speech disorders, and reflects their specific capabilities. At this age most children cannot read, there is no sustained concentration, they are emotional, etc. The aim is to unite all the media information accompanying the recording and processing of spoken speech, thereby to facilitate the work of researchers in the field of speech recognition. This database will be used for the development of systems for children’s speech recognition, children's speech synthesis systems, games which allow voice control, etc. As a result of the proposed model a prototype system for speech recognition is presented.",2011,9/26/2024 16:14,5/29/2025 10:02,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\FRJ7U4KR\Kraleva - 2011 - Design and development a children’s speech databas.pdf; ,https://arxiv.org/pdf/1605.07735,,
RT9696B5,conferencePaper,2021,"Xu, Runxin; Luo, Fuli; Zhang, Zhiyuan; Tan, Chuanqi; Chang, Baobao; Huang, Songfang; Huang, Fei",Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,,,10.18653/v1/2021.emnlp-main.749,https://aclanthology.org/2021.emnlp-main.749,"Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.",2021-11,9/10/2024 21:18,5/29/2025 9:59,9/10/2024 21:18,9514–9528,,,,Association for Computational Linguistics,,,ACLWeb,,,C:\Users\damru\Zotero\storage\A4J4Y889\Xu et al. - 2021 - Raise a Child in Large Language Model Towards Eff.pdf; ,https://aclanthology.org/2021.emnlp-main.749.pdf,,
79QUI6X7,conferencePaper,2024,"Southwell, Rosy; Ward, Wayne; Trinh, Viet Anh; Clevenger, Charis; Clevenger, Clay; Watts, Emily; Reitman, Jason; D’Mello, Sidney; Whitehill, Jacob",Automatic Speech Recognition Tuned for Child Speech in the Classroom,"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",979-8-3503-4485-1,,10.1109/ICASSP48485.2024.10447428,https://ieeexplore.ieee.org/document/10447428/,,4/14/2024,9/10/2024 20:54,5/29/2025 12:58,9/10/2024 20:54,12291-12295,,,,IEEE,en,https://doi.org/10.15223/policy-029,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\3Y2PML24\Southwell et al. - 2024 - Automatic Speech Recognition Tuned for Child Speec.pdf; ,https://www.colorado.edu/research/ai-institute/sites/default/files/attached-files/childasr_icassp24_camera-ready_0.pdf,,
DHPET2YT,preprint,2020,"Kawakami, Kazuya; Wang, Luyu; Dyer, Chris; Blunsom, Phil; Oord, Aaron van den",Learning Robust and Multilingual Speech Representations,,,,10.48550/arXiv.2001.11128,http://arxiv.org/abs/2001.11128,"Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages including tonal languages and low-resource languages.",1/29/2020,8/1/2024 18:02,5/29/2025 10:02,8/1/2024 18:02,,,,,arXiv,,,arXiv.org,"arXiv:2001.11128 [cs, eess]",,C:\Users\damru\Zotero\storage\LCMEEQRM\Kawakami et al. - 2020 - Learning Robust and Multilingual Speech Representa.pdf; ; C:\Users\damru\Zotero\storage\QAJD64E4\2001.html; ,https://arxiv.org/pdf/2001.11128.pdf; https://arxiv.org/abs/2001.11128,,Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language; Computer Science - Machine Learning
DF3B47SG,journalArticle,2022,"Mohamed, Abdelrahman; Lee, Hung-yi; Borgholt, Lasse; Havtorn, Jakob D.; Edin, Joakim; Igel, Christian; Kirchhoff, Katrin; Li, Shang-Wen; Livescu, Karen; Maaløe, Lars; Sainath, Tara N.; Watanabe, Shinji",Self-Supervised Speech Representation Learning: A Review,IEEE Journal of Selected Topics in Signal Processing,,"1932-4553, 1941-0484",10.1109/JSTSP.2022.3207050,http://arxiv.org/abs/2205.10643,"Although supervised deep learning has revolutionized speech and audio processing, it has necessitated the building of specialist models for individual tasks and application scenarios. It is likewise difficult to apply this to dialects and languages for which only limited labeled data is available. Self-supervised representation learning methods promise a single universal model that would benefit a wide variety of tasks and domains. Such methods have shown success in natural language processing and computer vision domains, achieving new levels of performance while reducing the number of labels required for many downstream scenarios. Speech representation learning is experiencing similar progress in three main categories: generative, contrastive, and predictive methods. Other approaches rely on multi-modal data for pre-training, mixing text or visual data streams with speech. Although self-supervised speech representation is still a nascent research area, it is closely related to acoustic word embedding and learning with zero lexical resources, both of which have seen active research for many years. This review presents approaches for self-supervised speech representation learning and their connection to other research areas. Since many current methods focus solely on automatic speech recognition as a downstream task, we review recent efforts on benchmarking learned representations to extend the application beyond speech recognition.",2022-10,8/1/2024 16:48,5/29/2025 12:59,8/1/2024 16:48,1179-1210,6,16,IEEE J. Sel. Top. Signal Process.,,,,arXiv.org,"arXiv:2205.10643 [cs, eess]",,C:\Users\damru\Zotero\storage\SFQZ8IT8\Mohamed et al. - 2022 - Self-Supervised Speech Representation Learning A .pdf; ; C:\Users\damru\Zotero\storage\P9AKADGQ\2205.html; ,https://arxiv.org/pdf/2205.10643.pdf; https://arxiv.org/abs/2205.10643,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
PGSMEJF2,conferencePaper,2021,"Cumbal, Ronald; Moell, Birger; Águas Lopes, José David; Engwall, Olov",“You don’t understand me!”: Comparing ASR results for L1 and L2 speakers of Swedish,"22nd Annual Conference of the International Speech Communication Association, INTERSPEECH 2021, Brno, 30 August 2021, through 3 September 2021",,,,https://www.diva-portal.org/smash/record.jsf?pid=diva2:1663482,,2021,4/9/2024 4:27,5/29/2025 11:52,4/9/2024 4:27,96–100,,,,International Speech Communication Association,,,Google Scholar,,,C:\Users\damru\Zotero\storage\8FE4KID8\Cumbal et al. - 2021 - “You don’t understand me!” Comparing ASR results .pdf; ,https://www.diva-portal.org/smash/get/diva2:1663482/FULLTEXT01.pdf,,
P5Q58F9Y,journalArticle,2023,"Lüdtke, Ulrike; Bornman, Juan; De Wet, Febe; Heid, Ulrich; Ostermann, Jörn; Rumberg, Lars; Van der Linde, Jeannie; Ehlert, Hanna",Multidisciplinary Perspectives on Automatic Analysis of Children’s Language Samples: Where Do We Go from Here?,Folia Phoniatrica et Logopaedica,,,,https://karger.com/fpl/article-abstract/75/1/1/841975,,2023,4/9/2024 4:37,5/29/2025 10:02,4/9/2024 4:37,1–12,1,75,,,,,Google Scholar,Number: 1,,C:\Users\damru\Zotero\storage\4PJP9CM7\Lüdtke et al. - 2023 - Multidisciplinary Perspectives on Automatic Analys.pdf; ,https://repository.up.ac.za/bitstream/handle/2263/92336/Ludtke_Multidisciplinary_2023.pdf?sequence=1,,
F2T6B2EG,conferencePaper,2021,"Xu, Gaopeng; Yang, Song; Ma, Lu; Li, Chengfei; Wu, Zhongqin",The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech.,Interspeech,,,,https://www.isca-archive.org/interspeech_2021/xu21c_interspeech.pdf,,2021,4/9/2024 4:39,5/29/2025 9:59,4/9/2024 4:39,1294–1298,,,,,,,Google Scholar,,,C:\Users\damru\Zotero\storage\2J9EQM3V\Xu et al. - 2021 - The TAL System for the INTERSPEECH2021 Shared Task.pdf; ,https://www.isca-archive.org/interspeech_2021/xu21c_interspeech.pdf,,
LMQ9HWRG,journalArticle,2023,"Ravishankar, Vinit","Understanding Multilingual Language Models: Training, Representation and Architecture",,,,,https://www.duo.uio.no/handle/10852/102833,,2023,4/11/2024 23:00,5/29/2025 12:59,4/11/2024 23:00,,,,,,,,Google Scholar,,,C:\Users\damru\Zotero\storage\UP6HBVEI\Ravishankar - 2023 - Understanding Multilingual Language Models Traini.pdf; ,https://www.duo.uio.no/bitstream/handle/10852/102833/PhD-Ravishankar-2023.pdf?sequence=1,,
UYFGMGGM,journalArticle,2022,"Mustafa, Mumtaz Begum; Yusoof, Mansoor Ali; Khalaf, Hasan Kahtan; Rahman Mahmoud Abushariah, Ahmad Abdel; Kiah, Miss Laiha Mat; Ting, Hua Nong; Muthaiyah, Saravanan",Code-switching in automatic speech recognition: The issues and future directions,Applied Sciences,,,,https://www.mdpi.com/2076-3417/12/19/9541,,2022,4/11/2024 23:14,5/29/2025 10:01,4/11/2024 23:14,9541,19,12,,,,,Google Scholar,Number: 19 Publisher: MDPI,,C:\Users\damru\Zotero\storage\TXW92UIY\Begum Mustafa et al. - 2022 - Code-Switching in Automatic Speech Recognition Th.pdf; ; ,https://www.mdpi.com/2076-3417/12/19/9541; https://figshare.cardiffmet.ac.uk/articles/journal_contribution/Code-Switching_in_Automatic_Speech_Recognition_The_Issues_and_Future_Directions/21511053/1/files/38126151.pdf,,multilingual speech recognition; automatic speech recognition system; bilingual speech recognition; code-switching; evaluation metrics; language and acoustic models
8NVK2K6U,preprint,2024,"Zhou, Zhong",Massively Multilingual Text Translation For Low-Resource Languages,,,,,http://arxiv.org/abs/2401.16582,"Translation into severely low-resource languages has both the cultural goal of saving and reviving those languages and the humanitarian goal of assisting the everyday needs of local communities that are accelerated by the recent COVID-19 pandemic. In many humanitarian efforts, translation into severely low-resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, low-resource languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich-resource languages to efficiently produce best possible translation quality for well known texts, which are available in multiple languages, in a new, low-resource language. To reach this goal, we argue that in translating a closed text into low-resource languages, generalization to out-of-domain texts is not necessary, but generalization to new languages is. Performance gain comes from massive source parallelism by careful choice of close-by language families, style-consistent corpus-level paraphrases within the same language and strategic adaptation of existing large pretrained multilingual models to the domain first and then to the language. Such performance gain makes it possible for machine translation systems to collaborate with human translators to expedite the translation process into new, low-resource languages.",1/29/2024,5/29/2025 9:45,5/29/2025 9:57,4/12/2024 21:38,,,,,arXiv,,,arXiv.org,Issue: arXiv:2401.16582 arXiv:2401.16582 [cs],,C:\Users\damru\Zotero\storage\7JCL9M98\Zhou - 2024 - Massively Multilingual Text Translation For Low-Re.pdf; ; C:\Users\damru\Zotero\storage\WQF8I6TJ\2401.html; ,https://arxiv.org/pdf/2401.16582.pdf; https://arxiv.org/abs/2401.16582,,Computer Science - Computation and Language
J4XRPD3G,journalArticle,2024,"Kuhn, Korbinian; Kersken, Verena; Reuter, Benedikt; Egger, Niklas; Zimmermann, Gottfried",Measuring the Accuracy of Automatic Speech Recognition Solutions,ACM Transactions on Accessible Computing,,1936-7228,10.1145/3636513,https://dl.acm.org/doi/10.1145/3636513,"For d/Deaf and hard of hearing (DHH) people, captioning is an essential accessibility tool. Significant developments in artificial intelligence mean that automatic speech recognition (ASR) is now a part of many popular applications. This makes creating captions easy and broadly available—but transcription needs high levels of accuracy to be accessible. Scientific publications and industry report very low error rates, claiming that artificial intelligence has reached human parity or even outperforms manual transcription. At the same time, the DHH community reports serious issues with the accuracy and reliability of ASR. There seems to be a mismatch between technical innovations and the real-life experience for people who depend on transcription. Independent and comprehensive data is needed to capture the state of ASR. We measured the performance of 11 common ASR services with recordings of Higher Education lectures. We evaluated the influence of technical conditions like streaming, the use of vocabularies, and differences between languages. Our results show that accuracy ranges widely between vendors and for the individual audio samples. We also measured a significant lower quality for streaming ASR, which is used for live events. Our study shows that despite the recent improvements of ASR, common services lack reliability in accuracy.",1/9/2024,4/12/2024 21:43,5/29/2025 10:02,4/12/2024 21:43,25:1–25:23,4,16,ACM Trans. Access. Comput.,,,,ACM Digital Library,Number: 4,,C:\Users\damru\Zotero\storage\S4NRHT7Q\Kuhn et al. - 2024 - Measuring the Accuracy of Automatic Speech Recogni.pdf; ,https://dl.acm.org/doi/pdf/10.1145/3636513,,captions; real time; subtitles; Transcription
LFITLS6B,journalArticle,2007,"Genesee, Fred; Nicoladis, Elena",Bilingual first language acquisition,Blackwell handbook of language development,,,,https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=262871b2accf5071b90bc663e4867b92c40c83b9,,2007,4/19/2024 21:09,5/29/2025 13:00,4/19/2024 21:09,324–342,,,,,,,Google Scholar,Publisher: Citeseer,,C:\Users\damru\Zotero\storage\VN3CY48T\Genesee - BILINGUAL FIRST LANGAUGE ACQUISITION.pdf,,,
2DU7EAFG,journalArticle,2022,"Jain, Rishabh; Barcovschi, Andrei; Yiwere, Mariam; Bigioi, Dan; Corcoran, Peter; Cucu, Horia",A Wav2vec2-Based Experimental Study on Self-Supervised Learning Methods to Improve Child Speech Recognition,,,,10.48550/ARXIV.2204.05419,https://arxiv.org/abs/2204.05419,"Despite recent advancements in deep learning technologies, Child Speech Recognition remains a challenging task. Current Automatic Speech Recognition (ASR) models require substantial amounts of annotated data for training, which is scarce. In this work, we explore using the ASR model, wav2vec2, with different pretraining and finetuning configurations for self-supervised learning (SSL) toward improving automatic child speech recognition. The pretrained wav2vec2 models were finetuned using different amounts of child speech training data, adult speech data, and a combination of both, to discover the optimum amount of data required to finetune the model for the task of child ASR. Our trained model achieves the best Word Error Rate (WER) of 7.42 on the MyST child speech dataset, 2.99 on the PFSTAR dataset and 12.47 on the CMU KIDS dataset as compared to any other previous methods. Our models outperformed the wav2vec2 BASE 960 on child speech which is considered a state-of-the-art ASR model on adult speech by just using 10 hours of child speech data in finetuning. The analysis of different types of training data and their effect on inference is also provided by using a combination of datasets in pretraining, finetuning and inference.",2022,5/29/2025 9:45,5/29/2025 12:59,7/19/2024 22:12,,,,,,,"arXiv.org perpetual, non-exclusive license",Semantic Scholar,Publisher: arXiv Version Number: 3,,C:\Users\damru\Zotero\storage\3SJUU4B7\Jain et al. - 2023 - A WAV2VEC2-Based Experimental Study on Self-Superv.pdf; ; ; ; ,https://ieeexplore.ieee.org/ielx7/6287639/10005208/10122501.pdf; https://www.semanticscholar.org/paper/A-WAV2VEC2-Based-Experimental-Study-on-Learning-to-Jain-Barcovschi/1784f193a4e15dd21ff4ae40334266bb0c3af7a9; https://www.semanticscholar.org/paper/A-WAV2VEC2-Based-Experimental-Study-on-Learning-to-Jain-Barcovschi/1784f193a4e15dd21ff4ae40334266bb0c3af7a9; https://www.semanticscholar.org/paper/Can-Self-Supervised-Learning-solve-the-problem-of-Jain-Yiwere/52e7b2074278477b276cdf11433d08335e25fb9e,,"Audio and Speech Processing (eess.AS); FOS: Computer and information sciences; FOS: Electrical engineering, electronic engineering, information engineering; Sound (cs.SD)"
4I5LJNGD,conferencePaper,2024,"Li, Jinpeng; Pu, Yu; Sun, Qi; Zhang, Wei-Qiang",Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text,Interspeech 2024,,,10.21437/Interspeech.2024-1790,https://www.isca-archive.org/interspeech_2024/li24ia_interspeech.html,"Whisper and other large-scale automatic speech recognition models have made significant progress in performance. However, their performance on many low-resource languages, such as Kazakh, is not satisfactory. It is worth researching how to utilize low-cost data to improve the performance of Whisper on under-represented languages. In this study, we utilized easily accessible unpaired speech and text data and combined the language model GPT with Whisper on Kazakh. We implemented end of transcript (EOT) judgment modification and hallucination penalty to improve the performance of speech recognition. Further, we employed the decoding average token log probability as a criterion to select samples from unlabeled speech data and used pseudo-labeled data to fine-tune the model to further improve its performance. Ultimately, we achieved more than 10% absolute WER reduction in multiple experiments, and the whole process has the potential to be generalized to other underrepresented languages.",9/1/2024,5/29/2025 11:30,5/29/2025 11:30,5/29/2025 11:30,2514-2518,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\PHMC5XQB\Li et al. - 2024 - Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaire.pdf,,,
5S98NXCN,preprint,2024,"Ferraz, Thomas Palmeira; Boito, Marcely Zanon; Brun, Caroline; Nikoulina, Vassilina",Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts,,,,10.48550/arXiv.2311.01070,http://arxiv.org/abs/2311.01070,"Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.",3/12/2024,5/29/2025 11:30,5/29/2025 11:30,5/29/2025 11:30,,,,,arXiv,en,,arXiv.org,arXiv:2311.01070 [cs],,C:\Users\damru\Zotero\storage\6NNWMTYH\Ferraz et al. - 2024 - Multilingual DistilWhisper Efficient Distillation of Multi-task Speech Models via Language-Specific.pdf,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
4CRJSR2A,preprint,2020,"Chen, Guoguo; Na, Xingyu; Wang, Yongqing; Yan, Zhiyong; Zhang, Junbo; Ma, Sifan; Wang, Yujun","Data Augmentation For Children's Speech Recognition -- The ""Ethiopian"" System For The SLT 2021 Children Speech Recognition Challenge",,,,10.48550/arXiv.2011.04547,http://arxiv.org/abs/2011.04547,"This paper presents the “Ethiopian” system for the SLT 2021 Children Speech Recognition Challenge. Various data processing and augmentation techniques are proposed to tackle children’s speech recognition problem, especially the lack of the children’s speech recognition training data issue. Detailed experiments are designed and conducted to show the effectiveness of each technique, across different speech recognition toolkits and model architectures. Step by step, we explain how we come up with our ﬁnal system, which provides the state-of-the-art results in the SLT 2021 Children Speech Recognition Challenge, with 21.66% CER on the Track 1 evaluation set (4th place overall), and 16.53% CER on the Track 2 evaluation set (1st place overall). Post-challenge analysis shows that our system actually achieves 18.82% CER on the Track 1 evaluation set, but we submitted the wrong version to the challenge organizer for Track 1.",11/9/2020,5/29/2025 11:31,5/29/2025 11:31,5/29/2025 11:31,,,,,arXiv,en,,arXiv.org,arXiv:2011.04547 [cs],,C:\Users\damru\Zotero\storage\75NQ996J\Chen et al. - 2020 - Data Augmentation For Children's Speech Recognition -- The Ethiopian System For The SLT 2021 Child.pdf,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing
LBAF3R2G,journalArticle,2014,"Duarte, Joana",Language development in mono- and multilingual children: A longitudinal approach,Diskurs Kindheits- und Jugendforschung,,,10.3224/DISKURS.V9I3.16624,https://www.academia.edu/80730786/Language_development_in_mono_and_multilingual_children_A_longitudinal_approach,Both theoretical considerations and empirical results have emphasized the need for longitudinal data in order to gain a more fine-grained insight into the processes of language acquisition and development. Based on data from a two-wave study on the,1/1/2014,5/29/2025 16:01,5/29/2025 16:01,5/29/2025 16:01,,,,,,,,www.academia.edu,,,C:\Users\damru\Zotero\storage\J7EDVHNG\Language_development_in_mono_and_multilingual_children_A_longitudinal_approach.html; C:\Users\damru\Zotero\storage\M6EDJG6Z\Duarte - 2014 - Language development in mono- and multilingual children A longitudinal approach.pdf,,,
6NWR3H9Q,journalArticle,2024,"Attia, Ahmed Adel; Liu, Jing; Ai, Wei; Demszky, Dorottya; Espy-Wilson, Carol",Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults,"Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society",,3065-8365,10.1609/aies.v7i1.31618,https://ojs.aaai.org/index.php/AIES/article/view/31618,"Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn’t readily extend to ASR for children due to the lim- ited availability of suitable child-specific databases and the distinct characteristics of children’s speech. A recent study investigated leveraging the My Science Tutor (MyST) chil- dren’s speech corpus to enhance Whisper’s performance in recognizing children’s speech. They were able to demon- strate some improvement on a limited testset. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We reduce the Word Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be generalized to unseen datasets. We also highlight important challenges towards improving children’s ASR performance and the effect of fine-tuning in improving the transcription of disfluent speech.",10/16/2024,5/29/2025 16:43,5/29/2025 16:43,5/29/2025 16:43,74-80,1,7,,,en,Copyright (c) 2024 Association for the Advancement of Artificial Intelligence,ojs.aaai.org,Number: 1,,C:\Users\damru\Zotero\storage\A8IBXJC3\Attia et al. - 2024 - Kid-Whisper Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. A.pdf,,,
AQCM8ZQ2,conferencePaper,2009,"Barnard, Etienne; Davel, Marelie; Heerden, Charl Van",ASR corpus design for resource-scarce languages,Interspeech 2009,,,10.21437/Interspeech.2009-727,https://www.isca-archive.org/interspeech_2009/barnard09_interspeech.html,"We investigate the number of speakers and the amount of data that is required for the development of useable speakerindependent speech-recognition systems in resource-scarce languages. Our experiments employ the Lwazi corpus, which contains speech in the eleven ofﬁcial languages of South Africa. We ﬁnd that a surprisingly small number of speakers (fewer than 50) and around 10 to 20 hours of speech per language are sufﬁcient for the purposes of acceptable phone-based recognition.",9/6/2009,5/29/2025 16:45,5/29/2025 16:45,5/29/2025 16:45,2847-2850,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\AZV4EP9H\Barnard et al. - 2009 - ASR corpus design for resource-scarce languages.pdf,,,
GLXEK57X,webpage,2021,,(PDF) Towards inclusive automatic speech recognition,ResearchGate,,,,https://www.researchgate.net/publication/374063854_Towards_inclusive_automatic_speech_recognition,"ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.",3/28/2021,5/29/2025 16:48,5/29/2025 16:48,5/29/2025 16:48,,,,,,en,,,,,,,,
KUXSGJZ9,journalArticle,2023,"Feng, Siyuan; Halpern, Bence Mark; Kudina, Olya; Scharenborg, Odette",Towards inclusive automatic speech recognition,Computer Speech and Language,,0885-2308,10.1016/j.csl.2023.101567,http://www.scopus.com/inward/record.url?scp=85171763233&partnerID=8YFLogxK,"Practice and recent evidence show that state-of-the-art (SotA) automatic speech recognition (ASR) systems do not perform equally well for all speaker groups. Many factors can cause this bias against different speaker groups. This paper, for the first time, systematically quantifies and finds speech recognition bias against gender, age, regional accents and non-native accents, and investigates the origin of this bias by investigating bias cross-lingually (i.e., Dutch and Mandarin) and for two different SotA ASR architectures (a hybrid DNN-HMM and an attention based end-to-end (E2E) model) through a phoneme error analysis. The results show that only a fraction of the bias can be explained by pronunciation differences between speaker groups, and that in order to mitigate bias, language- and architecture specific solutions need to be found.",2023,5/29/2025 16:48,5/29/2025 16:48,5/29/2025 16:48,,101567,84,,,,,TU Delft Research Portal,,,C:\Users\damru\Zotero\storage\MUGTEXUC\Feng et al. - 2023 - Towards inclusive automatic speech recognition.pdf,,,Accent; Age; Bias; Gender; Inclusive automatic speech recognition
624KLRXU,conferencePaper,2024,"Ghimire, Rupak Raj; Poudyal, Prakash; Bal, Bal Krishna",Improving on the Limitations of the ASR Model in Low-Resourced Environments Using Parameter-Efficient Fine-Tuning,Proceedings of the 21st International Conference on Natural Language Processing (ICON),,,,https://aclanthology.org/2024.icon-1.47/,"Modern general-purpose speech recognition systems are more robust in languages with high resources. In contrast, achieving state-of-the-art accuracy for low-resource languages is still challenging. The fine-tuning of the pre-trained model is one of the highly popular practices which utilizes the existing information while efficiently learning from a small amount of data to enhance the precision and robustness of speech recognition tasks. This work attempts to diagnose the performance of a pre-trained model when transcribing the audio from the low-resource language. In this work, we apply an adapter-based iterative parameter-efficient fine-tuning strategy on a limited dataset aiming to improve the quality of the transcription of a previously fine-tuned model. For the experiment we used Whisper's multilingual pre-trained speech model and Nepali as a test language. Using this approach we achieved Word Error Rate of 27.9%,which is more than 19% improvement over pre-trained Whisper Large \ensuremath- V2.",2024-12,5/29/2025 16:49,5/29/2025 16:49,5/29/2025 16:49,408–415,,,,NLP Association of India (NLPAI),,,ACLWeb,,,C:\Users\damru\Zotero\storage\FF9QXPCX\Ghimire et al. - 2024 - Improving on the Limitations of the ASR Model in Low-Resourced Environments Using Parameter-Efficien.pdf,,,
MY3EFIF4,conferencePaper,2024,"Graave, Thomas; Li, Zhengyang; Lohrenz, Timo; Fingscheidt, Tim",Mixed Children/Adult/Childrenized Fine-Tuning for Children’s ASR: How to Reduce Age Mismatch and Speaking Style Mismatch,Interspeech 2024,,,10.21437/Interspeech.2024-499,https://www.isca-archive.org/interspeech_2024/graave24_interspeech.html,"Today’s end-to-end (E2E) ASR models achieve strong performance when applied to adult speech, but deteriorate on children’s speech. Most E2E ASR models are pre-trained on adult speech, which introduces an age mismatch that can be addressed by finetuning on child data. However, due to limited availability of child datasets, fine-tuning on children’s speech may introduce new domain shifts such as speaking style mismatch. In this work, we explore mixed fine-tuning on partially matched data, namely read adult speech and spontaneous children’s speech, to improve the performance of E2E ASR on read children’s speech. We isolate the individual impact of age mismatch and speaking style mismatch and investigate the use of childrenization of read adult speech. Our proposed method reduces the WER by up to 5% absolute (21% relative) compared to the pre-trained E2E ASR and by roughly 3% absolute (15% relative) compared to individual fine-tuning on partially matched datasets.",9/1/2024,5/30/2025 7:12,5/30/2025 7:12,5/30/2025 7:12,5188-5192,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\KCTSRTVF\Graave et al. - 2024 - Mixed ChildrenAdultChildrenized Fine-Tuning for Children’s ASR How to Reduce Age Mismatch and Spe.pdf,,,
A3DA27VB,preprint,2020,"Huang, Jocelyn; Kuchaiev, Oleksii; O'Neill, Patrick; Lavrukhin, Vitaly; Li, Jason; Flores, Adriana; Kucsko, Georg; Ginsburg, Boris","Cross-Language Transfer Learning, Continuous Learning, and Domain Adaptation for End-to-End Automatic Speech Recognition",,,,10.48550/arXiv.2005.04290,http://arxiv.org/abs/2005.04290,"In this paper, we demonstrate the efficacy of transfer learning and continuous learning for various automatic speech recognition (ASR) tasks. We start with a pre-trained English ASR model and show that transfer learning can be effectively and easily performed on: (1) different English accents, (2) different languages (German, Spanish and Russian) and (3) application-specific domains. Our experiments demonstrate that in all three cases, transfer learning from a good base model has higher accuracy than a model trained from scratch. It is preferred to fine-tune large models than small pre-trained models, even if the dataset for fine-tuning is small. Moreover, transfer learning significantly speeds up convergence for both very small and very large target datasets.",5/8/2020,5/30/2025 7:14,5/30/2025 7:14,5/30/2025 7:14,,,,,arXiv,,,arXiv.org,arXiv:2005.04290 [eess],,"C:\Users\damru\Zotero\storage\KFT3PA9M\Huang et al. - 2020 - Cross-Language Transfer Learning, Continuous Learning, and Domain Adaptation for End-to-End Automati.pdf; C:\Users\damru\Zotero\storage\W2TSHFKS\2005.html",,,Electrical Engineering and Systems Science - Audio and Speech Processing
PVXXEMP4,conferencePaper,2023,"Jain, Rishabh; Barcovschi, Andrei; Yiwere, Mariam; Corcoran, Peter; Cucu, Horia",Adaptation of Whisper models to child speech recognition,INTERSPEECH 2023,,,10.21437/Interspeech.2023-935,https://www.isca-archive.org/interspeech_2023/jain23_interspeech.html,"Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non-finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.",8/20/2023,5/30/2025 7:15,5/30/2025 7:15,5/30/2025 7:15,5242-5246,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\5UTB562N\Jain et al. - 2023 - Adaptation of Whisper models to child speech recognition.pdf,,,
ENZRWEZV,conferencePaper,1997,"Lee, Sungbok; Potamianos, Alexandros; Narayanan, Shrikanth","Analysis of children's speech: duration, pitch and formants",5th European Conference on Speech Communication and Technology (Eurospeech 1997),,,10.21437/Eurospeech.1997-161,https://www.isca-archive.org/eurospeech_1997/lee97b_eurospeech.html,,9/22/1997,5/30/2025 7:19,5/30/2025 7:19,5/30/2025 7:19,473-476,,,,ISCA,en,,DOI.org (Crossref),,,"C:\Users\damru\Zotero\storage\H8AB8JV4\Lee et al. - 1997 - Analysis of children's speech duration, pitch and formants.pdf",,,
XNRJ5MSN,journalArticle,,"Lee, Sungbok; Potamianos, Alexandros; Narayanan, Shrikanth",Acoustics of children’s speech: Developmental changes of temporal and spectral parametersa),,,,,,,,5/30/2025 7:20,5/30/2025 7:20,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\WIB48XQA\Lee et al. - Acoustics of children’s speech Developmental changes of temporal and spectral parametersa).pdf,,,
KZWKVSWD,conferencePaper,2020,"Xu, Jin; Tan, Xu; Ren, Yi; Qin, Tao; Li, Jian; Zhao, Sheng; Liu, Tie-Yan",LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition,Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,978-1-4503-7998-4,,10.1145/3394486.3403331,https://dl.acm.org/doi/10.1145/3394486.3403331,"Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely lowresource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than 98% intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.",8/23/2020,5/30/2025 7:21,5/30/2025 7:21,5/30/2025 7:21,2802-2812,,,,ACM,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\25RFZCHU\Xu et al. - 2020 - LRSpeech Extremely Low-Resource Speech Synthesis and Recognition.pdf,,,
6LDV77TN,journalArticle,2024,"Liu, Yunpeng; Yang, Xukui; Qu, Dan",Exploration of Whisper fine-tuning strategies for low-resource ASR,"EURASIP Journal on Audio, Speech, and Music Processing",,1687-4722,10.1186/s13636-024-00349-3,https://doi.org/10.1186/s13636-024-00349-3,"Limited data availability remains a significant challenge for Whisper’s low-resource speech recognition performance, falling short of practical application requirements. While previous studies have successfully reduced the recognition error rates of target language speech through fine-tuning, a comprehensive exploration and analysis of Whisper’s fine-tuning capabilities and the advantages and disadvantages of various fine-tuning strategies are still lacking. This paper aims to fill this gap by conducting comprehensive experimental exploration for Whisper’s low-resource speech recognition performance using five fine-tuning strategies with limited supervised data from seven low-resource languages. The results and analysis demonstrate that all fine-tuning strategies explored in this paper significantly enhance Whisper’s performance. However, different strategies vary in their suitability and practical effectiveness, highlighting the need for careful selection based on specific use cases and resources available.",6/1/2024,5/30/2025 7:22,5/30/2025 7:22,5/30/2025 7:22,29,1,2024,"EURASIP Journal on Audio, Speech, and Music Processing",,,,BioMed Central,,,C:\Users\damru\Zotero\storage\3MVHG2KJ\Liu et al. - 2024 - Exploration of Whisper fine-tuning strategies for low-resource ASR.pdf; C:\Users\damru\Zotero\storage\5URY7RH6\s13636-024-00349-3.html,,,ASR; Fine-tune; Low-resource; Whisper
5626ZETR,preprint,2022,"Lu, Renee; Shahin, Mostafa; Ahmed, Beena",Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations,,,,10.48550/arXiv.2211.07769,http://arxiv.org/abs/2211.07769,"Children's speech recognition is a vital, yet largely overlooked domain when building inclusive speech technologies. The major challenge impeding progress in this domain is the lack of adequate child speech corpora; however, recent advances in self-supervised learning have created a new opportunity for overcoming this problem of data scarcity. In this paper, we leverage self-supervised adult speech representations and use three well-known child speech corpora to build models for children's speech recognition. We assess the performance of fine-tuning on both native and non-native children's speech, examine the effect of cross-domain child corpora, and investigate the minimum amount of child speech required to fine-tune a model which outperforms a state-of-the-art adult model. We also analyze speech recognition performance across children's ages. Our results demonstrate that fine-tuning with cross-domain child corpora leads to relative improvements of up to 46.08% and 45.53% for native and non-native child speech respectively, and absolute improvements of 14.70% and 31.10%. We also show that with as little as 5 hours of transcribed children's speech, it is possible to fine-tune a children's speech recognition system that outperforms a state-of-the-art adult model fine-tuned on 960 hours of adult speech.",11/14/2022,5/30/2025 7:23,5/30/2025 7:23,5/30/2025 7:23,,,,,arXiv,,,arXiv.org,arXiv:2211.07769 [cs],,C:\Users\damru\Zotero\storage\JK4B6RDZ\Lu et al. - 2022 - Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations.pdf; C:\Users\damru\Zotero\storage\TQ7SBTHD\2211.html,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
TW7DLK5W,preprint,2025,"Singh, Vishwanath Pratap; Sahidullah, Md; Kinnunen, Tomi","Causal Analysis of Asr Errors for Children: Quantifying the Impact of Physiological, Cognitive, and Extrinsic Factors",,,,10.2139/ssrn.5125557,https://papers.ssrn.com/abstract=5125557,"The increasing use of children’s automatic speech recognition (ASR) systems has spurred research efforts to improve the accuracy of models designed for children’s speech in recent years. The current approach utilizes either open-source speech foundation models (SFMs) directly or fine-tuning them with children’s speech data. These SFMs, whether open-source or fine-tuned for children, often exhibit higher word error rates (WERs) compared to adult speech. However, there is a lack of systemic analysis of the cause of this degraded performance of SFMs. Understanding and addressing the reasons behind this performance disparity is crucial for improving the accuracy of SFMs for children’s speech. Our study addresses this gap by investigating the causes of accuracy degradation and the primary contributors to WER in children’s speech. In the first part of the study, we conduct a comprehensive benchmarking study on two self-supervised SFMs (Wav2Vec 2.0 and Hubert) and two weakly supervised SFMs (Whisper and MMS) across various age groups on two children speech corpora, establishing the raw data for the causal inference analysis in the second part. In the second part of the study, we analyze the impact of physiological factors (age, gender), cognitive factors (pronunciation ability), and external factors (vocabulary difficulty, background noise, and word count) on SFM accuracy in children’s speech using causal inference. The results indicate that physiology (age) and particular external factor (number of words in audio) have the highest impact on accuracy, followed by background noise and pronunciation ability. Fine-tuning SFMs on children’s speech reduces sensitivity to physiological and cognitive factors, while sensitivity to the number of words in audio persists.",2/5/2025,5/30/2025 7:27,5/30/2025 7:27,5/30/2025 7:27,,,,,Social Science Research Network,en,,papers.ssrn.com,,,"C:\Users\damru\Zotero\storage\FXAZKILL\Singh et al. - 2025 - Causal Analysis of Asr Errors for Children Quantifying the Impact of Physiological, Cognitive, and.pdf",,,Causal Inference; Children's ASR; Cognition; Physiology; Pronunciation; Speech Foundational Models
388RQSZE,preprint,2025,"Shankar, Natarajan Balaji; Wang, Zilai; Eren, Eray; Alwan, Abeer",Selective Attention Merging for low resource tasks: A case study of Child ASR,,,,10.48550/arXiv.2501.08468,http://arxiv.org/abs/2501.08468,"While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data. To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora. This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks. Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques. By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR.",1/14/2025,5/30/2025 7:28,5/30/2025 7:28,5/30/2025 7:28,,,,,arXiv,,,arXiv.org,arXiv:2501.08468 [cs],,C:\Users\damru\Zotero\storage\PQAQD4KS\Shankar et al. - 2025 - Selective Attention Merging for low resource tasks A case study of Child ASR.pdf; C:\Users\damru\Zotero\storage\96I8C4TS\2501.html,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing; Computer Science - Computation and Language
ZT2WBRT2,journalArticle,2024,"Patel, Tanvina; Scharenborg, Odette",Improving End-to-End Models for Children’s Speech Recognition,Applied Sciences,,2076-3417,10.3390/app14062353,https://www.mdpi.com/2076-3417/14/6/2353,"Children’s Speech Recognition (CSR) is a challenging task due to the high variability in children’s speech patterns and limited amount of available annotated children’s speech data. We aim to improve CSR in the often-occurring scenario that no children’s speech data is available for training the Automatic Speech Recognition (ASR) systems. Traditionally, Vocal Tract Length Normalization (VTLN) has been widely used in hybrid ASR systems to address acoustic mismatch and variability in children’s speech when training models on adults’ speech. Meanwhile, End-to-End (E2E) systems often use data augmentation methods to create child-like speech from adults’ speech. For adult speech-trained ASRs, we investigate the effectiveness of augmentation methods; speed perturbations and spectral augmentation, along with VTLN, in an E2E framework for the CSR task, comparing these across Dutch, German, and Mandarin. We applied VTLN at different stages (training/test) of the ASR and conducted age and gender analyses. Our experiments showed highly similar patterns across the languages: Speed Perturbations and Spectral Augmentation yield significant performance improvements, while VTLN provided further improvements while maintaining recognition performance on adults’ speech (depending on when it is applied). Additionally, VTLN showed performance improvement for both male and female speakers and was particularly effective for younger children.",3/11/2024,5/30/2025 7:29,5/30/2025 7:29,5/30/2025 7:29,2353,6,14,Applied Sciences,,en,https://creativecommons.org/licenses/by/4.0/,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\LTKEVKMN\Patel and Scharenborg - 2024 - Improving End-to-End Models for Children’s Speech Recognition.pdf,,,
2NVPU2AX,conferencePaper,1997,"Potamianos, Alexandros; Narayanan, Shrikanth; Lee, Sungbok",Automatic speech recognition for children,5th European Conference on Speech Communication and Technology (Eurospeech 1997),,,10.21437/Eurospeech.1997-623,https://www.isca-archive.org/eurospeech_1997/potamianos97b_eurospeech.html,"In this paper, the acoustic and linguistic characteristics of children speech are investigated in the context of automatic speech recognition. Acoustic variability is identi ed as a major hurdle in building high performance ASR applications for children. A simple speaker normalization algorithm combining frequency warping and spectral shaping introduced in [5] is shown to reduce acoustic variability and signi cantly improve recognition performance for children speakers (by 25{ 45%). Age-dependent acoustic modeling further reduces word error rate by 10%. Piece-wise linear and phoneme-dependent frequency warping algorithms are proposed for reducing acoustic mismatch between the children and adult acoustic spaces.",9/22/1997,5/30/2025 7:29,5/30/2025 7:29,5/30/2025 7:29,2371-2374,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\3XXINQAT\Potamianos et al. - 1997 - Automatic speech recognition for children.pdf,,,
4UI5DU6G,journalArticle,,"Shivakumar, Prashanth Gurunath; Potamianos, Alexandros; Lee, Sungbok; Narayanan, Shrikanth",Improving Speech Recognition for Children using Acoustic Adaptation and Pronunciation Modeling,,,,,,"Developing a robust Automatic Speech Recognition (ASR) system for children is a challenging task because of increased variability in acoustic and linguistic correlates as function of young age. The acoustic variability is mainly due to the developmental changes associated with vocal tract growth. On the linguistic side, the variability is associated with limited knowledge of vocabulary, pronunciations and other linguistic constructs. This paper presents a preliminary study towards better acoustic modeling, pronunciation modeling and front-end processing for children’s speech. Results are presented as a function of age. Speaker adaptation signiﬁcantly reduces mismatch and variability improving recognition results across age groups. In addition, introduction of pronunciation modeling shows promising performance improvements.",,5/30/2025 7:31,5/30/2025 7:31,,,,,,,en,,Zotero,,,C:\Users\damru\Zotero\storage\H49QZP7K\Shivakumar et al. - Improving Speech Recognition for Children using Acoustic Adaptation and Pronunciation Modeling.pdf,,,
F6S8U4CX,journalArticle,2003,"Potamianos, A.; Narayanan, S.",Robust recognition of children's speech,IEEE Transactions on Speech and Audio Processing,,1063-6676,10.1109/TSA.2003.818026,http://ieeexplore.ieee.org/document/1255448/,"Developmental changes in speech production introduce age-dependent spectral and temporal variability in the speech signal produced by children. Such variabilities pose challenges for robust automatic recognition of children’s speech. Through an analysis of age-related acoustic characteristics of children’s speech in the context of automatic speech recognition (ASR), effects such as frequency scaling of spectral envelope parameters are demonstrated. Recognition experiments using acoustic models trained from adult speech and tested against speech from children of various ages clearly show performance degradation with decreasing age. On average, the word error rates are two to five times worse for children speech than for adult speech. Various techniques for improving ASR performance on children’s speech are reported. A speaker normalization algorithm that combines frequency warping and model transformation is shown to reduce acoustic variability and significantly improve ASR performance for children speakers (by 25–45% under various model training and testing conditions). The use of age-dependent acoustic models further reduces word error rate by 10%. The potential of using piece-wise linear and phoneme-dependent frequency warping algorithms for reducing the variability in the acoustic feature space of children is also investigated.",2003-11,5/30/2025 7:31,5/30/2025 7:31,5/30/2025 7:31,603-616,6,11,IEEE Trans. Speech Audio Process.,,en,https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\CYLUYAEK\Potamianos and Narayanan - 2003 - Robust recognition of children's speech.pdf,,,
LBP4JMBU,conferencePaper,2019,"Shor, Joel; Emanuel, Dotan; Lang, Oran; Tuval, Omry; Brenner, Michael; Cattiau, Julie; Vieira, Fernando; McNally, Maeve; Charbonneau, Taylor; Nollstadt, Melissa; Hassidim, Avinatan; Matias, Yossi",Personalizing ASR for Dysarthric and Accented Speech with Limited Data,Interspeech 2019,,,10.21437/Interspeech.2019-1427,https://www.isca-archive.org/interspeech_2019/shor19_interspeech.html,"Automatic speech recognition (ASR) systems have dramatically improved over the last few years. ASR systems are most often trained from ‘typical’ speech, which means that underrepresented groups don’t experience the same level of improvement. In this paper, we present and evaluate ﬁnetuning techniques to improve ASR for users with non-standard speech. We focus on two types of non-standard speech: speech from people with amyotrophic lateral sclerosis (ALS) and accented speech. We train personalized models that achieve 62% and 35% relative WER improvement on these two groups, bringing the absolute WER for ALS speakers, on a test set of message bank phrases, down to 10% for mild dysarthria and 20% for more serious dysarthria. We show that 71% of the improvement comes from only 5 minutes of training data. Finetuning a particular subset of layers (with many fewer parameters) often gives better results than ﬁnetuning the entire model. This is the ﬁrst step towards building state of the art ASR models for dysarthric speech.",9/15/2019,5/30/2025 7:33,5/30/2025 7:33,5/30/2025 7:33,784-788,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\I393Q7WX\Shor et al. - 2019 - Personalizing ASR for Dysarthric and Accented Speech with Limited Data.pdf,,,
25HDU7QK,conferencePaper,2023,"Zhao, Shuyang; Singh, Mittul; Woubie, Abraham; Karhila, Reima",Data augmentation for children ASR and child-adult speaker classification using voice conversion methods,INTERSPEECH 2023,,,10.21437/Interspeech.2023-702,https://www.isca-archive.org/interspeech_2023/zhao23c_interspeech.html,"Many young children prefer speech based interfaces over text, as they are relatively slow and error-prone with text input. However, children ASR can be challenging due to the lack of transcribed children speech corpora. In this paper, we investigate a voice conversion method based on WORLD vocoder to generate childlike speech for data augmentation. Since noise may lead to severe artifacts in converted speech, we also investigate using speech enhancement to improve the quality of converted speech. On a publicly available children speech corpus, we evaluated the performance of the proposed data augmentation method against existing data augmentation methods based on linear prediction coefficients. Our proposed data augmentation method substantially outperformed the prior work on children ASR. Additionally, on a task to classify the speaker, adult or child, data generated using our proposed method was shown to mimic real children better compared to the reference methods.",8/20/2023,5/30/2025 7:34,5/30/2025 7:34,5/30/2025 7:34,4593-4597,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\9EC3NRSI\Zhao et al. - 2023 - Data augmentation for children ASR and child-adult speaker classification using voice conversion met.pdf,,,
F4DV5HLE,conferencePaper,2022,"Thienpondt, Jenthe; Demuynck, Kris",Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping,Interspeech 2022,,,10.21437/Interspeech.2022-10964,https://www.isca-archive.org/interspeech_2022/thienpondt22_interspeech.html,"Automatic Speech Recognition (ASR) systems are known to exhibit difficulties when transcribing children’s speech. This can mainly be attributed to the absence of large children’s speech corpora to train robust ASR models and the resulting domain mismatch when decoding children’s speech with systems trained on adult data. In this paper, we propose multiple enhancements to alleviate these issues. First, we propose a data augmentation technique based on the source-filter model of speech to close the domain gap between adult and children’s speech. This enables us to leverage the data availability of adult speech corpora by making these samples perceptually similar to children’s speech. Second, using this augmentation strategy, we apply transfer learning on a Transformer model pre-trained on adult data. This model follows the recently introduced XLS-R architecture, a wav2vec 2.0 model pre-trained on several crosslingual adult speech corpora to learn general and robust acoustic frame-level representations. Adopting this model for the ASR task using adult data augmented with the proposed source-filter warping strategy and a limited amount of in-domain children’s speech significantly outperforms previous state-of-the-art results on the PF-STAR British English Children’s Speech corpus with a 4.86% WER on the official test set.",9/18/2022,5/30/2025 7:34,5/30/2025 7:34,5/30/2025 7:34,2213-2217,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\TFJZ27LP\Thienpondt and Demuynck - 2022 - Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter.pdf,,,
E3ITEB56,preprint,2024,"Singh, Vishwanath Pratap; Sahidullah, Md; Kinnunen, Tomi",ChildAugment: Data Augmentation Methods for Zero-Resource Children's Speaker Verification,,,,10.48550/arXiv.2402.15214,http://arxiv.org/abs/2402.15214,"The accuracy of modern automatic speaker verification (ASV) systems, when trained exclusively on adult data, drops substantially when applied to children's speech. The scarcity of children's speech corpora hinders fine-tuning ASV systems for children's speech. Hence, there is a timely need to explore more effective ways of reusing adults' speech data. One promising approach is to align vocal-tract parameters between adults and children through children-specific data augmentation, referred here to as ChildAugment. Specifically, we modify the formant frequencies and formant bandwidths of adult speech to emulate children's speech. The modified spectra are used to train ECAPA-TDNN (emphasized channel attention, propagation, and aggregation in time-delay neural network) recognizer for children. We compare ChildAugment against various state-of-the-art data augmentation techniques for children's ASV. We also extensively compare different scoring methods, including cosine scoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural PLDA). We also propose a low-complexity weighted cosine score for extremely low-resource children ASV. Our findings on the CSLU kids corpus indicate that ChildAugment holds promise as a simple, acoustics-motivated approach, for improving state-of-the-art deep learning based ASV for children. We achieve up to 12.45% (boys) and 11.96% (girls) relative improvement over the baseline.",2/23/2024,5/30/2025 7:35,5/30/2025 7:35,5/30/2025 7:35,,,,,arXiv,,,arXiv.org,arXiv:2402.15214 [eess],,C:\Users\damru\Zotero\storage\N2JNXY77\Singh et al. - 2024 - ChildAugment Data Augmentation Methods for Zero-Resource Children's Speaker Verification.pdf; C:\Users\damru\Zotero\storage\YGK8ZBBJ\2402.html,,,Computer Science - Sound; Electrical Engineering and Systems Science - Audio and Speech Processing
JBU3PPSZ,journalArticle,2021,"Yeung, Gary; Fan, Ruchao; Alwan, Abeer",Fundamental frequency feature warping for frequency normalization and data augmentation in child automatic speech recognition,Speech Communication,,1676393,10.1016/j.specom.2021.08.002,https://linkinghub.elsevier.com/retrieve/pii/S0167639321000881,"Effective child automatic speech recognition (ASR) systems have become increasingly important due to the growing use of interactive technology. Due to the lack of publicly available child speech databases, young child ASR systems often rely on older child or adult speech for training data. However, there is a large acoustic mismatch between child and adult speech. This study proposes a novel fundamental frequency (𝑓𝑜)based frequency warping technique for both frequency normalization and data augmentation to combat this acoustic mismatch and address the lack of available child speech training data. The technique is inspired by the tonotopic distances between formants and 𝑓𝑜, developed to model human vowel perception. The tonotopic distances are reformulated as a linear relationship between 𝑓𝑜 and vowel formants on the Mel scale. This reformulation is verified using 𝑓𝑜 and formant measurements from child utterances. The relationship is further generalized such that the frequency warping technique only relies on two parameters. The LibriSpeech ASR corpus is used for training, and both the OGI Kids’ Speech and CMU Kids Corpora are used for both training and testing. A single word ASR experiment and a continuous read speech ASR experiment are performed to evaluate the 𝑓𝑜-based frequency normalization and data augmentation techniques. In the single word experiment, the system using 𝑓𝑜-based frequency normalization significantly improved over the baseline system with no normalization, with a relative improvement of up to 22.3%, when the mismatch between training and testing data was large. In the continuous speech experiment, the combination of 𝑓𝑜-based frequency normalization and data augmentation resulted in a relative improvement of 19.3% over the baseline. Additionally, in all experiments, the 𝑓𝑜-based techniques outperformed other techniques such as vocal tract length normalization (VTLN) or vocal tract length perturbation (VTLP). Results were validated using Gaussian mixture model (GMM), deep neural network (DNN), and bidirectional long–short term memory (BLSTM) acoustic models.",2021-12,5/30/2025 7:37,5/30/2025 7:37,5/30/2025 7:37,10-Jan,,135,Speech Communication,,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\YR2JNT78\Yeung et al. - 2021 - Fundamental frequency feature warping for frequency normalization and data augmentation in child aut.pdf,,,
6PXPW8TG,conferencePaper,2018,"Yeung, Gary; Alwan, Abeer",On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children,Interspeech 2018,,,10.21437/Interspeech.2018-2297,https://www.isca-archive.org/interspeech_2018/yeung18_interspeech.html,"Automatic speech recognition (ASR) systems for children have lagged behind in performance when compared to adult ASR. The exact problems and evaluation methods for child ASR have not yet been fully investigated. Recent work from the robotics community suggests that ASR for kindergarten speech is especially difﬁcult, even though this age group may beneﬁt most from voice-based educational and diagnostic tools. Our study focused on ASR performance for speciﬁc grade levels (K-10) using a word identiﬁcation task. Grade-speciﬁc ASR systems were evaluated, with particular attention placed on the evaluation of kindergarten-aged children (5-6 years old). Experiments included investigation of grade-speciﬁc interactions with triphone models using feature space maximum likelihood linear regression (fMLLR), vocal tract length normalization (VTLN), and subglottal resonance (SGR) normalization. Our results indicate that kindergarten ASR performs dramatically worse than even 1st grade ASR, likely due to large speech variability at that age. As such, ASR systems may require targeted evaluations on kindergarten speech rather than being evaluated under the guise of “child ASR.” Additionally, results show that systems trained in matched conditions on kindergarten speech may be less suitable than mismatched-grade training with 1st grade speech. Finally, we analyzed the phonetic errors made by the kindergarten ASR.",9/2/2018,5/30/2025 7:37,5/30/2025 7:37,5/30/2025 7:37,1661-1665,,,,ISCA,en,,DOI.org (Crossref),,,C:\Users\damru\Zotero\storage\QCTWQFTL\Yeung and Alwan - 2018 - On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children.pdf,,,
ZRPBEFD7,journalArticle,2024,"Feng, Siyuan; Halpern, Bence Mark; Kudina, Olya; Scharenborg, Odette",Towards inclusive automatic speech recognition,Computer Speech & Language,,0885-2308,10.1016/j.csl.2023.101567,https://www.sciencedirect.com/science/article/pii/S0885230823000864,"Practice and recent evidence show that state-of-the-art (SotA) automatic speech recognition (ASR) systems do not perform equally well for all speaker groups. Many factors can cause this bias against different speaker groups. This paper, for the first time, systematically quantifies and finds speech recognition bias against gender, age, regional accents and non-native accents, and investigates the origin of this bias by investigating bias cross-lingually (i.e., Dutch and Mandarin) and for two different SotA ASR architectures (a hybrid DNN-HMM and an attention based end-to-end (E2E) model) through a phoneme error analysis. The results show that only a fraction of the bias can be explained by pronunciation differences between speaker groups, and that in order to mitigate bias, language- and architecture specific solutions need to be found.",3/1/2024,5/30/2025 7:38,5/30/2025 7:40,5/30/2025 7:38,101567,,84,Computer Speech & Language,,,,ScienceDirect,,,C:\Users\damru\Zotero\storage\FQGNPSY6\Feng et al. - 2024 - Towards inclusive automatic speech recognition.pdf; C:\Users\damru\Zotero\storage\4PMGM995\S0885230823000864.html,,,Accent; Age; Bias; Gender; Inclusive automatic speech recognition
4JQIDRF8,journalArticle,2024,"Zeisler, F. A.",Reducing Bias in State-of-the-Art ASR Systems for Child Speech,,,,,https://repository.tudelft.nl/record/uuid:86f166d9-e13f-4084-a5bf-7a7632604b52,"Automatic Speech Recognition (ASR) systems have transformed human-machine interaction, yet they often struggle with child speech due to the unique vocal characteristics. This thesis investigates age and gender biases, focusing on enhancing the performance of state-of-the-art ASR model Whisper on child speech. Initial experiments reveal significant disparities in recognition accuracy across age groups and genders within child speech, highlighting the critical need for targeted improvements. The study uses Low-Rank Adaptation (LoRA) to finetune the model using four child-specific datasets, aiming to simultaneously enhance recognition performance and mitigate biases. Results demonstrate substantial reductions in Word Error Rates (WER) and biases after finetuning, showcasing the effectiveness of transfer learning in addressing demographic inequality. Gender biases decreased by 32.77% relative to their initial values, and age biases also improved, with a relative decrease of 27.52% after finetuning. This research showcases the potential of tailored approaches to advance ASR technology for low-resource user demographics, with implications for improving educational and assistive technologies.<br/><br/><b>Index Terms</b>: Automatic Speech Recognition, Child speech, Whisper ASR model, Age and gender biases, Low-Rank Adaptation, Transfer learning, Demographic disparities",2024,5/30/2025 7:39,5/30/2025 7:39,5/30/2025 7:39,,,,,,en,,repository.tudelft.nl,,,C:\Users\damru\Zotero\storage\8XQRCKCV\Zeisler - 2024 - Reducing Bias in State-of-the-Art ASR Systems for Child Speech.pdf,,,
