{
  "Like a bilingual baby: The advantage of visually grounding a bilingual language model": {
    "title": "Like a bilingual baby: The advantage of visually grounding a bilingual language model",
    "authors": [
      "Nguyen, Khai-Nguyen",
      "Tang, Zixin",
      "Mali, Ankur",
      "Kelly, Alex"
    ],
    "doi": null,
    "url": "http://arxiv.org/abs/2210.05487",
    "journal": null,
    "year": 2023,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2210.05487.pdf",
      "https://arxiv.org/abs/2210.05487"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Computation and Language"
    ],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "arXiv",
    "abstract": "Unlike most neural language models, humans learn language in a rich, multi-sensory and, often, multi-lingual environment. Current language models typically fail to fully capture the complexities of multilingual language use. We train an LSTM language model on images and captions in English and Spanish from MS-COCO-ES. We find that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity. However, we find no significant advantage of visual grounding for abstract words. Our results provide additional evidence of the advantages of visually grounded language models and point to the need for more naturalistic language data from multilingual speakers and multilingual datasets with perceptual grounding.",
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://arxiv.org/pdf/2210.05487.pdf",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": "2210.05487"
  },
  "Few-shot Learning with Multilingual Language Models": {
    "title": "Few-shot Learning with Multilingual Generative Language Models",
    "authors": [
      "Xi Victoria Lin",
      "Todor Mihaylov",
      "Mikel Artetxe",
      "Tianlu Wang",
      "Shuohui Chen",
      "Daniel Simig",
      "Myle Ott",
      "Naman Goyal",
      "Shruti Bhosale",
      "Jingfei Du",
      "Ramakanth Pasunuru",
      "Sam Shleifer",
      "Punit Singh Koura",
      "Vishrav Chaudhary",
      "Brian O’Horo",
      "Jeff Wang",
      "Luke Zettlemoyer",
      "Zornitsa Kozareva",
      "Mona Diab",
      "Veselin Stoyanov",
      "Xian Li"
    ],
    "doi": "10.18653/v1/2022.emnlp-main.616",
    "url": "https://doi.org/10.18653/v1/2022.emnlp-main.616",
    "journal": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    "year": 2022,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2112.10668.pdf",
      "https://arxiv.org/abs/2112.10668"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Artificial Intelligence",
      "Computer Science - Computation and Language"
    ],
    "pages": "9019-9052",
    "issue": null,
    "volume": null,
    "publisher": "Association for Computational Linguistics",
    "abstract": "Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4% absolute accuracy improvement in 0-shot settings and +9.4% in 4-shot settings) and natural language inference (+5.4% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples. Finally, we evaluate our models in social value tasks such as hate speech detection in five languages and find it has limitations similar to comparable sized GPT-3 models.",
    "publication_date": "2022-01-01T00:00:00",
    "citation_count": 296,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2112.10668",
    "docname": "lin2022fewshotlearningwith",
    "dockey": "1d6d68aa53569219",
    "citation": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Jan 2022. URL: https://doi.org/10.18653/v1/2022.emnlp-main.616, doi:10.18653/v1/2022.emnlp-main.616.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "lin2022fewshotlearningwith",
    "bibtex": "@article{lin2022fewshotlearningwith,\n    author = \"Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh and Chaudhary, Vishrav and O’Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and Stoyanov, Veselin and Li, Xian\",\n    title = \"Few-shot Learning with Multilingual Generative Language Models\",\n    year = \"2022\",\n    journal = \"Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing\",\n    pages = \"9019-9052\",\n    month = \"Jan\",\n    doi = \"10.18653/v1/2022.emnlp-main.616\",\n    url = \"https://doi.org/10.18653/v1/2022.emnlp-main.616\",\n    publisher = \"Association for Computational Linguistics\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.18653/v1/2022.emnlp-main.616",
    "doc_id": "1d6d68aa53569219",
    "formatted_citation": "Xi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu Wang, Shuohui Chen, Daniel Simig, Myle Ott, Naman Goyal, Shruti Bhosale, Jingfei Du, Ramakanth Pasunuru, Sam Shleifer, Punit Singh Koura, Vishrav Chaudhary, Brian O’Horo, Jeff Wang, Luke Zettlemoyer, Zornitsa Kozareva, Mona Diab, Veselin Stoyanov, and Xian Li. Few-shot learning with multilingual generative language models. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9019-9052, Jan 2022. URL: https://doi.org/10.18653/v1/2022.emnlp-main.616, doi:10.18653/v1/2022.emnlp-main.616. This article has 296 citations."
  },
  "MoLE : Mixture Of Language Experts For Multi-Lingual Automatic Speech Recognition": {
    "title": "MoLE : Mixture Of Language Experts For Multi-Lingual Automatic Speech Recognition",
    "authors": [
      "Yoohwan Kwon",
      "Soo-Whan Chung"
    ],
    "doi": "10.1109/icassp49357.2023.10096227",
    "url": "https://ieeexplore.ieee.org/document/10096227/",
    "journal": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2023,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.semanticscholar.org/paper/MoLE-%3A-Mixture-Of-Language-Experts-For-Automatic-Kwon-Chung/250373008e4441678006a6ba7dbb186449c7169f",
      "https://arxiv.org/pdf/2302.13750"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1-5",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "Multi-lingual speech recognition aims to distinguish linguistic expressions in different languages and integrate acoustic processing simultaneously. In contrast, current multilingual speech recognition research follows a language-aware paradigm, mainly targeted to improve recognition performance rather than discriminate language characteristics. In this paper, we present a multi-lingual speech recognition network named Mixture-of-Language-Experts (MoLE), which digests speech in a variety of languages. Specifically, MoLE analyzes linguistic expression from input speech in arbitrary languages, activating a language-specific expert with a lightweight language gating network. The gating network not only activates experts, but also estimates the reliability of the activation. Based on the reliability, the activated expert and the language-agnostic expert are aggregated to represent language-conditioned embedding for efficient speech recognition. Our proposed model is evaluated in 5 languages scenario, and the experimental results show that our structure is advantageous on multi-lingual recognition, especially for speech in low-resource language.",
    "publication_date": "2023-05-05T00:00:00",
    "citation_count": 18,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "kwon2023molemixture",
    "dockey": "34061d2463c24216",
    "citation": "Yoohwan Kwon and Soo-Whan Chung. Mole : mixture of language experts for multi-lingual automatic speech recognition. ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5, May 2023. URL: https://doi.org/10.1109/icassp49357.2023.10096227, doi:10.1109/icassp49357.2023.10096227.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "kwon2023molemixture",
    "bibtex": "@article{kwon2023molemixture,\n    author = \"Kwon, Yoohwan and Chung, Soo-Whan\",\n    title = \"MoLE : Mixture Of Language Experts For Multi-Lingual Automatic Speech Recognition\",\n    year = \"2023\",\n    journal = \"ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"1-5\",\n    month = \"May\",\n    doi = \"10.1109/icassp49357.2023.10096227\",\n    url = \"https://doi.org/10.1109/icassp49357.2023.10096227\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp49357.2023.10096227",
    "doc_id": "34061d2463c24216",
    "formatted_citation": "Yoohwan Kwon and Soo-Whan Chung. Mole : mixture of language experts for multi-lingual automatic speech recognition. ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 1-5, May 2023. URL: https://doi.org/10.1109/icassp49357.2023.10096227, doi:10.1109/icassp49357.2023.10096227. This article has 18 citations."
  },
  "Overview of the interspeech tlt2020 shared task onasr for non-native children’s speech": {
    "title": "Overview of the interspeech tlt2020 shared task onasr for non-native children’s speech",
    "authors": [
      "Gretter, Roberto",
      "Matassoni, Marco",
      "Falavigna, Giuseppe Daniele",
      "Keelan, Evanini",
      "Leong, Chee Wee"
    ],
    "doi": null,
    "url": "https://cris.fbk.eu/handle/11582/324906",
    "journal": "Proceedings of Interspeech 2020",
    "year": 2020,
    "genre": "bookSection",
    "link_attachments": [
      "https://cris.fbk.eu/bitstream/11582/324906/1/is2020_challenge.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "245–249",
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": null,
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://cris.fbk.eu/bitstream/11582/324906/1/is2020_challenge.pdf",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  },
  "Improved Children’s Automatic Speech Recognition Combining Adapters and Synthetic Data Augmentation": {
    "title": "Improved Children’s Automatic Speech Recognition Combining Adapters and Synthetic Data Augmentation",
    "authors": [
      "Thomas Rolland",
      "Alberto Abad"
    ],
    "doi": "10.1109/icassp48485.2024.10446889",
    "url": "https://doi.org/10.1109/icassp48485.2024.10446889",
    "journal": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2024,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "12757-12761",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2024-04-14T00:00:00",
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "rolland2024improvedchildren’sautomatic",
    "dockey": "a5542138973d8e4a",
    "citation": "Thomas Rolland and Alberto Abad. Improved children’s automatic speech recognition combining adapters and synthetic data augmentation. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12757-12761, Apr 2024. URL: https://doi.org/10.1109/icassp48485.2024.10446889, doi:10.1109/icassp48485.2024.10446889.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "rolland2024improvedchildren’sautomatic",
    "bibtex": "@article{rolland2024improvedchildren’sautomatic,\n    author = \"Rolland, Thomas and Abad, Alberto\",\n    title = \"Improved Children’s Automatic Speech Recognition Combining Adapters and Synthetic Data Augmentation\",\n    year = \"2024\",\n    journal = \"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"12757-12761\",\n    month = \"Apr\",\n    doi = \"10.1109/icassp48485.2024.10446889\",\n    url = \"https://doi.org/10.1109/icassp48485.2024.10446889\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp48485.2024.10446889",
    "doc_id": "a5542138973d8e4a",
    "formatted_citation": "Thomas Rolland and Alberto Abad. Improved children’s automatic speech recognition combining adapters and synthetic data augmentation. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12757-12761, Apr 2024. URL: https://doi.org/10.1109/icassp48485.2024.10446889, doi:10.1109/icassp48485.2024.10446889. This article has 2 citations."
  },
  "Masked-Speech Recognition for Linguistically Diverse Populations: A Focused Review and Suggestions for the Future": {
    "title": "Masked-Speech Recognition for Linguistically Diverse Populations: A Focused Review and Suggestions for the Future.",
    "authors": [
      "Tiana Cowan",
      "Caroline Paroby",
      "Lori J. Leibold",
      "E. Buss",
      "Barbara L. Rodríguez",
      "Lauren Calandruccio"
    ],
    "doi": "10.1044/2022_jslhr-22-00011",
    "url": "http://pubs.asha.org/doi/10.1044/2022_JSLHR-22-00011",
    "journal": "Journal of speech, language, and hearing research : JSLHR",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9911100/",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9911100/pdf/JSLHR-65-3195.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "\n1-22\n",
    "issue": "8",
    "volume": "65",
    "publisher": null,
    "abstract": "Purpose:               Twenty years ago, von Hapsburg and Peña (2002) wrote a tutorial that reviewed the literature on speech audiometry and bilingualism and outlined valuable recommendations to increase the rigor of the evidence base. This review article returns to that seminal tutorial to reflect on how that advice was applied over the last 20 years and to provide updated recommendations for future inquiry.                                         Method:               We conducted a focused review of the literature on masked-speech recognition for bilingual children and adults. First, we evaluated how studies published since 2002 described bilingual participants. Second, we reviewed the literature on native language masked-speech recognition. Third, we discussed theoretically motivated experimental work. Fourth, we outlined how recent research in bilingual speech recognition can be used to improve clinical practice.                                         Results:               Research conducted since 2002 commonly describes bilingual samples in terms of their language status, competency, and history. Bilingualism was not consistently associated with poor masked-speech recognition. For example, bilinguals who were exposed to English prior to age 7 years and who were dominant in English performed comparably to monolinguals for masked-sentence recognition tasks. To the best of our knowledge, there are no data to document the masked-speech recognition ability of these bilinguals in their other language compared to a second monolingual group, which is an important next step. Nonetheless, individual factors that commonly vary within bilingual populations were associated with masked-speech recognition and included language dominance, competency, and age of acquisition. We identified methodological issues in sampling strategies that could, in part, be responsible for inconsistent findings between studies. For instance, disparities in socioeconomic status (SES) between recruited bilingual and monolingual groups could cause confounding bias within the research design.                                         Conclusions:               Dimensions of the bilingual linguistic profile should be considered in clinical practice to inform counseling and (re)habilitation strategies since susceptibility to masking is elevated in at least one language for most bilinguals. Future research should continue to report language status, competency, and history but should also report language stability and demand for use data. In addition, potential confounds (e.g., SES, educational attainment) when making group comparisons between monolinguals and bilinguals must be considered.",
    "publication_date": null,
    "citation_count": 7,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "cowan2022maskedspeechrecognitionfor",
    "dockey": "e5c6a8b0dc809e72",
    "citation": "Tiana Cowan, Caroline Paroby, Lori J. Leibold, E. Buss, Barbara L. Rodríguez, and Lauren Calandruccio. Masked-speech recognition for linguistically diverse populations: a focused review and suggestions for the future. Journal of speech, language, and hearing research : JSLHR, pages 1-22, 2022. URL: https://doi.org/10.1044/2022\\_jslhr-22-00011, doi:10.1044/2022\\_jslhr-22-00011.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "cowan2022maskedspeechrecognitionfor",
    "bibtex": "@article{cowan2022maskedspeechrecognitionfor,\n    author = \"Cowan, Tiana and Paroby, Caroline and Leibold, Lori J. and Buss, E. and Rodríguez, Barbara L. and Calandruccio, Lauren\",\n    title = \"Masked-Speech Recognition for Linguistically Diverse Populations: A Focused Review and Suggestions for the Future.\",\n    year = \"2022\",\n    journal = \"Journal of speech, language, and hearing research : JSLHR\",\n    pages = \"\n1-22\n\",\n    doi = \"10.1044/2022\\_jslhr-22-00011\",\n    url = \"https://doi.org/10.1044/2022\\_jslhr-22-00011\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1044/2022_jslhr-22-00011",
    "doc_id": "e5c6a8b0dc809e72",
    "formatted_citation": "Tiana Cowan, Caroline Paroby, Lori J. Leibold, E. Buss, Barbara L. Rodríguez, and Lauren Calandruccio. Masked-speech recognition for linguistically diverse populations: a focused review and suggestions for the future. Journal of speech, language, and hearing research : JSLHR, pages 1-22, 2022. URL: https://doi.org/10.1044/2022\\_jslhr-22-00011, doi:10.1044/2022\\_jslhr-22-00011. This article has 7 citations."
  },
  "Development of robust automatic speech recognition system for children's using kaldi toolkit": {
    "title": "Development of Robust Automatic Speech Recognition System for Children's using Kaldi Toolkit",
    "authors": [
      "Vivek Bhardwaj",
      "Sashi Bala",
      "Virender Kadyan",
      "Vinay Kukreja"
    ],
    "doi": "10.1109/icirca48905.2020.9182941",
    "url": "https://doi.org/10.1109/icirca48905.2020.9182941",
    "journal": "2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)",
    "year": 2020,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.researchgate.net/profile/Vivek-Bhardwaj-4/publication/344056935_Development_of_Robust_Automatic_Speech_Recognition_System_for_Children%27s_using_Kaldi_Toolkit/links/5fc4f5f7458515b7978abb2f/Development-of-Robust-Automatic-Speech-Recognition-System-for-Childrens-using-Kaldi-Toolkit.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "10-13",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2020-07-01T00:00:00",
    "citation_count": 61,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "bhardwaj2020developmentofrobust",
    "dockey": "2f91186baa667a91",
    "citation": "Vivek Bhardwaj, Sashi Bala, Virender Kadyan, and Vinay Kukreja. Development of robust automatic speech recognition system for children's using kaldi toolkit. 2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA), pages 10-13, Jul 2020. URL: https://doi.org/10.1109/icirca48905.2020.9182941, doi:10.1109/icirca48905.2020.9182941.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "bhardwaj2020developmentofrobust",
    "bibtex": "@article{bhardwaj2020developmentofrobust,\n    author = \"Bhardwaj, Vivek and Bala, Sashi and Kadyan, Virender and Kukreja, Vinay\",\n    title = \"Development of Robust Automatic Speech Recognition System for Children's using Kaldi Toolkit\",\n    year = \"2020\",\n    journal = \"2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA)\",\n    pages = \"10-13\",\n    month = \"Jul\",\n    doi = \"10.1109/icirca48905.2020.9182941\",\n    url = \"https://doi.org/10.1109/icirca48905.2020.9182941\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icirca48905.2020.9182941",
    "doc_id": "2f91186baa667a91",
    "formatted_citation": "Vivek Bhardwaj, Sashi Bala, Virender Kadyan, and Vinay Kukreja. Development of robust automatic speech recognition system for children's using kaldi toolkit. 2020 Second International Conference on Inventive Research in Computing Applications (ICIRCA), pages 10-13, Jul 2020. URL: https://doi.org/10.1109/icirca48905.2020.9182941, doi:10.1109/icirca48905.2020.9182941. This article has 61 citations."
  },
  "Etlt 2021: Shared task on automatic speech recognition for non-native children’s speech": {
    "title": "2021 Reviewers List",
    "authors": [
      "Roberto Gomez-Garcia"
    ],
    "doi": "10.1109/lmwc.2022.3142465",
    "url": "https://doi.org/10.1109/lmwc.2022.3142465",
    "journal": "IEEE Microwave and Wireless Components Letters",
    "year": 2022,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.repository.cam.ac.uk/bitstreams/10893a3e-0b58-4597-81bd-4552e190c98d/download"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "187-192",
    "issue": "2",
    "volume": "32",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "abstract": null,
    "publication_date": "2022-02-01T00:00:00",
    "citation_count": 0,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "gomezgarcia20222021reviewerslist",
    "dockey": "e604471e6d2d5c3c",
    "citation": "Roberto Gomez-Garcia. 2021 reviewers list. IEEE Microwave and Wireless Components Letters, 32:187-192, Feb 2022. URL: https://doi.org/10.1109/lmwc.2022.3142465, doi:10.1109/lmwc.2022.3142465.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "gomezgarcia20222021reviewerslist",
    "bibtex": "@article{gomezgarcia20222021reviewerslist,\n    author = \"Gomez-Garcia, Roberto\",\n    title = \"2021 Reviewers List\",\n    year = \"2022\",\n    journal = \"IEEE Microwave and Wireless Components Letters\",\n    volume = \"32\",\n    pages = \"187-192\",\n    month = \"Feb\",\n    doi = \"10.1109/lmwc.2022.3142465\",\n    url = \"https://doi.org/10.1109/lmwc.2022.3142465\",\n    publisher = \"Institute of Electrical and Electronics Engineers (IEEE)\",\n    issue = \"2\",\n    issn = \"1531-1309\"\n}\n",
    "issn": "1531-1309",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/lmwc.2022.3142465",
    "doc_id": "e604471e6d2d5c3c",
    "formatted_citation": "Roberto Gomez-Garcia. 2021 reviewers list. IEEE Microwave and Wireless Components Letters, 32:187-192, Feb 2022. URL: https://doi.org/10.1109/lmwc.2022.3142465, doi:10.1109/lmwc.2022.3142465. This article has 0 citations."
  },
  "Multilingual phonetic dataset for low resource speech recognition": {
    "title": "Multilingual Phonetic Dataset for Low Resource Speech Recognition",
    "authors": [
      "Xinjian Li",
      "David R. Mortensen",
      "Florian Metze",
      "Alan W Black"
    ],
    "doi": "10.1109/icassp39728.2021.9413720",
    "url": "https://doi.org/10.1109/icassp39728.2021.9413720",
    "journal": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2021,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.cs.cmu.edu/~awb/papers/ICASSP21_Multilingual_Phonetic_Dataset.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "6958-6962",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2021-06-06T00:00:00",
    "citation_count": 13,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "li2021multilingualphoneticdataset",
    "dockey": "a6071ae800278854",
    "citation": "Xinjian Li, David R. Mortensen, Florian Metze, and Alan W Black. Multilingual phonetic dataset for low resource speech recognition. ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6958-6962, Jun 2021. URL: https://doi.org/10.1109/icassp39728.2021.9413720, doi:10.1109/icassp39728.2021.9413720.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "li2021multilingualphoneticdataset",
    "bibtex": "@article{li2021multilingualphoneticdataset,\n    author = \"Li, Xinjian and Mortensen, David R. and Metze, Florian and Black, Alan W\",\n    title = \"Multilingual Phonetic Dataset for Low Resource Speech Recognition\",\n    year = \"2021\",\n    journal = \"ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"6958-6962\",\n    month = \"Jun\",\n    doi = \"10.1109/icassp39728.2021.9413720\",\n    url = \"https://doi.org/10.1109/icassp39728.2021.9413720\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp39728.2021.9413720",
    "doc_id": "a6071ae800278854",
    "formatted_citation": "Xinjian Li, David R. Mortensen, Florian Metze, and Alan W Black. Multilingual phonetic dataset for low resource speech recognition. ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6958-6962, Jun 2021. URL: https://doi.org/10.1109/icassp39728.2021.9413720, doi:10.1109/icassp39728.2021.9413720. This article has 13 citations."
  },
  "Learning not to Discriminate: Task Agnostic Learning for Improving Monolingual and Code-switched Speech Recognition": {
    "title": "EMG Signal Analysis Using Intrinsic Mode Functions to Discriminate Upper Limb Movements",
    "authors": [
      "M. Karuna",
      "Sitaramanjaneya Reddy Guntur"
    ],
    "doi": "10.1109/aisp48273.2020.9073313",
    "url": "https://doi.org/10.1109/aisp48273.2020.9073313",
    "journal": "2020 International Conference on Artificial Intelligence and Signal Processing (AISP)",
    "year": 2020,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": "1-3",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "Recognizing code-switched speech is challenging for Automatic Speech Recognition (ASR) for a variety of reasons, including the lack of code-switched training data. Recently, we showed that monolingual ASR systems fine-tuned on code-switched data deteriorate in performance on monolingual speech recognition, which is not desirable as ASR systems deployed in multilingual scenarios should recognize both monolingual and code-switched speech with high accuracy. Our experiments indicated that this loss in performance could be mitigated by using certain strategies for fine-tuning and regularization, leading to improvements in both monolingual and code-switched ASR. In this work, we present further improvements over our previous work by using domain adversarial learning to train task agnostic models. We evaluate the classification accuracy of an adversarial discriminator and show that it can learn shared layer parameters that are task agnostic. We train end-to-end ASR systems starting with a pooled model that uses monolingual and code-switched data along with the adversarial discriminator. Our proposed technique leads to reductions in Word Error Rates (WER) in monolingual and code-switched test sets across three language pairs.",
    "publication_date": "2020-01-01T00:00:00",
    "citation_count": 6,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2006.05257",
    "docname": "karuna2020emgsignalanalysis",
    "dockey": "603f4a04ed0e4ed3",
    "citation": "M. Karuna and Sitaramanjaneya Reddy Guntur. Emg signal analysis using intrinsic mode functions to discriminate upper limb movements. 2020 International Conference on Artificial Intelligence and Signal Processing (AISP), pages 1-3, Jan 2020. URL: https://doi.org/10.1109/aisp48273.2020.9073313, doi:10.1109/aisp48273.2020.9073313.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "karuna2020emgsignalanalysis",
    "bibtex": "@article{karuna2020emgsignalanalysis,\n    author = \"Karuna, M. and Guntur, Sitaramanjaneya Reddy\",\n    title = \"EMG Signal Analysis Using Intrinsic Mode Functions to Discriminate Upper Limb Movements\",\n    year = \"2020\",\n    journal = \"2020 International Conference on Artificial Intelligence and Signal Processing (AISP)\",\n    pages = \"1-3\",\n    month = \"Jan\",\n    doi = \"10.1109/aisp48273.2020.9073313\",\n    url = \"https://doi.org/10.1109/aisp48273.2020.9073313\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/aisp48273.2020.9073313",
    "doc_id": "603f4a04ed0e4ed3",
    "formatted_citation": "M. Karuna and Sitaramanjaneya Reddy Guntur. Emg signal analysis using intrinsic mode functions to discriminate upper limb movements. 2020 International Conference on Artificial Intelligence and Signal Processing (AISP), pages 1-3, Jan 2020. URL: https://doi.org/10.1109/aisp48273.2020.9073313, doi:10.1109/aisp48273.2020.9073313. This article has 6 citations."
  },
  "SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network": {
    "title": "SpeechStew: Simply Mix All Available Speech Recognition Data to Train One Large Neural Network",
    "authors": [
      "Chan, William",
      "Park, Daniel",
      "Lee, Chris",
      "Zhang, Yu",
      "Le, Quoc",
      "Norouzi, Mohammad"
    ],
    "doi": null,
    "url": "http://arxiv.org/abs/2104.02133",
    "journal": null,
    "year": 2021,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2104.02133.pdf",
      "https://arxiv.org/abs/2104.02133"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Computation and Language",
      "Computer Science - Machine Learning"
    ],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "arXiv",
    "abstract": "We present SpeechStew, a speech recognition model that is trained on a combination of various publicly available speech recognition datasets: AMI, Broadcast News, Common Voice, LibriSpeech, Switchboard/Fisher, Tedlium, and Wall Street Journal. SpeechStew simply mixes all of these datasets together, without any special re-weighting or re-balancing of the datasets. SpeechStew achieves SoTA or near SoTA results across a variety of tasks, without the use of an external language model. Our results include 9.0\\% WER on AMI-IHM, 4.7\\% WER on Switchboard, 8.3\\% WER on CallHome, and 1.3\\% on WSJ, which significantly outperforms prior work with strong external language models. We also demonstrate that SpeechStew learns powerful transfer learning representations. We fine-tune SpeechStew on a noisy low resource speech dataset, CHiME-6. We achieve 38.9\\% WER without a language model, which compares to 38.6\\% WER to a strong HMM baseline with a language model.",
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://arxiv.org/pdf/2104.02133.pdf",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": "2104.02133"
  },
  "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale": {
    "title": "XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale",
    "authors": [
      "Babu, Arun",
      "Wang, Changhan",
      "Tjandra, Andros",
      "Lakhotia, Kushal",
      "Xu, Qiantong",
      "Goyal, Naman",
      "Singh, Kritika",
      "von Platen, Patrick",
      "Saraf, Yatharth",
      "Pino, Juan",
      "Baevski, Alexei",
      "Conneau, Alexis",
      "Auli, Michael"
    ],
    "doi": null,
    "url": "http://arxiv.org/abs/2111.09296",
    "journal": null,
    "year": 2021,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2111.09296.pdf",
      "https://arxiv.org/abs/2111.09296"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "arXiv",
    "abstract": "This paper presents XLS-R, a large-scale model for cross-lingual speech representation learning based on wav2vec 2.0. We train models with up to 2B parameters on nearly half a million hours of publicly available speech audio in 128 languages, an order of magnitude more public data than the largest known prior work. Our evaluation covers a wide range of tasks, domains, data regimes and languages, both high and low-resource. On the CoVoST-2 speech translation benchmark, we improve the previous state of the art by an average of 7.4 BLEU over 21 translation directions into English. For speech recognition, XLS-R improves over the best known prior work on BABEL, MLS, CommonVoice as well as VoxPopuli, lowering error rates by 14-34% relative on average. XLS-R also sets a new state of the art on VoxLingua107 language identification. Moreover, we show that with sufficient model size, cross-lingual pretraining can outperform English-only pretraining when translating English speech into other languages, a setting which favors monolingual pretraining. We hope XLS-R can help to improve speech processing tasks for many more languages of the world.",
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://arxiv.org/pdf/2111.09296.pdf",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": "2111.09296"
  },
  "GigaSpeech: An Evolving, Multi-domain ASR Corpus with 10,000 Hours of Transcribed Audio": {
    "title": "GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio",
    "authors": [
      "Guoguo Chen",
      "Shuzhou Chai",
      "Guan-Bo Wang",
      "Jiayu Du",
      "Weiqiang Zhang",
      "Chao Weng",
      "Dan Su",
      "Daniel Povey",
      "J. Trmal",
      "Junbo Zhang",
      "Mingjie Jin",
      "S. Khudanpur",
      "Shinji Watanabe",
      "Shuaijiang Zhao",
      "Wei Zou",
      "Xiangang Li",
      "Xuchen Yao",
      "Yongqing Wang",
      "Yujun Wang",
      "Zhao You",
      "Zhiyong Yan"
    ],
    "doi": "10.21437/interspeech.2021-1965",
    "url": "https://doi.org/10.21437/interspeech.2021-1965",
    "journal": "Interspeech 2021",
    "year": 2021,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": null,
    "issue": null,
    "volume": "abs/2106.06909",
    "publisher": "ISCA",
    "abstract": "This paper introduces GigaSpeech, an evolving, multi-domain English speech recognition corpus with 10,000 hours of high quality labeled audio suitable for supervised training, and 40,000 hours of total audio suitable for semi-supervised and unsupervised training. Around 40,000 hours of transcribed audio is first collected from audiobooks, podcasts and YouTube, covering both read and spontaneous speaking styles, and a variety of topics, such as arts, science, sports, etc. A new forced alignment and segmentation pipeline is proposed to create sentence segments suitable for speech recognition training, and to filter out segments with low-quality transcription. For system training, GigaSpeech provides five subsets of different sizes, 10h, 250h, 1000h, 2500h, and 10000h. For our 10,000-hour XL training subset, we cap the word error rate at 4% during the filtering/validation stage, and for all our other smaller training subsets, we cap it at 0%. The DEV and TEST evaluation sets, on the other hand, are re-processed by professional human transcribers to ensure high transcription quality. Baseline systems are provided for popular speech recognition toolkits, namely Athena, ESPnet, Kaldi and Pika.",
    "publication_date": "2021-08-30T00:00:00",
    "citation_count": 362,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2106.06909",
    "docname": "chen2021gigaspeechanevolving",
    "dockey": "ac0c3ecc57cd5952",
    "citation": "Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Weiqiang Zhang, Chao Weng, Dan Su, Daniel Povey, J. Trmal, Junbo Zhang, Mingjie Jin, S. Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. Gigaspeech: an evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. Interspeech 2021, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-1965, doi:10.21437/interspeech.2021-1965.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "chen2021gigaspeechanevolving",
    "bibtex": "@article{chen2021gigaspeechanevolving,\n    author = \"Chen, Guoguo and Chai, Shuzhou and Wang, Guan-Bo and Du, Jiayu and Zhang, Weiqiang and Weng, Chao and Su, Dan and Povey, Daniel and Trmal, J. and Zhang, Junbo and Jin, Mingjie and Khudanpur, S. and Watanabe, Shinji and Zhao, Shuaijiang and Zou, Wei and Li, Xiangang and Yao, Xuchen and Wang, Yongqing and Wang, Yujun and You, Zhao and Yan, Zhiyong\",\n    title = \"GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio\",\n    year = \"2021\",\n    journal = \"Interspeech 2021\",\n    volume = \"abs/2106.06909\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2021-1965\",\n    url = \"https://doi.org/10.21437/interspeech.2021-1965\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2021-1965",
    "doc_id": "ac0c3ecc57cd5952",
    "formatted_citation": "Guoguo Chen, Shuzhou Chai, Guan-Bo Wang, Jiayu Du, Weiqiang Zhang, Chao Weng, Dan Su, Daniel Povey, J. Trmal, Junbo Zhang, Mingjie Jin, S. Khudanpur, Shinji Watanabe, Shuaijiang Zhao, Wei Zou, Xiangang Li, Xuchen Yao, Yongqing Wang, Yujun Wang, Zhao You, and Zhiyong Yan. Gigaspeech: an evolving, multi-domain asr corpus with 10,000 hours of transcribed audio. Interspeech 2021, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-1965, doi:10.21437/interspeech.2021-1965. This article has 362 citations."
  },
  "Voice Conversion Based Data Augmentation to Improve Children’s Speech Recognition in Limited Data Scenario": {
    "title": "Voice Conversion Based Data Augmentation to Improve Children’s Speech Recognition in Limited Data Scenario",
    "authors": [
      "S. Shahnawazuddin",
      "Nagaraj Adiga",
      "K. Kumar",
      "Aayushi Poddar",
      "Waquar Ahmad"
    ],
    "doi": "10.21437/interspeech.2020-1112",
    "url": "https://www.isca-archive.org/interspeech_2020/shahnawazuddin20_interspeech.html",
    "journal": "Interspeech 2020",
    "year": 2020,
    "genre": "conferencePaper",
    "link_attachments": [
      "http://www.interspeech2020.org/uploadfile/pdf/Thu-2-8-10.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "4382-4386",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Automatic recognition of children’s speech is a challenging research problem due to several reasons. One among those is unavailability of large amounts of speech data from child speakers to develop automatic speech recognition (ASR) systems employing deep learning architectures.Using a limited amount of training data limits the power of the learned system. To overcome this issue, we have explored means to effectively make use of adults’ speech data for training an ASR system. For that purpose, generative adversarial network (GAN) based voice conversion (VC) is exploited to modify the acoustic attributes of adults’ speech making it perceptually similar to that of children’s speech. The original and converted speech samples from adult speakers are then pooled together to learn the statistical model parameters. Signiﬁcantly improved recognition rate for children’s speech is noted due to VC-based data augmentation. To further enhance the recognition rate, a limited amount of children’s speech data is also pooled into training. Large reduction in error rate is observed in this case as well. It is worth mentioning that GAN-based VC does not change the speakingrate. To demonstrate the need to deal with speaking-rate differences we report the results of time-scale modiﬁcation of childrens speech test data.",
    "publication_date": "2020-10-25T00:00:00",
    "citation_count": 40,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "shahnawazuddin2020voiceconversionbased",
    "dockey": "ae28d086ecd24aa2",
    "citation": "S. Shahnawazuddin, Nagaraj Adiga, K. Kumar, Aayushi Poddar, and Waquar Ahmad. Voice conversion based data augmentation to improve children’s speech recognition in limited data scenario. Interspeech 2020, pages 4382-4386, Oct 2020. URL: https://doi.org/10.21437/interspeech.2020-1112, doi:10.21437/interspeech.2020-1112.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shahnawazuddin2020voiceconversionbased",
    "bibtex": "@article{shahnawazuddin2020voiceconversionbased,\n    author = \"Shahnawazuddin, S. and Adiga, Nagaraj and Kumar, K. and Poddar, Aayushi and Ahmad, Waquar\",\n    title = \"Voice Conversion Based Data Augmentation to Improve Children’s Speech Recognition in Limited Data Scenario\",\n    year = \"2020\",\n    journal = \"Interspeech 2020\",\n    pages = \"4382-4386\",\n    month = \"Oct\",\n    doi = \"10.21437/interspeech.2020-1112\",\n    url = \"https://doi.org/10.21437/interspeech.2020-1112\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2020-1112",
    "doc_id": "ae28d086ecd24aa2",
    "formatted_citation": "S. Shahnawazuddin, Nagaraj Adiga, K. Kumar, Aayushi Poddar, and Waquar Ahmad. Voice conversion based data augmentation to improve children’s speech recognition in limited data scenario. Interspeech 2020, pages 4382-4386, Oct 2020. URL: https://doi.org/10.21437/interspeech.2020-1112, doi:10.21437/interspeech.2020-1112. This article has 40 citations."
  },
  "Transfer learning from adult to children for speech recognition: Evaluation, analysis and recommendations": {
    "title": "Transfer learning from adult to children for speech recognition: Evaluation, analysis and recommendations",
    "authors": [
      "Prashanth Gurunath Shivakumar",
      "Panayiotis Georgiou"
    ],
    "doi": "10.1016/j.csl.2020.101077",
    "url": "https://doi.org/10.1016/j.csl.2020.101077",
    "journal": "Computer Speech &amp; Language",
    "year": 2020,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7199459/"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Analysis of children’s speech",
      "Automatic speech recognition",
      "Children speech recognition",
      "Deep learning",
      "Deep neural network",
      "Transfer learning"
    ],
    "pages": "101077",
    "issue": null,
    "volume": "63",
    "publisher": "Elsevier BV",
    "abstract": "Children speech recognition is challenging mainly due to the inherent high variability in children’s physical and articulatory characteristics and expressions. This variability manifests in both acoustic constructs and linguistic usage due to the rapidly changing developmental stage in children’s life. Part of the challenge is due to the lack of large amounts of available children speech data for efficient modeling. This work attempts to address the key challenges using transfer learning from adult’s models to children’s models in a Deep Neural Network (DNN) framework for children’s Automatic Speech Recognition (ASR) task evaluating on multiple children’s speech corpora with a large vocabulary. The paper presents a systematic and an extensive analysis of the proposed transfer learning technique considering the key factors affecting children’s speech recognition from prior literature. Evaluations are presented on (i) comparisons of earlier GMM-HMM and the newer DNN Models, (ii) effectiveness of standard adaptation techniques versus transfer learning, (iii) various adaptation configurations in tackling the variabilities present in children speech, in terms of (a) acoustic spectral variability, and (b) pronunciation variability and linguistic constraints. Our Analysis spans over (i) number of DNN model parameters (for adaptation), (ii) amount of adaptation data, (iii) ages of children, (iv) age dependent-independent adaptation. Finally, we provide Recommendations on (i) the favorable strategies over various aforementioned - analyzed parameters, and (ii) potential future research directions and relevant challenges/problems persisting in DNN based ASR for children’s speech.",
    "publication_date": "2020-09-01T00:00:00",
    "citation_count": 132,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "shivakumar2020transferlearningfrom",
    "dockey": "b8d89cd375d83296",
    "citation": "Prashanth Gurunath Shivakumar and Panayiotis Georgiou. Transfer learning from adult to children for speech recognition: evaluation, analysis and recommendations. Computer Speech &amp; Language, 63:101077, Sep 2020. URL: https://doi.org/10.1016/j.csl.2020.101077, doi:10.1016/j.csl.2020.101077.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shivakumar2020transferlearningfrom",
    "bibtex": "@article{shivakumar2020transferlearningfrom,\n    author = \"Shivakumar, Prashanth Gurunath and Georgiou, Panayiotis\",\n    title = \"Transfer learning from adult to children for speech recognition: Evaluation, analysis and recommendations\",\n    year = \"2020\",\n    journal = \"Computer Speech \\&amp; Language\",\n    volume = \"63\",\n    pages = \"101077\",\n    month = \"Sep\",\n    doi = \"10.1016/j.csl.2020.101077\",\n    url = \"https://doi.org/10.1016/j.csl.2020.101077\",\n    publisher = \"Elsevier BV\",\n    issn = \"0885-2308\"\n}\n",
    "issn": "0885-2308",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1016/j.csl.2020.101077",
    "doc_id": "b8d89cd375d83296",
    "formatted_citation": "Prashanth Gurunath Shivakumar and Panayiotis Georgiou. Transfer learning from adult to children for speech recognition: evaluation, analysis and recommendations. Computer Speech &amp; Language, 63:101077, Sep 2020. URL: https://doi.org/10.1016/j.csl.2020.101077, doi:10.1016/j.csl.2020.101077. This article has 132 citations."
  },
  "End-to-end neural systems for automatic children speech recognition: An empirical study": {
    "title": "End-to-end neural systems for automatic children speech recognition: An empirical study",
    "authors": [
      "Prashanth Gurunath Shivakumar",
      "Shrikanth S. Narayanan"
    ],
    "doi": "10.1016/j.csl.2021.101289",
    "url": "https://doi.org/10.1016/j.csl.2021.101289",
    "journal": "Computer Speech &amp; Language",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.sciencedirect.com/science/article/abs/pii/S0885230821000905",
      "https://arxiv.org/pdf/2102.09918"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Children speech recognition",
      "End-to-end speech recognition",
      "Residual network",
      "Time depth separable convolutional network",
      "Transformer"
    ],
    "pages": "101289",
    "issue": null,
    "volume": "72",
    "publisher": "Elsevier BV",
    "abstract": "A key desiderata for inclusive and accessible speech recognition technology is ensuring its robust performance to children’s speech. Notably, this includes the rapidly advancing neural network based end-to-end speech recognition systems. Children speech recognition is more challenging due to the larger intra-inter speaker variability in terms of acoustic and linguistic characteristics compared to adult speech. Furthermore, the lack of adequate and appropriate children speech resources adds to the challenge of designing robust end-to-end neural architectures. This study provides a critical assessment of automatic children speech recognition through an empirical study of contemporary state-of-the-art end-to-end speech recognition systems. Insights are provided on the aspects of training data requirements, adaptation on children data, and the effect of children age, utterance lengths, different architectures and loss functions for end-to-end systems and role of language models on the speech recognition performance.",
    "publication_date": "2022-03-01T00:00:00",
    "citation_count": 51,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "shivakumar2022endtoendneuralsystems",
    "dockey": "55d91d7bdc0463c0",
    "citation": "Prashanth Gurunath Shivakumar and Shrikanth S. Narayanan. End-to-end neural systems for automatic children speech recognition: an empirical study. Computer Speech &amp; Language, 72:101289, Mar 2022. URL: https://doi.org/10.1016/j.csl.2021.101289, doi:10.1016/j.csl.2021.101289.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shivakumar2022endtoendneuralsystems",
    "bibtex": "@article{shivakumar2022endtoendneuralsystems,\n    author = \"Shivakumar, Prashanth Gurunath and Narayanan, Shrikanth S.\",\n    title = \"End-to-end neural systems for automatic children speech recognition: An empirical study\",\n    year = \"2022\",\n    journal = \"Computer Speech \\&amp; Language\",\n    volume = \"72\",\n    pages = \"101289\",\n    month = \"Mar\",\n    doi = \"10.1016/j.csl.2021.101289\",\n    url = \"https://doi.org/10.1016/j.csl.2021.101289\",\n    publisher = \"Elsevier BV\",\n    issn = \"0885-2308\"\n}\n",
    "issn": "0885-2308",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1016/j.csl.2021.101289",
    "doc_id": "55d91d7bdc0463c0",
    "formatted_citation": "Prashanth Gurunath Shivakumar and Shrikanth S. Narayanan. End-to-end neural systems for automatic children speech recognition: an empirical study. Computer Speech &amp; Language, 72:101289, Mar 2022. URL: https://doi.org/10.1016/j.csl.2021.101289, doi:10.1016/j.csl.2021.101289. This article has 51 citations."
  },
  "[PDF] Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children's Speech | Semantic Scholar": {
    "title": "Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children’s Speech",
    "authors": [
      "Vishwanath Pratap Singh",
      "Hardik Sailor",
      "Supratik Bhattacharya",
      "Abhishek Pandey"
    ],
    "doi": "10.21437/interspeech.2022-11343",
    "url": "https://doi.org/10.21437/interspeech.2022-11343",
    "journal": "Interspeech 2022",
    "year": 2022,
    "genre": "webpage",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "3213-3217",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": null,
    "publication_date": "2022-09-18T00:00:00",
    "citation_count": 14,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "singh2022spectralmodificationbased",
    "dockey": "557d1823a126fe36",
    "citation": "Vishwanath Pratap Singh, Hardik Sailor, Supratik Bhattacharya, and Abhishek Pandey. Spectral modification based data augmentation for improving end-to-end asr for children’s speech. Interspeech 2022, pages 3213-3217, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-11343, doi:10.21437/interspeech.2022-11343.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "singh2022spectralmodificationbased",
    "bibtex": "@article{singh2022spectralmodificationbased,\n    author = \"Singh, Vishwanath Pratap and Sailor, Hardik and Bhattacharya, Supratik and Pandey, Abhishek\",\n    title = \"Spectral Modification Based Data Augmentation For Improving End-to-End ASR For Children’s Speech\",\n    year = \"2022\",\n    journal = \"Interspeech 2022\",\n    pages = \"3213-3217\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2022-11343\",\n    url = \"https://doi.org/10.21437/interspeech.2022-11343\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2022-11343",
    "doc_id": "557d1823a126fe36",
    "formatted_citation": "Vishwanath Pratap Singh, Hardik Sailor, Supratik Bhattacharya, and Abhishek Pandey. Spectral modification based data augmentation for improving end-to-end asr for children’s speech. Interspeech 2022, pages 3213-3217, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-11343, doi:10.21437/interspeech.2022-11343. This article has 14 citations."
  },
  "A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition": {
    "title": "A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition",
    "authors": [
      "Andrei Barcovschi",
      "Rishabh Jain",
      "Peter Corcoran"
    ],
    "doi": "10.1109/sped59241.2023.10314867",
    "url": "https://ieeexplore.ieee.org/document/10314867/",
    "journal": "2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)",
    "year": 2023,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.semanticscholar.org/paper/A-comparative-analysis-between-Whisper%2C-and-for-the-Barcovschi-Jain/949bef66273661eab439b6bd9b9a4015de9a5f4d"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "42-47",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "Automatic Speech Recognition (ASR) systems have progressed significantly in their performance on adult speech data; however, transcribing child speech remains challenging due to the acoustic differences in the characteristics of child and adult voices. This work aims to explore the potential of adapting state-of-the-art Conformer-transducer models to child speech to improve child speech recognition performance. Furthermore, the results are compared with those of self-supervised wav2vec2 models and semi-supervised multi-domain Whisper models that were previously finetuned on the same data. We demonstrate that finetuning Conformer-transducer models on child speech yields significant improvements in ASR performance on child speech, compared to the non-finetuned models. We also show Whisper and wav2vec2 adaptation on different child speech datasets. Our detailed comparative analysis shows that wav2vec2 provides the most consistent performance improvements among the three methods studied.",
    "publication_date": "2023-10-25T00:00:00",
    "citation_count": 3,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "barcovschi2023acomparativeanalysis",
    "dockey": "337f903ccfedf48b",
    "citation": "Andrei Barcovschi, Rishabh Jain, and Peter Corcoran. A comparative analysis between conformer-transducer, whisper, and wav2vec2 for improving the child speech recognition. 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), pages 42-47, Oct 2023. URL: https://doi.org/10.1109/sped59241.2023.10314867, doi:10.1109/sped59241.2023.10314867.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "barcovschi2023acomparativeanalysis",
    "bibtex": "@article{barcovschi2023acomparativeanalysis,\n    author = \"Barcovschi, Andrei and Jain, Rishabh and Corcoran, Peter\",\n    title = \"A comparative analysis between Conformer-Transducer, Whisper, and wav2vec2 for improving the child speech recognition\",\n    year = \"2023\",\n    journal = \"2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)\",\n    pages = \"42-47\",\n    month = \"Oct\",\n    doi = \"10.1109/sped59241.2023.10314867\",\n    url = \"https://doi.org/10.1109/sped59241.2023.10314867\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/sped59241.2023.10314867",
    "doc_id": "337f903ccfedf48b",
    "formatted_citation": "Andrei Barcovschi, Rishabh Jain, and Peter Corcoran. A comparative analysis between conformer-transducer, whisper, and wav2vec2 for improving the child speech recognition. 2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD), pages 42-47, Oct 2023. URL: https://doi.org/10.1109/sped59241.2023.10314867, doi:10.1109/sped59241.2023.10314867. This article has 3 citations."
  },
  "FASA: a Flexible and Automatic Speech Aligner for Extracting High-quality Aligned Children Speech Data": {
    "title": "FASA",
    "authors": [
      "V.S. Subramaniam"
    ],
    "doi": "10.1002/9783527809080.cataz06749",
    "url": "https://www.semanticscholar.org/paper/FASA%3A-a-Flexible-and-Automatic-Speech-Aligner-for-Liu-Xiong/62a721694bbb2c9de1899374bb68ae7ecb3ef419",
    "journal": "Catalysis from A to Z",
    "year": 2020,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://arxiv.org/pdf/2406.17926.pdf",
      "https://www.semanticscholar.org/paper/FASA%3A-a-Flexible-and-Automatic-Speech-Aligner-for-Liu-Xiong/62a721694bbb2c9de1899374bb68ae7ecb3ef419"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "Wiley",
    "abstract": "Automatic Speech Recognition (ASR) for adults' speeches has made significant progress by employing deep neural network (DNN) models recently, but improvement in children's speech is still unsatisfactory due to children's speech's distinct characteristics. DNN models pre-trained on adult data often struggle in generalizing children's speeches with fine tuning because of the lack of high-quality aligned children's speeches. When generating datasets, human annotations are not scalable, and existing forced-alignment tools are not usable as they make impractical assumptions about the quality of the input transcriptions. To address these challenges, we propose a new forced-alignment tool, FASA, as a flexible and automatic speech aligner to extract high-quality aligned children's speech data from many of the existing noisy children's speech data. We demonstrate its usage on the CHILDES dataset and show that FASA can improve data quality by 13.6$\\times$ over human annotations.",
    "publication_date": "2020-04-17T00:00:00",
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "subramaniam2020fasa",
    "dockey": "e3fa9d17fac6304b",
    "citation": "V.S. Subramaniam. Fasa. Catalysis from A to Z, Apr 2020. URL: https://doi.org/10.1002/9783527809080.cataz06749, doi:10.1002/9783527809080.cataz06749.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "subramaniam2020fasa",
    "bibtex": "@article{subramaniam2020fasa,\n    author = \"Subramaniam, V.S.\",\n    title = \"FASA\",\n    year = \"2020\",\n    journal = \"Catalysis from A to Z\",\n    month = \"Apr\",\n    doi = \"10.1002/9783527809080.cataz06749\",\n    url = \"https://doi.org/10.1002/9783527809080.cataz06749\",\n    publisher = \"Wiley\"\n}\n",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1002/9783527809080.cataz06749",
    "doc_id": "e3fa9d17fac6304b",
    "formatted_citation": "V.S. Subramaniam. Fasa. Catalysis from A to Z, Apr 2020. URL: https://doi.org/10.1002/9783527809080.cataz06749, doi:10.1002/9783527809080.cataz06749."
  },
  "Exploring Adapters with Conformers for Children’s Automatic Speech Recognition": {
    "title": "Exploring Adapters with Conformers for Children’s Automatic Speech Recognition",
    "authors": [
      "Thomas Rolland",
      "Alberto Abad"
    ],
    "doi": "10.1109/icassp48485.2024.10447091",
    "url": "https://ieeexplore.ieee.org/document/10447091/",
    "journal": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.semanticscholar.org/paper/Exploring-Adapters-with-Conformers-for-Children%E2%80%99s-Rolland-Abad/d21661500948db9df426089508de1faeb15ef43c"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "12747-12751",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "The high variability in acoustic, pronunciation, and linguistic characteristics of children’s speech makes of children’s automatic speech recognition (ASR) a complex task. Training a dedicated ASR model from scratch for children remains challenging, mainly due to the limited availability of children’s data. To tackle this limitation, a common strategy involves fine-tuning a pre-trained ASR model. However, this approach faces challenges due to the diversity of speakers and data scarcity, especially when dealing with large ASR models like the Conformer. In this study, we explore an alternative approach known as Adapter transfer. Adapter transfer requires training fewer parameters and can be more effective in adapting large ASR models for children’s speech. In this paper, we assess various Adapter configurations in the literature and introduce a novel configuration called Two Serial Adapter (TSA). The experimental results indicate that Adapter transfer consistently outperforms traditional fine-tuning across various configurations for the Conformer model.",
    "publication_date": "2024-03-18T00:00:00",
    "citation_count": 4,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "rolland2024exploringadapterswith",
    "dockey": "8814a0264f0e9800",
    "citation": "Thomas Rolland and Alberto Abad. Exploring adapters with conformers for children’s automatic speech recognition. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12747-12751, Mar 2024. URL: https://doi.org/10.1109/icassp48485.2024.10447091, doi:10.1109/icassp48485.2024.10447091.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "rolland2024exploringadapterswith",
    "bibtex": "@article{rolland2024exploringadapterswith,\n    author = \"Rolland, Thomas and Abad, Alberto\",\n    title = \"Exploring Adapters with Conformers for Children’s Automatic Speech Recognition\",\n    year = \"2024\",\n    journal = \"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"12747-12751\",\n    month = \"Mar\",\n    doi = \"10.1109/icassp48485.2024.10447091\",\n    url = \"https://doi.org/10.1109/icassp48485.2024.10447091\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp48485.2024.10447091",
    "doc_id": "8814a0264f0e9800",
    "formatted_citation": "Thomas Rolland and Alberto Abad. Exploring adapters with conformers for children’s automatic speech recognition. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12747-12751, Mar 2024. URL: https://doi.org/10.1109/icassp48485.2024.10447091, doi:10.1109/icassp48485.2024.10447091. This article has 4 citations."
  },
  "Exploring Native and Non-Native English Child Speech Recognition With Whisper": {
    "title": "Exploring Native and Non-Native English Child Speech Recognition With Whisper",
    "authors": [
      "Rishabh Jain",
      "Andrei Barcovschi",
      "Mariam Yiwere",
      "Peter Corcoran",
      "H. Cucu"
    ],
    "doi": "10.1109/access.2024.3378738",
    "url": "https://ieeexplore.ieee.org/document/10474352/",
    "journal": "IEEE Access",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Data models",
      "Training",
      "Automatic speech recognition",
      "Speech recognition",
      "Adaptation models",
      "Child automatic speech recognition",
      "CMU_Kids",
      "Decoding",
      "large-scale supervision",
      "MyST",
      "non-native child speech",
      "Pediatrics",
      "PFSTAR",
      "speechocean762",
      "Testing",
      "Transformers",
      "whisper"
    ],
    "pages": "41601-41610",
    "issue": null,
    "volume": "12",
    "publisher": null,
    "abstract": "Modern end-to-end Automatic Speech Recognition (ASR) systems struggle to recognise children’s speech. This challenge is due to the high acoustic variability in children’s voices and the scarcity of child speech training data, particularly for accented or low-resource languages. This study focuses on improving the performance of ASR on native and non-native English child speech using publicly available datasets. We evaluate how the large-scale whisper models (trained with a large amount of adult speech data) perform with child speech. In addition, we perform finetuning experiments using different child speech datasets to investigate the performance of whisper ASR on non-native English-speaking children’s speech. Our findings indicate relative Word Error Rate (WER) improvements ranging from 29% to 89% over previous benchmarks on the same datasets. Notably, these gains were achieved by finetuning with only a 10% sample of unseen non-native datasets. These results demonstrate the potential of whisper for improving ASR in a low-resource scenario for non-native child speech.",
    "publication_date": null,
    "citation_count": 5,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "jain2024exploringnativeand",
    "dockey": "bf07f0486521a035",
    "citation": "Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, and H. Cucu. Exploring native and non-native english child speech recognition with whisper. IEEE Access, 12:41601-41610, 2024. URL: https://doi.org/10.1109/access.2024.3378738, doi:10.1109/access.2024.3378738.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "jain2024exploringnativeand",
    "bibtex": "@article{jain2024exploringnativeand,\n    author = \"Jain, Rishabh and Barcovschi, Andrei and Yiwere, Mariam and Corcoran, Peter and Cucu, H.\",\n    title = \"Exploring Native and Non-Native English Child Speech Recognition With Whisper\",\n    year = \"2024\",\n    journal = \"IEEE Access\",\n    volume = \"12\",\n    pages = \"41601-41610\",\n    doi = \"10.1109/access.2024.3378738\",\n    url = \"https://doi.org/10.1109/access.2024.3378738\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/access.2024.3378738",
    "doc_id": "bf07f0486521a035",
    "formatted_citation": "Rishabh Jain, Andrei Barcovschi, Mariam Yiwere, Peter Corcoran, and H. Cucu. Exploring native and non-native english child speech recognition with whisper. IEEE Access, 12:41601-41610, 2024. URL: https://doi.org/10.1109/access.2024.3378738, doi:10.1109/access.2024.3378738. This article has 5 citations and is from a peer-reviewed journal."
  },
  "[PDF] Reducing Multilingual Context Confusion for End-to-end Code-switching Automatic Speech Recognition | Semantic Scholar": {
    "title": "reducing multilingual context confusion for end-to-end code-switching automatic speech recognition",
    "authors": [
      "Shuai Zhang",
      "Jiangyan Yi",
      "Zhengkun Tian",
      "Jianhua Tao",
      "Yu Ting Yeung",
      "Liqun Deng"
    ],
    "doi": "10.21437/interspeech.2022-10286",
    "url": "https://doi.org/10.21437/interspeech.2022-10286",
    "journal": "Interspeech 2022",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "3894-3898",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "This work proposes a language-related attention mechanism to reduce multilingual context confusion for the E2E code-switching ASR model based on the Equivalence Constraint (EC) Theory, and efficiently transfers language knowledge from rich monolingual data to improve the performance of the code- Switched AsR model. Code-switching deals with alternative languages in communication process. Training end-to-end (E2E) automatic speech recognition (ASR) systems for code-switching is especially challenging as code-switching training data are always insufficient to combat the increased multilingual context confusion due to the presence of more than one language. We propose a language-related attention mechanism to reduce multilingual context confusion for the E2E code-switching ASR model based on the Equivalence Constraint (EC) Theory. The linguistic theory requires that any monolingual fragment that occurs in the code-switching sentence must occur in one of the monolingual sentences. The theory establishes a bridge between monolingual data and code-switching data. We leverage this linguistics theory to design the code-switching E2E ASR model. The proposed model efficiently transfers language knowledge from rich monolingual data to improve the performance of the code-switching ASR model. We evaluate our model on ASRU 2019 Mandarin-English code-switching challenge dataset. Compared to the baseline model, our proposed model achieves a 17.12% relative error reduction.",
    "publication_date": "2022-09-18T00:00:00",
    "citation_count": 11,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "zhang2022reducingmultilingualcontext",
    "dockey": "27c2ab185b282130",
    "citation": "Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Jianhua Tao, Yu Ting Yeung, and Liqun Deng. Reducing multilingual context confusion for end-to-end code-switching automatic speech recognition. Interspeech 2022, pages 3894-3898, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-10286, doi:10.21437/interspeech.2022-10286.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "zhang2022reducingmultilingualcontext",
    "bibtex": "@article{zhang2022reducingmultilingualcontext,\n    author = \"Zhang, Shuai and Yi, Jiangyan and Tian, Zhengkun and Tao, Jianhua and Yeung, Yu Ting and Deng, Liqun\",\n    title = \"reducing multilingual context confusion for end-to-end code-switching automatic speech recognition\",\n    year = \"2022\",\n    journal = \"Interspeech 2022\",\n    pages = \"3894-3898\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2022-10286\",\n    url = \"https://doi.org/10.21437/interspeech.2022-10286\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2022-10286",
    "doc_id": "27c2ab185b282130",
    "formatted_citation": "Shuai Zhang, Jiangyan Yi, Zhengkun Tian, Jianhua Tao, Yu Ting Yeung, and Liqun Deng. Reducing multilingual context confusion for end-to-end code-switching automatic speech recognition. Interspeech 2022, pages 3894-3898, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-10286, doi:10.21437/interspeech.2022-10286. This article has 11 citations."
  },
  "Evaluating and improving child-directed automatic speech recognition": {
    "title": "Continuous Automatic Speech Recognition by Lipreading",
    "authors": [
      "Alan J. Goldschen",
      "Oscar N. Garcia",
      "Eric D. Petajan"
    ],
    "doi": "10.1007/978-94-015-8935-2_14",
    "url": "https://doi.org/10.1007/978-94-015-8935-2_14",
    "journal": "Computational imaging and vision",
    "year": 1997,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://aclanthology.org/2020.lrec-1.778.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "321-343",
    "issue": null,
    "volume": "",
    "publisher": "Springer Netherlands",
    "abstract": null,
    "publication_date": "1997-01-01T00:00:00",
    "citation_count": 122,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "goldschen1997continuousautomaticspeech",
    "dockey": "0fc0174f7c5b6bbd",
    "citation": "Alan J. Goldschen, Oscar N. Garcia, and Eric D. Petajan. Continuous automatic speech recognition by lipreading. Computational imaging and vision, pages 321-343, Jan 1997. URL: https://doi.org/10.1007/978-94-015-8935-2\\_14, doi:10.1007/978-94-015-8935-2\\_14.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "goldschen1997continuousautomaticspeech",
    "bibtex": "@article{goldschen1997continuousautomaticspeech,\n    author = \"Goldschen, Alan J. and Garcia, Oscar N. and Petajan, Eric D.\",\n    title = \"Continuous Automatic Speech Recognition by Lipreading\",\n    year = \"1997\",\n    journal = \"Computational imaging and vision\",\n    pages = \"321-343\",\n    month = \"Jan\",\n    doi = \"10.1007/978-94-015-8935-2\\_14\",\n    url = \"https://doi.org/10.1007/978-94-015-8935-2\\_14\",\n    publisher = \"Springer Netherlands\",\n    issn = \"1381-6446\"\n}\n",
    "issn": "1381-6446",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1007/978-94-015-8935-2_14",
    "doc_id": "0fc0174f7c5b6bbd",
    "formatted_citation": "Alan J. Goldschen, Oscar N. Garcia, and Eric D. Petajan. Continuous automatic speech recognition by lipreading. Computational imaging and vision, pages 321-343, Jan 1997. URL: https://doi.org/10.1007/978-94-015-8935-2\\_14, doi:10.1007/978-94-015-8935-2\\_14. This article has 122 citations."
  },
  "A review of ASR technologies for children's speech": {
    "title": "A review of ASR technologies for children's speech",
    "authors": [
      "Matteo Gerosa",
      "Diego Giuliani",
      "Shrikanth Narayanan",
      "Alexandros Potamianos"
    ],
    "doi": "10.1145/1640377.1640384",
    "url": "https://doi.org/10.1145/1640377.1640384",
    "journal": "Proceedings of the 2nd Workshop on Child, Computer and Interaction",
    "year": 2009,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://sail.usc.edu/publications/files/gerosawocci2009.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1-8",
    "issue": null,
    "volume": null,
    "publisher": "ACM",
    "abstract": null,
    "publication_date": "2009-11-05T00:00:00",
    "citation_count": 145,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "gerosa2009areviewof",
    "dockey": "5c563224fd417f82",
    "citation": "Matteo Gerosa, Diego Giuliani, Shrikanth Narayanan, and Alexandros Potamianos. A review of asr technologies for children's speech. Proceedings of the 2nd Workshop on Child, Computer and Interaction, pages 1-8, Nov 2009. URL: https://doi.org/10.1145/1640377.1640384, doi:10.1145/1640377.1640384.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "gerosa2009areviewof",
    "bibtex": "@article{gerosa2009areviewof,\n    author = \"Gerosa, Matteo and Giuliani, Diego and Narayanan, Shrikanth and Potamianos, Alexandros\",\n    title = \"A review of ASR technologies for children's speech\",\n    year = \"2009\",\n    journal = \"Proceedings of the 2nd Workshop on Child, Computer and Interaction\",\n    pages = \"1-8\",\n    month = \"Nov\",\n    doi = \"10.1145/1640377.1640384\",\n    url = \"https://doi.org/10.1145/1640377.1640384\",\n    publisher = \"ACM\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1145/1640377.1640384",
    "doc_id": "5c563224fd417f82",
    "formatted_citation": "Matteo Gerosa, Diego Giuliani, Shrikanth Narayanan, and Alexandros Potamianos. A review of asr technologies for children's speech. Proceedings of the 2nd Workshop on Child, Computer and Interaction, pages 1-8, Nov 2009. URL: https://doi.org/10.1145/1640377.1640384, doi:10.1145/1640377.1640384. This article has 145 citations."
  },
  "Deep-neural network approaches for speech recognition with heterogeneous groups of speakers including children": {
    "title": "Deep-neural network approaches for speech recognition with heterogeneous groups of speakers including children",
    "authors": [
      "ROMAIN SERIZEL",
      "DIEGO GIULIANI"
    ],
    "doi": "10.1017/s135132491600005x",
    "url": "https://doi.org/10.1017/s135132491600005x",
    "journal": "Natural Language Engineering",
    "year": 2016,
    "genre": "journalArticle",
    "link_attachments": [
      "https://inria.hal.science/hal-01390905/file/15-1.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "325-350",
    "issue": "3",
    "volume": "23",
    "publisher": "Cambridge University Press (CUP)",
    "abstract": null,
    "publication_date": "2016-04-12T00:00:00",
    "citation_count": 62,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 3,
    "arxiv_id": null,
    "docname": "serizel2016deepneuralnetworkapproaches",
    "dockey": "8af084282cbe6c5b",
    "citation": "ROMAIN SERIZEL and DIEGO GIULIANI. Deep-neural network approaches for speech recognition with heterogeneous groups of speakers including children. Natural Language Engineering, 23:325-350, Apr 2016. URL: https://doi.org/10.1017/s135132491600005x, doi:10.1017/s135132491600005x.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "serizel2016deepneuralnetworkapproaches",
    "bibtex": "@article{serizel2016deepneuralnetworkapproaches,\n    author = \"SERIZEL, ROMAIN and GIULIANI, DIEGO\",\n    title = \"Deep-neural network approaches for speech recognition with heterogeneous groups of speakers including children\",\n    year = \"2016\",\n    journal = \"Natural Language Engineering\",\n    volume = \"23\",\n    pages = \"325-350\",\n    month = \"Apr\",\n    doi = \"10.1017/s135132491600005x\",\n    url = \"https://doi.org/10.1017/s135132491600005x\",\n    publisher = \"Cambridge University Press (CUP)\",\n    issue = \"3\",\n    issn = \"1351-3249\"\n}\n",
    "issn": "1351-3249",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1017/s135132491600005x",
    "doc_id": "8af084282cbe6c5b",
    "formatted_citation": "ROMAIN SERIZEL and DIEGO GIULIANI. Deep-neural network approaches for speech recognition with heterogeneous groups of speakers including children. Natural Language Engineering, 23:325-350, Apr 2016. URL: https://doi.org/10.1017/s135132491600005x, doi:10.1017/s135132491600005x. This article has 62 citations and is from a highest quality peer-reviewed journal."
  },
  "Language-agnostic age and gender classification of voice using self-supervised pre-training": {
    "title": "Language-agnostic Age and Gender Classification of Voice using Self-supervised Pre-training",
    "authors": [
      "Fredrik Lastow",
      "Edwin Ekberg",
      "Pierre Nugues"
    ],
    "doi": "10.1109/sais55783.2022.9833071",
    "url": "https://doi.org/10.1109/sais55783.2022.9833071",
    "journal": "2022 Swedish Artificial Intelligence Society Workshop (SAIS)",
    "year": 2022,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1-9",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2022-06-13T00:00:00",
    "citation_count": 4,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "lastow2022languageagnosticageand",
    "dockey": "aed781d972802c48",
    "citation": "Fredrik Lastow, Edwin Ekberg, and Pierre Nugues. Language-agnostic age and gender classification of voice using self-supervised pre-training. 2022 Swedish Artificial Intelligence Society Workshop (SAIS), pages 1-9, Jun 2022. URL: https://doi.org/10.1109/sais55783.2022.9833071, doi:10.1109/sais55783.2022.9833071.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "lastow2022languageagnosticageand",
    "bibtex": "@article{lastow2022languageagnosticageand,\n    author = \"Lastow, Fredrik and Ekberg, Edwin and Nugues, Pierre\",\n    title = \"Language-agnostic Age and Gender Classification of Voice using Self-supervised Pre-training\",\n    year = \"2022\",\n    journal = \"2022 Swedish Artificial Intelligence Society Workshop (SAIS)\",\n    pages = \"1-9\",\n    month = \"Jun\",\n    doi = \"10.1109/sais55783.2022.9833071\",\n    url = \"https://doi.org/10.1109/sais55783.2022.9833071\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/sais55783.2022.9833071",
    "doc_id": "aed781d972802c48",
    "formatted_citation": "Fredrik Lastow, Edwin Ekberg, and Pierre Nugues. Language-agnostic age and gender classification of voice using self-supervised pre-training. 2022 Swedish Artificial Intelligence Society Workshop (SAIS), pages 1-9, Jun 2022. URL: https://doi.org/10.1109/sais55783.2022.9833071, doi:10.1109/sais55783.2022.9833071. This article has 4 citations."
  },
  "Towards age-independent acoustic modeling": {
    "title": "Towards age-independent acoustic modeling",
    "authors": [
      "Matteo Gerosa",
      "Diego Giuliani",
      "Fabio Brugnara"
    ],
    "doi": "10.1016/j.specom.2009.01.006",
    "url": "https://doi.org/10.1016/j.specom.2009.01.006",
    "journal": "Speech Communication",
    "year": 2009,
    "genre": "journalArticle",
    "link_attachments": [
      "https://hal.science/hal-00524121/file/PEER_stage2_10.1016%252Fj.specom.2009.01.006.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "499-509",
    "issue": "6",
    "volume": "51",
    "publisher": "Elsevier BV",
    "abstract": null,
    "publication_date": "2009-06-01T00:00:00",
    "citation_count": 32,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 2,
    "arxiv_id": null,
    "docname": "gerosa2009towardsageindependentacoustic",
    "dockey": "b7d7773f920bc66e",
    "citation": "Matteo Gerosa, Diego Giuliani, and Fabio Brugnara. Towards age-independent acoustic modeling. Speech Communication, 51:499-509, Jun 2009. URL: https://doi.org/10.1016/j.specom.2009.01.006, doi:10.1016/j.specom.2009.01.006.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "gerosa2009towardsageindependentacoustic",
    "bibtex": "@article{gerosa2009towardsageindependentacoustic,\n    author = \"Gerosa, Matteo and Giuliani, Diego and Brugnara, Fabio\",\n    title = \"Towards age-independent acoustic modeling\",\n    year = \"2009\",\n    journal = \"Speech Communication\",\n    volume = \"51\",\n    pages = \"499-509\",\n    month = \"Jun\",\n    doi = \"10.1016/j.specom.2009.01.006\",\n    url = \"https://doi.org/10.1016/j.specom.2009.01.006\",\n    publisher = \"Elsevier BV\",\n    issue = \"6\",\n    issn = \"0167-6393\"\n}\n",
    "issn": "0167-6393",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1016/j.specom.2009.01.006",
    "doc_id": "b7d7773f920bc66e",
    "formatted_citation": "Matteo Gerosa, Diego Giuliani, and Fabio Brugnara. Towards age-independent acoustic modeling. Speech Communication, 51:499-509, Jun 2009. URL: https://doi.org/10.1016/j.specom.2009.01.006, doi:10.1016/j.specom.2009.01.006. This article has 32 citations and is from a domain leading peer-reviewed journal."
  },
  "The child language data exchange system": {
    "title": "The child language data exchange system",
    "authors": [
      "B. MacWhinney",
      "Catherine E. Snow"
    ],
    "doi": "10.1017/s0305000900006449",
    "url": "https://doi.org/10.1017/s0305000900006449",
    "journal": "Journal of Child Language",
    "year": 1985,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.academia.edu/download/78100178/The_child_language_data_exchange_system20220105-16984-1akuf6w.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "271-295",
    "issue": "2",
    "volume": "12",
    "publisher": "Cambridge University Press (CUP)",
    "abstract": null,
    "publication_date": "1985-06-01T00:00:00",
    "citation_count": 925,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 3,
    "arxiv_id": null,
    "docname": "macwhinney1985thechildlanguage",
    "dockey": "86adbdbe944dfb38",
    "citation": "B. MacWhinney and Catherine E. Snow. The child language data exchange system. Journal of Child Language, 12:271-295, Jun 1985. URL: https://doi.org/10.1017/s0305000900006449, doi:10.1017/s0305000900006449.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "macwhinney1985thechildlanguage",
    "bibtex": "@article{macwhinney1985thechildlanguage,\n    author = \"MacWhinney, B. and Snow, Catherine E.\",\n    title = \"The child language data exchange system\",\n    year = \"1985\",\n    journal = \"Journal of Child Language\",\n    volume = \"12\",\n    pages = \"271-295\",\n    month = \"Jun\",\n    doi = \"10.1017/s0305000900006449\",\n    url = \"https://doi.org/10.1017/s0305000900006449\",\n    publisher = \"Cambridge University Press (CUP)\",\n    issue = \"2\",\n    issn = \"0305-0009\"\n}\n",
    "issn": "0305-0009",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1017/s0305000900006449",
    "doc_id": "86adbdbe944dfb38",
    "formatted_citation": "B. MacWhinney and Catherine E. Snow. The child language data exchange system. Journal of Child Language, 12:271-295, Jun 1985. URL: https://doi.org/10.1017/s0305000900006449, doi:10.1017/s0305000900006449. This article has 925 citations and is from a highest quality peer-reviewed journal."
  },
  "Mapping the Early Language Environment Using All-Day Recordings and Automated Analysis": {
    "title": "Mapping the Early Language Environment Using All-Day Recordings and Automated Analysis.",
    "authors": [
      "J. Gilkerson",
      "J. Richards",
      "S. Warren",
      "J. Montgomery",
      "C. Greenwood",
      "D. Kimbrough Oller",
      "J. Hansen",
      "T. Paul"
    ],
    "doi": "10.1044/2016_ajslp-15-0169",
    "url": "http://pubs.asha.org/doi/10.1044/2016_AJSLP-15-0169",
    "journal": "American journal of speech-language pathology",
    "year": 2017,
    "genre": "journalArticle",
    "link_attachments": [
      "https://pubs.asha.org/doi/full/10.1044/2016_AJSLP-15-0169",
      "https://europepmc.org/articles/pmc6195063?pdf=render"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "\n248-265\n",
    "issue": "2",
    "volume": "26 2",
    "publisher": null,
    "abstract": "Purpose               This research provided a first-generation standardization of automated language environment estimates, validated these estimates against standard language assessments, and extended on previous research reporting language behavior differences across socioeconomic groups.                                         Method               Typically developing children between 2 to 48 months of age completed monthly, daylong recordings in their natural language environments over a span of approximately 6–38 months. The resulting data set contained 3,213 12-hr recordings automatically analyzed by using the Language Environment Analysis (LENA) System to generate estimates of (a) the number of adult words in the child's environment, (b) the amount of caregiver–child interaction, and (c) the frequency of child vocal output.                                         Results               Child vocalization frequency and turn-taking increased with age, whereas adult word counts were age independent after early infancy. Child vocalization and conversational turn estimates predicted 7%–16% of the variance observed in child language assessment scores. Lower socioeconomic status (SES) children produced fewer vocalizations, engaged in fewer adult–child interactions, and were exposed to fewer daily adult words compared with their higher socioeconomic status peers, but within-group variability was high.                                         Conclusions               The results offer new insight into the landscape of the early language environment, with clinical implications for identification of children at-risk for impoverished language environments.",
    "publication_date": null,
    "citation_count": 329,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 2,
    "arxiv_id": null,
    "docname": "gilkerson2017mappingtheearly",
    "dockey": "1d457d69a8d4b2a8",
    "citation": "J. Gilkerson, J. Richards, S. Warren, J. Montgomery, C. Greenwood, D. Kimbrough Oller, J. Hansen, and T. Paul. Mapping the early language environment using all-day recordings and automated analysis. American journal of speech-language pathology, 26 2:248-265, 2017. URL: https://doi.org/10.1044/2016\\_ajslp-15-0169, doi:10.1044/2016\\_ajslp-15-0169.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "gilkerson2017mappingtheearly",
    "bibtex": "@article{gilkerson2017mappingtheearly,\n    author = \"Gilkerson, J. and Richards, J. and Warren, S. and Montgomery, J. and Greenwood, C. and Oller, D. Kimbrough and Hansen, J. and Paul, T.\",\n    title = \"Mapping the Early Language Environment Using All-Day Recordings and Automated Analysis.\",\n    year = \"2017\",\n    journal = \"American journal of speech-language pathology\",\n    volume = \"26 2\",\n    pages = \"\n248-265\n\",\n    doi = \"10.1044/2016\\_ajslp-15-0169\",\n    url = \"https://doi.org/10.1044/2016\\_ajslp-15-0169\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1044/2016_ajslp-15-0169",
    "doc_id": "1d457d69a8d4b2a8",
    "formatted_citation": "J. Gilkerson, J. Richards, S. Warren, J. Montgomery, C. Greenwood, D. Kimbrough Oller, J. Hansen, and T. Paul. Mapping the early language environment using all-day recordings and automated analysis. American journal of speech-language pathology, 26 2:248-265, 2017. URL: https://doi.org/10.1044/2016\\_ajslp-15-0169, doi:10.1044/2016\\_ajslp-15-0169. This article has 329 citations and is from a domain leading peer-reviewed journal."
  },
  "Multilingual Children: Beyond Myths and Toward Best Practices and commentaries": {
    "title": "Multilingual Children: Beyond Myths and Toward Best Practices and commentaries",
    "authors": [
      "Allyssa McCabe",
      "Marc H. Bornstein",
      "Alison Wishard Guerra",
      "Yana Kuchirko",
      "Mariela Páez",
      "Catherine S. Tamis‐LeMonda",
      "Carolyn Brockmeyer Cates",
      "Kathy Hirsh‐Pasek",
      "Gigliana Melzi",
      "Lulu Song",
      "Roberta Golinkoff",
      "Erika Hoff",
      "Alan Mendelsohn"
    ],
    "doi": "10.1002/j.2379-3988.2013.tb00077.x",
    "url": "https://doi.org/10.1002/j.2379-3988.2013.tb00077.x",
    "journal": "Social policy report",
    "year": 2013,
    "genre": "journalArticle",
    "link_attachments": [
      "https://files.eric.ed.gov/fulltext/ED595539.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1-37",
    "issue": "4",
    "volume": "27",
    "publisher": "Wiley",
    "abstract": null,
    "publication_date": "2013-12-01T00:00:00",
    "citation_count": 50,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "mccabe2013multilingualchildrenbeyond",
    "dockey": "ac9985f1640cdf78",
    "citation": "Allyssa McCabe, Marc H. Bornstein, Alison Wishard Guerra, Yana Kuchirko, Mariela Páez, Catherine S. Tamis‐LeMonda, Carolyn Brockmeyer Cates, Kathy Hirsh‐Pasek, Gigliana Melzi, Lulu Song, Roberta Golinkoff, Erika Hoff, and Alan Mendelsohn. Multilingual children: beyond myths and toward best practices and commentaries. Social policy report, 27:1-37, Dec 2013. URL: https://doi.org/10.1002/j.2379-3988.2013.tb00077.x, doi:10.1002/j.2379-3988.2013.tb00077.x.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "mccabe2013multilingualchildrenbeyond",
    "bibtex": "@article{mccabe2013multilingualchildrenbeyond,\n    author = \"McCabe, Allyssa and Bornstein, Marc H. and Guerra, Alison Wishard and Kuchirko, Yana and Páez, Mariela and Tamis‐LeMonda, Catherine S. and Cates, Carolyn Brockmeyer and Hirsh‐Pasek, Kathy and Melzi, Gigliana and Song, Lulu and Golinkoff, Roberta and Hoff, Erika and Mendelsohn, Alan\",\n    title = \"Multilingual Children: Beyond Myths and Toward Best Practices and commentaries\",\n    year = \"2013\",\n    journal = \"Social policy report\",\n    volume = \"27\",\n    pages = \"1-37\",\n    month = \"Dec\",\n    doi = \"10.1002/j.2379-3988.2013.tb00077.x\",\n    url = \"https://doi.org/10.1002/j.2379-3988.2013.tb00077.x\",\n    publisher = \"Wiley\",\n    issue = \"4\",\n    issn = \"1075-7031\"\n}\n",
    "issn": "1075-7031",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1002/j.2379-3988.2013.tb00077.x",
    "doc_id": "ac9985f1640cdf78",
    "formatted_citation": "Allyssa McCabe, Marc H. Bornstein, Alison Wishard Guerra, Yana Kuchirko, Mariela Páez, Catherine S. Tamis‐LeMonda, Carolyn Brockmeyer Cates, Kathy Hirsh‐Pasek, Gigliana Melzi, Lulu Song, Roberta Golinkoff, Erika Hoff, and Alan Mendelsohn. Multilingual children: beyond myths and toward best practices and commentaries. Social policy report, 27:1-37, Dec 2013. URL: https://doi.org/10.1002/j.2379-3988.2013.tb00077.x, doi:10.1002/j.2379-3988.2013.tb00077.x. This article has 50 citations."
  },
  "Language acquisition in a multilingual society: English vocabulary norms and predictors in Singaporean children": {
    "title": "Language acquisition in a multilingual society: English vocabulary norms and predictors in Singaporean children",
    "authors": [
      "Leher Singh",
      "QiQi Cheng",
      "Seok Hui Tan",
      "Agnes Tan",
      "Yen Ling Low"
    ],
    "doi": "10.1111/cdev.13676",
    "url": "https://doi.org/10.1111/cdev.13676",
    "journal": "Child Development",
    "year": 2021,
    "genre": "journalArticle",
    "link_attachments": [
      "http://www.ncbi.nlm.nih.gov/pubmed/34672368"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Child",
      "Child Language",
      "Female",
      "Humans",
      "Infant",
      "Language",
      "Language Development",
      "Language Tests",
      "Male",
      "Multilingualism",
      "Vocabulary"
    ],
    "pages": "288-305",
    "issue": "1",
    "volume": "93",
    "publisher": "Wiley",
    "abstract": "In this study, infant vocabulary development was tracked in a multilingual society (Singapore) within a socioeconomically diverse sample. The sample comprised 1316 infants from 17.4 to 27.7 months (669 females, 647 males; 88% Chinese race, 4% Malay, 4% Indian, and 0.004% mixed-race [4% declined to provide race information]). Children varied in English language exposure and socioeconomic status. Analyses focused on identifying demographic predictors of English vocabulary size in multilingually exposed infants. Adaptations of the Macarthur-Bates Communicative Development Inventory for English, Mandarin, and Malay are provided as well as English vocabulary norms that account for variation in English exposure. This manuscript reports the first set of English language norms-calibrated to English exposure-for multilingual infants in a non-Western setting.",
    "publication_date": "2021-10-21T00:00:00",
    "citation_count": 12,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 3,
    "arxiv_id": null,
    "docname": "singh2021languageacquisitionin",
    "dockey": "d292dd4e452937be",
    "citation": "Leher Singh, QiQi Cheng, Seok Hui Tan, Agnes Tan, and Yen Ling Low. Language acquisition in a multilingual society: english vocabulary norms and predictors in singaporean children. Child Development, 93:288-305, Oct 2021. URL: https://doi.org/10.1111/cdev.13676, doi:10.1111/cdev.13676.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "singh2021languageacquisitionin",
    "bibtex": "@article{singh2021languageacquisitionin,\n    author = \"Singh, Leher and Cheng, QiQi and Tan, Seok Hui and Tan, Agnes and Low, Yen Ling\",\n    title = \"Language acquisition in a multilingual society: English vocabulary norms and predictors in Singaporean children\",\n    year = \"2021\",\n    journal = \"Child Development\",\n    volume = \"93\",\n    pages = \"288-305\",\n    month = \"Oct\",\n    doi = \"10.1111/cdev.13676\",\n    url = \"https://doi.org/10.1111/cdev.13676\",\n    publisher = \"Wiley\",\n    issue = \"1\",\n    issn = \"0009-3920\"\n}\n",
    "issn": "0009-3920",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1111/cdev.13676",
    "doc_id": "d292dd4e452937be",
    "formatted_citation": "Leher Singh, QiQi Cheng, Seok Hui Tan, Agnes Tan, and Yen Ling Low. Language acquisition in a multilingual society: english vocabulary norms and predictors in singaporean children. Child Development, 93:288-305, Oct 2021. URL: https://doi.org/10.1111/cdev.13676, doi:10.1111/cdev.13676. This article has 12 citations and is from a highest quality peer-reviewed journal."
  },
  "Multilingual children's language use: The case of “the return of Superman” show": {
    "title": "Multilingual children's language use: The case of “the return of Superman” show",
    "authors": [
      "Amalika, S.",
      "Putri, C. D. M.",
      "Susanto, D."
    ],
    "doi": null,
    "url": null,
    "journal": "Reimagining Innovation in Education and Social Sciences",
    "year": 2023,
    "genre": "bookSection",
    "link_attachments": [
      "https://s3-euw1-ap-pe-df-pch-content-store-p.s3.eu-west-1.amazonaws.com/9781003366683/772dc5da-5ce5-43c8-a9c8-02eecb8772f8/chapters/chapter14.pdf?AWSAccessKeyId=ASIAQFVOSJ57QWPA6M4X&Expires=1719460374&Signature=ErjlLmWDTwDrqx7CsBk15MZx0C4%3D&response-content-disposition=attachment%3B%20filename%3D%229781003366683_chapterpdf.pdf%22&x-amz-security-token=IQoJb3JpZ2luX2VjEDkaCXVzLWVhc3QtMSJGMEQCIG%2BbavPhgwvcZS9Vaq%2B4LMT0bXJCVn7KlaZ3YHZDu0fVAiAVS25cqVJVIwrtHFI2zrd01TKgQFbaFuaFtgRpoWQ1jyqWBAji%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAQaDDAxMjE3NzI2NDUxMSIMUVdhIOPbYtEayuJWKuoDDwBUl7tV5GSTJnMZfPWxSm2hZLoKvi%2FeTOKa3HQxnj0hSLImZf%2BsV3Yn4FSeiUilH77QMLnVvJ7mvXD6SYHNb4kupP2Xkx63n%2Fk6mD9L2G%2BmggeXucKo4kPEHIJq4paPIFXLBPk9y8BBYPVbzEFbixQAnOo9bM0bnFq387v2Z0suou%2FTxCaNmw9pHFTwDvpOS6GUBGIISUHPM14lhXTfYl9ZD6yrM10DIwrJrTS2rrRkd64%2BVDPzczdY8AFq1GNjviLoMYJ9Eny%2BxZyeDlKu4nmVCfU5sb1068Akrfhphati1Fn%2B7IH0yXaPlAg3QOKCZjoWTjzA%2B0UZbMgzckESljgf11rjXZR3E1ZAhJ0ly14H%2FrDttZ0qJJr3j3LbwwsgB4M8DOZR3EDXdaLEGEHtnnTTtTarFyC9Gi9glZ2i8lUFaBb9tfKHzPLHwZMqFVahnopptws4ffHc9TvGo%2BvScmcQoLVu8kjHnJIQmanuJmFPlLh9TWiVJlfauNocJMCDPoeGdL6UXN0tA8BOC53WPOvw0xQDY5KLgeMEZkj%2B6W14IFH62aYUYpb0Hxi%2FSjhATB7lA0SigsYJh30UO%2FIYVGzxnQzWS%2FWr9NGVjzdT%2BzEL2so8lEy41sGYSywhWGPpFB4%2BIBLFlMX8OzD58fKzBjqmAaG%2FJJklSCe8He3wCEmJhHwWcOWzgSh%2FcF6qeWgzLkkyvwaOit2gxT2D%2BhApttAkVw1WYRTrA9B%2FJKM9or6ljVdmtcJrnPShj%2FzXjerSbokOFoZCs%2BShkChhBvNhhmoaGTeIsmpePBKBO8JjE9BkwasxZyMqNtBrxnpAspIveTbpsxaEULitHhe7ynqiSVigBfsn6HLR6jM09J2noIY%2BJjM1bzrMMcs%3D"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "Routledge",
    "abstract": "This research has conducted descriptive qualitative method and Barron-Hauwaert's theory about multilingual children's language pattern usage, in order to investigate multilingual children's language use in the community through the influence of their closest environment represented in ‘The Return of Superman’ variety shows in Korea. The subject of the study is ‘Park Naeun’ as well as her multicultural family. The researchers would use the online documentation technique to collect data from selected videos on YouTube channels, as described in the three research questions. The results from the findings demonstrate that Park Naeun was capable of speaking using both the majority and minority languages as a multilingual child. The influence of parents’ language on the children's language is a signal that contributes to the success of the children's language-learning process. When parents can maintain their children's first language, they could also maintain their children's next language acquisition. This analysis revealed that Park Naeun changed their language use for many reasons: politeness, meaningful conversations, and similar perceptions.",
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://s3-euw1-ap-pe-df-pch-content-store-p.s3.eu-west-1.amazonaws.com/9781003366683/772dc5da-5ce5-43c8-a9c8-02eecb8772f8/chapters/chapter14.pdf?AWSAccessKeyId=ASIAQFVOSJ57QWPA6M4X&Expires=1719460374&Signature=ErjlLmWDTwDrqx7CsBk15MZx0C4%3D&response-content-disposition=attachment%3B%20filename%3D%229781003366683_chapterpdf.pdf%22&x-amz-security-token=IQoJb3JpZ2luX2VjEDkaCXVzLWVhc3QtMSJGMEQCIG%2BbavPhgwvcZS9Vaq%2B4LMT0bXJCVn7KlaZ3YHZDu0fVAiAVS25cqVJVIwrtHFI2zrd01TKgQFbaFuaFtgRpoWQ1jyqWBAji%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAQaDDAxMjE3NzI2NDUxMSIMUVdhIOPbYtEayuJWKuoDDwBUl7tV5GSTJnMZfPWxSm2hZLoKvi%2FeTOKa3HQxnj0hSLImZf%2BsV3Yn4FSeiUilH77QMLnVvJ7mvXD6SYHNb4kupP2Xkx63n%2Fk6mD9L2G%2BmggeXucKo4kPEHIJq4paPIFXLBPk9y8BBYPVbzEFbixQAnOo9bM0bnFq387v2Z0suou%2FTxCaNmw9pHFTwDvpOS6GUBGIISUHPM14lhXTfYl9ZD6yrM10DIwrJrTS2rrRkd64%2BVDPzczdY8AFq1GNjviLoMYJ9Eny%2BxZyeDlKu4nmVCfU5sb1068Akrfhphati1Fn%2B7IH0yXaPlAg3QOKCZjoWTjzA%2B0UZbMgzckESljgf11rjXZR3E1ZAhJ0ly14H%2FrDttZ0qJJr3j3LbwwsgB4M8DOZR3EDXdaLEGEHtnnTTtTarFyC9Gi9glZ2i8lUFaBb9tfKHzPLHwZMqFVahnopptws4ffHc9TvGo%2BvScmcQoLVu8kjHnJIQmanuJmFPlLh9TWiVJlfauNocJMCDPoeGdL6UXN0tA8BOC53WPOvw0xQDY5KLgeMEZkj%2B6W14IFH62aYUYpb0Hxi%2FSjhATB7lA0SigsYJh30UO%2FIYVGzxnQzWS%2FWr9NGVjzdT%2BzEL2so8lEy41sGYSywhWGPpFB4%2BIBLFlMX8OzD58fKzBjqmAaG%2FJJklSCe8He3wCEmJhHwWcOWzgSh%2FcF6qeWgzLkkyvwaOit2gxT2D%2BhApttAkVw1WYRTrA9B%2FJKM9or6ljVdmtcJrnPShj%2FzXjerSbokOFoZCs%2BShkChhBvNhhmoaGTeIsmpePBKBO8JjE9BkwasxZyMqNtBrxnpAspIveTbpsxaEULitHhe7ynqiSVigBfsn6HLR6jM09J2noIY%2BJjM1bzrMMcs%3D",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  },
  "Bi- and Multilingual Language Acquisition": {
    "title": "Bi- and Multilingual Language Acquisition",
    "authors": [
      "Hua, Zhu",
      "Wei, Li"
    ],
    "doi": null,
    "url": null,
    "journal": null,
    "year": null,
    "genre": "journalArticle",
    "link_attachments": [
      "https://d1wqtxts1xzle7.cloudfront.net/7559919/Bi_and_multilingual_acquisition0001-libre.pdf?1390852224=&response-content-disposition=inline%3B+filename%3DBi_and_Multilingual_Language_Acquisition.pdf&Expires=1719463337&Signature=gEFlAx47GJ0GiYGoHE03dr9S322QH7OWTJvH820UX0I6YIZqwab2~Rcc~3gK6zdR~Atzk3GuwFoTWeXCxYkA1t6zrh4YYesRw3hBKLHzQyS6aveSii18tEaAwSi9og84SLmnUrGZNG2ZI10QgJwyWk7nWaUDVdTGAmYd6votkTbVuH4pQ4RQ4aj7kmMmLtlnPvgroTtDqhXFq7VKieAFuIHvDqrjmfMdOfbeR0xAYsxYrEiGi-Y9LyTark02q~cI~4LVPQv7QCv2k4H4gOelT1LA4SKcqLWEgjt6WvHxSQb50d-R5fnIwpfjPReevAY9TTESRwvcmvOoI5GsCLSNlg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": null,
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://d1wqtxts1xzle7.cloudfront.net/7559919/Bi_and_multilingual_acquisition0001-libre.pdf?1390852224=&response-content-disposition=inline%3B+filename%3DBi_and_Multilingual_Language_Acquisition.pdf&Expires=1719463337&Signature=gEFlAx47GJ0GiYGoHE03dr9S322QH7OWTJvH820UX0I6YIZqwab2~Rcc~3gK6zdR~Atzk3GuwFoTWeXCxYkA1t6zrh4YYesRw3hBKLHzQyS6aveSii18tEaAwSi9og84SLmnUrGZNG2ZI10QgJwyWk7nWaUDVdTGAmYd6votkTbVuH4pQ4RQ4aj7kmMmLtlnPvgroTtDqhXFq7VKieAFuIHvDqrjmfMdOfbeR0xAYsxYrEiGi-Y9LyTark02q~cI~4LVPQv7QCv2k4H4gOelT1LA4SKcqLWEgjt6WvHxSQb50d-R5fnIwpfjPReevAY9TTESRwvcmvOoI5GsCLSNlg__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  },
  "Code-mixing and language dominance: bilingual, trilingual and multilingual children compared": {
    "title": "Code-mixing and language dominance: bilingual, trilingual and multilingual children compared",
    "authors": [
      "Meike Poeste",
      "Natascha Müller",
      "Laia Arnaus Gil"
    ],
    "doi": "10.1080/14790718.2019.1569017",
    "url": "https://doi.org/10.1080/14790718.2019.1569017",
    "journal": "International Journal of Multilingualism",
    "year": 2019,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.romanistik.uni-wuppertal.de/fileadmin/romanistik/pdf/poeste_m-mueller_n-arnausgil_l19.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "459-491",
    "issue": "4",
    "volume": "16",
    "publisher": "Informa UK Limited",
    "abstract": null,
    "publication_date": "2019-02-01T00:00:00",
    "citation_count": 28,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 2,
    "arxiv_id": null,
    "docname": "poeste2019codemixingandlanguage",
    "dockey": "4a6088365231c85e",
    "citation": "Meike Poeste, Natascha Müller, and Laia Arnaus Gil. Code-mixing and language dominance: bilingual, trilingual and multilingual children compared. International Journal of Multilingualism, 16:459-491, Feb 2019. URL: https://doi.org/10.1080/14790718.2019.1569017, doi:10.1080/14790718.2019.1569017.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "poeste2019codemixingandlanguage",
    "bibtex": "@article{poeste2019codemixingandlanguage,\n    author = \"Poeste, Meike and Müller, Natascha and Gil, Laia Arnaus\",\n    title = \"Code-mixing and language dominance: bilingual, trilingual and multilingual children compared\",\n    year = \"2019\",\n    journal = \"International Journal of Multilingualism\",\n    volume = \"16\",\n    pages = \"459-491\",\n    month = \"Feb\",\n    doi = \"10.1080/14790718.2019.1569017\",\n    url = \"https://doi.org/10.1080/14790718.2019.1569017\",\n    publisher = \"Informa UK Limited\",\n    issue = \"4\",\n    issn = \"1479-0718\"\n}\n",
    "issn": "1479-0718",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1080/14790718.2019.1569017",
    "doc_id": "4a6088365231c85e",
    "formatted_citation": "Meike Poeste, Natascha Müller, and Laia Arnaus Gil. Code-mixing and language dominance: bilingual, trilingual and multilingual children compared. International Journal of Multilingualism, 16:459-491, Feb 2019. URL: https://doi.org/10.1080/14790718.2019.1569017, doi:10.1080/14790718.2019.1569017. This article has 28 citations and is from a domain leading peer-reviewed journal."
  },
  "19. Translation to Practice: Transcription of the Speech of Multilingual Children": {
    "title": "Ethics in translation practice",
    "authors": [
      "Phillippa May Bennett"
    ],
    "doi": "10.4312/vh.29.1.31-52",
    "url": "https://doi.org/10.4312/vh.29.1.31-52",
    "journal": "Verba Hispanica",
    "year": 2021,
    "genre": "bookSection",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "31-52",
    "issue": "1",
    "volume": "29",
    "publisher": "University of Ljubljana",
    "abstract": "19. Translation to Practice: Transcription of the Speech of Multilingual Children was published in Multilingual Aspects of Speech Sound Disorders in Children on page 182.",
    "publication_date": "2021-12-27T00:00:00",
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "bennett2021ethicsintranslation",
    "dockey": "275dc93e12bc48e8",
    "citation": "Phillippa May Bennett. Ethics in translation practice. Verba Hispanica, 29:31-52, Dec 2021. URL: https://doi.org/10.4312/vh.29.1.31-52, doi:10.4312/vh.29.1.31-52.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "bennett2021ethicsintranslation",
    "bibtex": "@article{bennett2021ethicsintranslation,\n    author = \"Bennett, Phillippa May\",\n    title = \"Ethics in translation practice\",\n    year = \"2021\",\n    journal = \"Verba Hispanica\",\n    volume = \"29\",\n    pages = \"31-52\",\n    month = \"Dec\",\n    doi = \"10.4312/vh.29.1.31-52\",\n    url = \"https://doi.org/10.4312/vh.29.1.31-52\",\n    publisher = \"University of Ljubljana\",\n    issue = \"1\",\n    issn = \"2350-4250\"\n}\n",
    "issn": "2350-4250",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.4312/vh.29.1.31-52",
    "doc_id": "275dc93e12bc48e8",
    "formatted_citation": "Phillippa May Bennett. Ethics in translation practice. Verba Hispanica, 29:31-52, Dec 2021. URL: https://doi.org/10.4312/vh.29.1.31-52, doi:10.4312/vh.29.1.31-52. This article has 2 citations."
  },
  "Multilingual Transfer Learning for Children Automatic Speech Recognition": {
    "title": "Multilingual Speech Recognition",
    "authors": [
      "Alex Waibel",
      "Hagen Soltau",
      "Tanja Schultz",
      "Thomas Schaaf",
      "Florian Metze"
    ],
    "doi": "10.1007/978-3-662-04230-4_3",
    "url": "https://doi.org/10.1007/978-3-662-04230-4_3",
    "journal": "Artificial Intelligence",
    "year": 2000,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "33-45",
    "issue": null,
    "volume": null,
    "publisher": "Springer Berlin Heidelberg",
    "abstract": "Despite recent advances in automatic speech recognition (ASR), the recognition of children’s speech still remains a significant challenge. This is mainly due to the high acoustic variability and the limited amount of available training data. The latter problem is particularly evident in languages other than English, which are usually less-resourced. In the current paper, we address children ASR in a number of less-resourced languages by combining several small-sized children speech corpora from these languages. In particular, we address the following research question: Does a novel two-step training strategy in which multilingual learning is followed by language-specific transfer learning outperform conventional single language/task training for children speech, as well as multilingual and transfer learning alone? Based on previous experimental results with English, we hypothesize that multilingual learning provides a better generalization of the underlying characteristics of children’s speech. Our results provide a positive answer to our research question, by showing that using transfer learning on top of a multilingual model for an unseen language outperforms conventional single language-specific learning.",
    "publication_date": "2000-01-01T00:00:00",
    "citation_count": 92,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 3,
    "arxiv_id": null,
    "docname": "waibel2000multilingualspeechrecognition",
    "dockey": "9095850879dda738",
    "citation": "Alex Waibel, Hagen Soltau, Tanja Schultz, Thomas Schaaf, and Florian Metze. Multilingual speech recognition. Artificial Intelligence, pages 33-45, Jan 2000. URL: https://doi.org/10.1007/978-3-662-04230-4\\_3, doi:10.1007/978-3-662-04230-4\\_3.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "waibel2000multilingualspeechrecognition",
    "bibtex": "@article{waibel2000multilingualspeechrecognition,\n    author = \"Waibel, Alex and Soltau, Hagen and Schultz, Tanja and Schaaf, Thomas and Metze, Florian\",\n    title = \"Multilingual Speech Recognition\",\n    year = \"2000\",\n    journal = \"Artificial Intelligence\",\n    pages = \"33-45\",\n    month = \"Jan\",\n    doi = \"10.1007/978-3-662-04230-4\\_3\",\n    url = \"https://doi.org/10.1007/978-3-662-04230-4\\_3\",\n    publisher = \"Springer Berlin Heidelberg\",\n    issn = \"1431-0066\"\n}\n",
    "issn": "1431-0066",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1007/978-3-662-04230-4_3",
    "doc_id": "9095850879dda738",
    "formatted_citation": "Alex Waibel, Hagen Soltau, Tanja Schultz, Thomas Schaaf, and Florian Metze. Multilingual speech recognition. Artificial Intelligence, pages 33-45, Jan 2000. URL: https://doi.org/10.1007/978-3-662-04230-4\\_3, doi:10.1007/978-3-662-04230-4\\_3. This article has 92 citations and is from a highest quality peer-reviewed journal."
  },
  "SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant’s Perspective": {
    "title": "SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant’s Perspective",
    "authors": [
      "Jessica Sullivan",
      "Michelle Mei",
      "Andrew Perfors",
      "Erica H Wojcik",
      "Michael C. Frank"
    ],
    "doi": "10.1162/opmi_a_00039",
    "url": "https://doi.org/10.1162/opmi_a_00039",
    "journal": "Open Mind",
    "year": 2021,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.researchgate.net/publication/349840130_SAYCam_A_Large_Longitudinal_Audiovisual_Dataset_Recorded_From_the_Infant's_Perspective/fulltext/60948af5a6fdccaebd11e5f4/SAYCam-A-Large-Longitudinal-Audiovisual-Dataset-Recorded-From-the-Infants-Perspective.pdf",
      "https://www.researchgate.net/publication/349840130_SAYCam_A_Large_Longitudinal_Audiovisual_Dataset_Recorded_From_the_Infant%27s_Perspective"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "20-29",
    "issue": null,
    "volume": "5",
    "publisher": "MIT Press",
    "abstract": "We introduce a new resource: the SAYCam corpus. Infants aged 6–32 months wore a head-mounted camera for approximately 2 hr per week, over the course of approximately two-and-a-half years. The result is a large, naturalistic, longitudinal dataset of infant- and child-perspective videos. Over 200,000 words of naturalistic speech have already been transcribed. Similarly, the dataset is searchable using a number of criteria (e.g., age of participant, location, setting, objects present). The resulting dataset will be of broad use to psychologists, linguists, and computer scientists.",
    "publication_date": "2021-01-01T00:00:00",
    "citation_count": 108,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "sullivan2021saycamalarge",
    "dockey": "d1bcf6f738c7030d",
    "citation": "Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica H Wojcik, and Michael C. Frank. Saycam: a large, longitudinal audiovisual dataset recorded from the infant’s perspective. Open Mind, 5:20-29, Jan 2021. URL: https://doi.org/10.1162/opmi\\_a\\_00039, doi:10.1162/opmi\\_a\\_00039.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "sullivan2021saycamalarge",
    "bibtex": "@article{sullivan2021saycamalarge,\n    author = \"Sullivan, Jessica and Mei, Michelle and Perfors, Andrew and Wojcik, Erica H and Frank, Michael C.\",\n    title = \"SAYCam: A Large, Longitudinal Audiovisual Dataset Recorded From the Infant’s Perspective\",\n    year = \"2021\",\n    journal = \"Open Mind\",\n    volume = \"5\",\n    pages = \"20-29\",\n    month = \"Jan\",\n    doi = \"10.1162/opmi\\_a\\_00039\",\n    url = \"https://doi.org/10.1162/opmi\\_a\\_00039\",\n    publisher = \"MIT Press\",\n    issn = \"2470-2986\"\n}\n",
    "issn": "2470-2986",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1162/opmi_a_00039",
    "doc_id": "d1bcf6f738c7030d",
    "formatted_citation": "Jessica Sullivan, Michelle Mei, Andrew Perfors, Erica H Wojcik, and Michael C. Frank. Saycam: a large, longitudinal audiovisual dataset recorded from the infant’s perspective. Open Mind, 5:20-29, Jan 2021. URL: https://doi.org/10.1162/opmi\\_a\\_00039, doi:10.1162/opmi\\_a\\_00039. This article has 108 citations and is from a peer-reviewed journal."
  },
  "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark": {
    "title": "ML-SUPERB: Multilingual Speech Universal PERformance Benchmark",
    "authors": [
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None"
    ],
    "doi": "10.21437/interspeech.2023-1316",
    "url": "https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.html",
    "journal": "INTERSPEECH 2023",
    "year": 2023,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "884-888",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of SelfSupervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from highresource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.",
    "publication_date": "2023-08-14T00:00:00",
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "none2023mlsuperbmultilingualspeech",
    "dockey": "7f2d70e31b94888b",
    "citation": "None None, None None, None None, None None, None None, None None, None None, None None, None None, None None, and None None. Ml-superb: multilingual speech universal performance benchmark. INTERSPEECH 2023, Aug 2023. URL: https://doi.org/10.21437/interspeech.2023-1316, doi:10.21437/interspeech.2023-1316.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "none2023mlsuperbmultilingualspeech",
    "bibtex": "@article{none2023mlsuperbmultilingualspeech,\n    author = \"None, None and None, None and None, None and None, None and None, None and None, None and None, None and None, None and None, None and None, None and None, None\",\n    title = \"ML-SUPERB: Multilingual Speech Universal PERformance Benchmark\",\n    year = \"2023\",\n    journal = \"INTERSPEECH 2023\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2023-1316\",\n    url = \"https://doi.org/10.21437/interspeech.2023-1316\",\n    publisher = \"ISCA\"\n}\n",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2023-1316",
    "doc_id": "7f2d70e31b94888b",
    "formatted_citation": "None None, None None, None None, None None, None None, None None, None None, None None, None None, None None, and None None. Ml-superb: multilingual speech universal performance benchmark. INTERSPEECH 2023, Aug 2023. URL: https://doi.org/10.21437/interspeech.2023-1316, doi:10.21437/interspeech.2023-1316."
  },
  "Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models": {
    "title": "Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models",
    "authors": [
      "Ruchao Fan",
      "Natarajan Balaji Shankar",
      "Abeer Alwan"
    ],
    "doi": "10.21437/interspeech.2024-1353",
    "url": "https://doi.org/10.21437/interspeech.2024-1353",
    "journal": "Interspeech 2024",
    "year": 2024,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": "5173-5177",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Speech foundation models (SFMs) have achieved state-ofthe-art results for various speech tasks in supervised (e.g. Whisper) or self-supervised systems (e.g. WavLM). However, the performance of SFMs for child ASR has not been systematically studied. In addition, there is no benchmark for child ASR with standard evaluations, making the comparisons of novel ideas difficult. In this paper, we initiate and present a comprehensive benchmark on several child speech databases based on various SFMs (Whisper, Wav2vec2.0, HuBERT, and WavLM). Moreover, we investigate finetuning strategies by comparing various data augmentation and parameter-efficient finetuning (PEFT) methods. We observe that the behaviors of these methods are different when the model size increases. For example, PEFT matches the performance of full finetuning for large models but worse for small models. To stabilize finetuning using augmented data, we propose a perturbation invariant finetuning (PIF) loss as a regularization.1.",
    "publication_date": "2024-09-01T00:00:00",
    "citation_count": 0,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2406.10507",
    "docname": "fan2024benchmarkingchildrensasr",
    "dockey": "94f38f999d1796d7",
    "citation": "Ruchao Fan, Natarajan Balaji Shankar, and Abeer Alwan. Benchmarking children's asr with supervised and self-supervised speech foundation models. Interspeech 2024, pages 5173-5177, Sep 2024. URL: https://doi.org/10.21437/interspeech.2024-1353, doi:10.21437/interspeech.2024-1353.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "fan2024benchmarkingchildrensasr",
    "bibtex": "@article{fan2024benchmarkingchildrensasr,\n    author = \"Fan, Ruchao and Shankar, Natarajan Balaji and Alwan, Abeer\",\n    title = \"Benchmarking Children's ASR with Supervised and Self-supervised Speech Foundation Models\",\n    year = \"2024\",\n    journal = \"Interspeech 2024\",\n    pages = \"5173-5177\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2024-1353\",\n    url = \"https://doi.org/10.21437/interspeech.2024-1353\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2024-1353",
    "doc_id": "94f38f999d1796d7",
    "formatted_citation": "Ruchao Fan, Natarajan Balaji Shankar, and Abeer Alwan. Benchmarking children's asr with supervised and self-supervised speech foundation models. Interspeech 2024, pages 5173-5177, Sep 2024. URL: https://doi.org/10.21437/interspeech.2024-1353, doi:10.21437/interspeech.2024-1353. This article has 0 citations."
  },
  "The OGI kids’ speech corpus and recognizers": {
    "title": "The OGI kids² speech corpus and recognizers",
    "authors": [
      "Khaldoun Shobaki",
      "John-Paul Hosom",
      "Ronald A. Cole"
    ],
    "doi": "10.21437/icslp.2000-800",
    "url": "https://doi.org/10.21437/icslp.2000-800",
    "journal": "6th International Conference on Spoken Language Processing (ICSLP 2000)",
    "year": 2000,
    "genre": "book",
    "link_attachments": [
      "https://www.researchgate.net/profile/Ronald-Cole/publication/221488491_The_OGI_kids'_speech_corpus_and_recognizers/links/53f5f4a10cf2888a7491fd21/The-OGI-kids-speech-corpus-and-recognizers.pdf",
      "https://www.researchgate.net/publication/221488491_The_OGI_kids%27_speech_corpus_and_recognizers"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "vol.4,258-261-0",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "We describe a corpus of children's speech, called the OGI Kids' Speech corpus, and a speaker- and vocabulary-independent recognition system trained and evaluated with these data. The corpus is composed of both prompted and spontaneous speech from 1100 children from kindergarten through grade 10. The prompted speech was presented as text appearing below an animated character (Baldi) that produced accurate visible speech synchronized with recorded prompts. The speech and text consists of isolated words, sentences, and digit strings. A phonetic recognizer was trained using an HMM/ANN framework, with training data taken from intervals of speech associated with phonetic segments in the isolated words in the corpus. Phonetic segments were derived using automatic phonetic alignment. To find out how well the recognizer is able to generalize to new words not found in the training set, we performed two test-set evaluations: one using a new set of utterances from the set of 205 words spoken in isolation (similar to the data used to train the recognizer) and one using words from the prompted sentences. Results were dramatically different (97.5% for isolated vs. 37.9% for words in sentences), and we explore methods that may be used to improve the recognizer's ability to generalize to new words.",
    "publication_date": "2000-10-16T00:00:00",
    "citation_count": 18,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "shobaki2000theogikids²",
    "dockey": "0453819a0c5e5a4e",
    "citation": "Khaldoun Shobaki, John-Paul Hosom, and Ronald A. Cole. The ogi kids² speech corpus and recognizers. 6th International Conference on Spoken Language Processing (ICSLP 2000), pages vol.4,258-261-0, Oct 2000. URL: https://doi.org/10.21437/icslp.2000-800, doi:10.21437/icslp.2000-800.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shobaki2000theogikids²",
    "bibtex": "@article{shobaki2000theogikids²,\n    author = \"Shobaki, Khaldoun and Hosom, John-Paul and Cole, Ronald A.\",\n    title = \"The OGI kids² speech corpus and recognizers\",\n    year = \"2000\",\n    journal = \"6th International Conference on Spoken Language Processing (ICSLP 2000)\",\n    pages = \"vol.4,258-261-0\",\n    month = \"Oct\",\n    doi = \"10.21437/icslp.2000-800\",\n    url = \"https://doi.org/10.21437/icslp.2000-800\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/icslp.2000-800",
    "doc_id": "0453819a0c5e5a4e",
    "formatted_citation": "Khaldoun Shobaki, John-Paul Hosom, and Ronald A. Cole. The ogi kids² speech corpus and recognizers. 6th International Conference on Spoken Language Processing (ICSLP 2000), pages vol.4,258-261-0, Oct 2000. URL: https://doi.org/10.21437/icslp.2000-800, doi:10.21437/icslp.2000-800. This article has 18 citations."
  },
  "Robust Speech Recognition via Large-Scale Weak Supervision": {
    "title": "Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision",
    "authors": [
      "Denis A. Gudovskiy",
      "Alec Hodgkinson",
      "Takuya Yamaguchi",
      "Sotaro Tsukizawa"
    ],
    "doi": "10.1109/cvpr42600.2020.00906",
    "url": "https://doi.org/10.1109/cvpr42600.2020.00906",
    "journal": "2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)",
    "year": 2020,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "9038-9046",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.",
    "publication_date": "2020-06-01T00:00:00",
    "citation_count": 59,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "gudovskiy2020deepactivelearning",
    "dockey": "5a05f2a480bb7755",
    "citation": "Denis A. Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, and Sotaro Tsukizawa. Deep active learning for biased datasets via fisher kernel self-supervision. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9038-9046, Jun 2020. URL: https://doi.org/10.1109/cvpr42600.2020.00906, doi:10.1109/cvpr42600.2020.00906.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "gudovskiy2020deepactivelearning",
    "bibtex": "@article{gudovskiy2020deepactivelearning,\n    author = \"Gudovskiy, Denis A. and Hodgkinson, Alec and Yamaguchi, Takuya and Tsukizawa, Sotaro\",\n    title = \"Deep Active Learning for Biased Datasets via Fisher Kernel Self-Supervision\",\n    year = \"2020\",\n    journal = \"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\n    pages = \"9038-9046\",\n    month = \"Jun\",\n    doi = \"10.1109/cvpr42600.2020.00906\",\n    url = \"https://doi.org/10.1109/cvpr42600.2020.00906\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/cvpr42600.2020.00906",
    "doc_id": "5a05f2a480bb7755",
    "formatted_citation": "Denis A. Gudovskiy, Alec Hodgkinson, Takuya Yamaguchi, and Sotaro Tsukizawa. Deep active learning for biased datasets via fisher kernel self-supervision. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9038-9046, Jun 2020. URL: https://doi.org/10.1109/cvpr42600.2020.00906, doi:10.1109/cvpr42600.2020.00906. This article has 59 citations."
  },
  "Scaling End-to-End Models for Large-Scale Multilingual ASR": {
    "title": "Scaling End-to-End Models for Large-Scale Multilingual ASR",
    "authors": [
      "Bo Li",
      "Ruoming Pang",
      "Tara N. Sainath",
      "Anmol Gulati",
      "Yu Zhang",
      "James Qin",
      "Parisa Haghani",
      "W. R. Huang",
      "Min Ma",
      "Junwen Bai"
    ],
    "doi": "10.1109/asru51503.2021.9687871",
    "url": "https://ieeexplore.ieee.org/abstract/document/9687871",
    "journal": "2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)",
    "year": 2021,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://ieeexplore.ieee.org/abstract/document/9687871",
      "https://arxiv.org/pdf/2104.14830"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Data models",
      "Training",
      "Adaptation models",
      "Buildings",
      "Costs",
      "Interference",
      "large-scale",
      "multilingual speech recognition",
      "Performance gain"
    ],
    "pages": "1011-1018",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "Building ASR models across many languages is a challenging multi-task learning problem due to large variations and heavily unbalanced data. Existing work has shown positive transfer from high resource to low resource languages. However, degradations on high resource languages are commonly observed due to interference from the heterogeneous multilingual data and reduction in per-language capacity. We conduct a capacity study on a 15-language task, with the amount of data per language varying from 7.6K to 53.5K hours. We adopt GShard [1] to efficiently scale up to 10B parameters. Empirically, we find that (1) scaling the number of model parameters is an effective way to solve the capacity bottleneck - our 500M-param model already outperforms monolingual baselines and scaling it to 1B and 10B brought further quality gains; (2) larger models are not only more data efficient, but also more efficient in terms of training cost as measured in TPU days - the 1B-param model reaches the same accuracy at 34% of training time as the 500M-param model; (3) given a fixed capacity budget, adding depth works better than width and large encoders do better than large decoders; (4) with continuous training, they can be adapted to new languages and domains.",
    "publication_date": "2021-12-13T00:00:00",
    "citation_count": 77,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "li2021scalingendtoendmodels",
    "dockey": "a8eb6cb8823118b9",
    "citation": "Bo Li, Ruoming Pang, Tara N. Sainath, Anmol Gulati, Yu Zhang, James Qin, Parisa Haghani, W. R. Huang, Min Ma, and Junwen Bai. Scaling end-to-end models for large-scale multilingual asr. 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1011-1018, Dec 2021. URL: https://doi.org/10.1109/asru51503.2021.9687871, doi:10.1109/asru51503.2021.9687871.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "li2021scalingendtoendmodels",
    "bibtex": "@article{li2021scalingendtoendmodels,\n    author = \"Li, Bo and Pang, Ruoming and Sainath, Tara N. and Gulati, Anmol and Zhang, Yu and Qin, James and Haghani, Parisa and Huang, W. R. and Ma, Min and Bai, Junwen\",\n    title = \"Scaling End-to-End Models for Large-Scale Multilingual ASR\",\n    year = \"2021\",\n    journal = \"2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)\",\n    pages = \"1011-1018\",\n    month = \"Dec\",\n    doi = \"10.1109/asru51503.2021.9687871\",\n    url = \"https://doi.org/10.1109/asru51503.2021.9687871\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/asru51503.2021.9687871",
    "doc_id": "a8eb6cb8823118b9",
    "formatted_citation": "Bo Li, Ruoming Pang, Tara N. Sainath, Anmol Gulati, Yu Zhang, James Qin, Parisa Haghani, W. R. Huang, Min Ma, and Junwen Bai. Scaling end-to-end models for large-scale multilingual asr. 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1011-1018, Dec 2021. URL: https://doi.org/10.1109/asru51503.2021.9687871, doi:10.1109/asru51503.2021.9687871. This article has 77 citations."
  },
  "The Phonbank Database within Talkbank, and a Practical Overview of the Phon Program": {
    "title": "The Phonbank Database within Talkbank, and a Practical Overview of the Phon Program",
    "authors": [
      "Yvan Rose",
      "Gregory J. Hedlund"
    ],
    "doi": "10.4324/9780429320903-19",
    "url": "https://doi.org/10.4324/9780429320903-19",
    "journal": "Manual of Clinical Phonetics",
    "year": 2021,
    "genre": "bookSection",
    "link_attachments": [
      "https://www.researchgate.net/profile/Yvan-Rose/publication/349435799_The_Phonbank_Database_within_Talkbank_and_a_Practical_Overview_of_the_Phon_Program/links/60898bed458515d315e2efe7/The-Phonbank-Database-within-Talkbank-and-a-Practical-Overview-of-the-Phon-Program.pdf",
      "https://www.researchgate.net/profile/Yvan-Rose/publication/349435799_The_Phonbank_Database_within_Talkbank_and_a_Practical_Overview_of_the_Phon_Program/links/60898bed458515d315e2efe7/The-Phonbank-Database-within-Talkbank-and-a-Practical-Overview-of-the-Phon-Program.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "246",
    "issue": null,
    "volume": null,
    "publisher": "Routledge",
    "abstract": null,
    "publication_date": "2021-02-25T00:00:00",
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "rose2021thephonbankdatabase",
    "dockey": "8558a36def332eb3",
    "citation": "Yvan Rose and Gregory J. Hedlund. The phonbank database within talkbank, and a practical overview of the phon program. Manual of Clinical Phonetics, pages 246, Feb 2021. URL: https://doi.org/10.4324/9780429320903-19, doi:10.4324/9780429320903-19.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "rose2021thephonbankdatabase",
    "bibtex": "@article{rose2021thephonbankdatabase,\n    author = \"Rose, Yvan and Hedlund, Gregory J.\",\n    title = \"The Phonbank Database within Talkbank, and a Practical Overview of the Phon Program\",\n    year = \"2021\",\n    journal = \"Manual of Clinical Phonetics\",\n    pages = \"246\",\n    month = \"Feb\",\n    doi = \"10.4324/9780429320903-19\",\n    url = \"https://doi.org/10.4324/9780429320903-19\",\n    publisher = \"Routledge\"\n}\n",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.4324/9780429320903-19",
    "doc_id": "8558a36def332eb3",
    "formatted_citation": "Yvan Rose and Gregory J. Hedlund. The phonbank database within talkbank, and a practical overview of the phon program. Manual of Clinical Phonetics, pages 246, Feb 2021. URL: https://doi.org/10.4324/9780429320903-19, doi:10.4324/9780429320903-19. This article has 2 citations."
  },
  "Cristia: Accuracy of the language environment analysis... - Google Scholar": {
    "title": "Cristia: Accuracy of the language environment analysis... - Google Scholar",
    "authors": null,
    "doi": null,
    "url": "https://scholar.google.com/scholar_lookup?&title=Accuracy%20of%20the%20Language%20Environment%20Analysis%20System%20Segmentation%20and%20Metrics%3A%20A%20Systematic%20Review&journal=Journal%20of%20Speech%2C%20Language%2C%20and%20Hearing%20Research&doi=10.1044%2F2020_JSLHR-19-00017&volume=63&issue=4&pages=1093-1105&publication_year=2020&author=Cristia%2CA&author=Bulgarelli%2CF&author=Bergelson%2CE",
    "journal": null,
    "year": null,
    "genre": "webpage",
    "link_attachments": [
      "https://scholar.google.com/scholar_lookup?&title=Accuracy%20of%20the%20Language%20Environment%20Analysis%20System%20Segmentation%20and%20Metrics%3A%20A%20Systematic%20Review&journal=Journal%20of%20Speech%2C%20Language%2C%20and%20Hearing%20Research&doi=10.1044%2F2020_JSLHR-19-00017&volume=63&issue=4&pages=1093-1105&publication_year=2020&author=Cristia%2CA&author=Bulgarelli%2CF&author=Bergelson%2CE"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": null,
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": "https://scholar.google.com/scholar_lookup?&title=Accuracy%20of%20the%20Language%20Environment%20Analysis%20System%20Segmentation%20and%20Metrics%3A%20A%20Systematic%20Review&journal=Journal%20of%20Speech%2C%20Language%2C%20and%20Hearing%20Research&doi=10.1044%2F2020_JSLHR-19-00017&volume=63&issue=4&pages=1093-1105&publication_year=2020&author=Cristia%2CA&author=Bulgarelli%2CF&author=Bergelson%2CE",
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  },
  "A thorough evaluation of the Language Environment Analysis (LENA) system": {
    "title": "A thorough evaluation of the Language Environment Analysis (LENA) system",
    "authors": [
      "Alejandrina Cristia",
      "Marvin Lavechin",
      "Camila Scaff",
      "Melanie Soderstrom",
      "Caroline Rowland",
      "Okko Räsänen",
      "John Bunce",
      "Elika Bergelson"
    ],
    "doi": "10.3758/s13428-020-01393-5",
    "url": "https://doi.org/10.3758/s13428-020-01393-5",
    "journal": "Behavior Research Methods",
    "year": 2020,
    "genre": "journalArticle",
    "link_attachments": [
      "https://link.springer.com/content/pdf/10.3758%2Fs13428-020-01393-5.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Adult Word Count",
      "Agreement",
      "Child Vocalization Count",
      "Conversational Turn Count",
      "English",
      "Human transcription",
      "LENA",
      "Measurement error",
      "Method comparison",
      "Reliability",
      "Speech technology",
      "Tsimane’"
    ],
    "pages": "467-486",
    "issue": "2",
    "volume": "53",
    "publisher": "Springer Science and Business Media LLC",
    "abstract": "In the previous decade, dozens of studies involving thousands of children across several research disciplines have made use of a combined daylong audio-recorder and automated algorithmic analysis called the LENAⓇ system, which aims to assess children’s language environment. While the system’s prevalence in the language acquisition domain is steadily growing, there are only scattered validation efforts on only some of its key characteristics. Here, we assess the LENAⓇ system’s accuracy across all of its key measures: speaker classification, Child Vocalization Counts (CVC), Conversational Turn Counts (CTC), and Adult Word Counts (AWC). Our assessment is based on manual annotation of clips that have been randomly or periodically sampled out of daylong recordings, collected from (a) populations similar to the system’s original training data (North American English-learning children aged 3-36 months), (b) children learning another dialect of English (UK), and (c) slightly older children growing up in a different linguistic and socio-cultural setting (Tsimane’ learners in rural Bolivia). We find reasonably high accuracy in some measures (AWC, CVC), with more problematic levels of performance in others (CTC, precision of male adults and other children). Statistical analyses do not support the view that performance is worse for children who are dissimilar from the LENAⓇ original training set. Whether LENAⓇ results are accurate enough for a given research, educational, or clinical application depends largely on the specifics at hand. We therefore conclude with a set of recommendations to help researchers make this determination for their goals.",
    "publication_date": "2020-07-29T00:00:00",
    "citation_count": 91,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 2,
    "arxiv_id": null,
    "docname": "cristia2020athoroughevaluation",
    "dockey": "aa7f7ba073137075",
    "citation": "Alejandrina Cristia, Marvin Lavechin, Camila Scaff, Melanie Soderstrom, Caroline Rowland, Okko Räsänen, John Bunce, and Elika Bergelson. A thorough evaluation of the language environment analysis (lena) system. Behavior Research Methods, 53:467-486, Jul 2020. URL: https://doi.org/10.3758/s13428-020-01393-5, doi:10.3758/s13428-020-01393-5.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "cristia2020athoroughevaluation",
    "bibtex": "@article{cristia2020athoroughevaluation,\n    author = \"Cristia, Alejandrina and Lavechin, Marvin and Scaff, Camila and Soderstrom, Melanie and Rowland, Caroline and Räsänen, Okko and Bunce, John and Bergelson, Elika\",\n    title = \"A thorough evaluation of the Language Environment Analysis (LENA) system\",\n    year = \"2020\",\n    journal = \"Behavior Research Methods\",\n    volume = \"53\",\n    pages = \"467-486\",\n    month = \"Jul\",\n    doi = \"10.3758/s13428-020-01393-5\",\n    url = \"https://doi.org/10.3758/s13428-020-01393-5\",\n    publisher = \"Springer Science and Business Media LLC\",\n    issue = \"2\",\n    issn = \"1554-3528\"\n}\n",
    "issn": "1554-3528",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.3758/s13428-020-01393-5",
    "doc_id": "aa7f7ba073137075",
    "formatted_citation": "Alejandrina Cristia, Marvin Lavechin, Camila Scaff, Melanie Soderstrom, Caroline Rowland, Okko Räsänen, John Bunce, and Elika Bergelson. A thorough evaluation of the language environment analysis (lena) system. Behavior Research Methods, 53:467-486, Jul 2020. URL: https://doi.org/10.3758/s13428-020-01393-5, doi:10.3758/s13428-020-01393-5. This article has 91 citations and is from a domain leading peer-reviewed journal."
  },
  "LPC Augment: An LPC-Based ASR Data Augmentation Algorithm for Low and Zero-Resource Children's Dialects": {
    "title": "LPC Augment: an LPC-based ASR Data Augmentation Algorithm for Low and Zero-Resource Children’s Dialects",
    "authors": [
      "Alexander Johnson",
      "Ruchao Fan",
      "Robin Morris",
      "Abeer Alwan"
    ],
    "doi": "10.1109/icassp43922.2022.9746281",
    "url": "https://doi.org/10.1109/icassp43922.2022.9746281",
    "journal": "ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2022,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2202.09529.pdf",
      "https://arxiv.org/abs/2202.09529"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "14J60 (Primary) 14F05, 14J26 (Secondary)",
      "F.2.2",
      "I.2.7"
    ],
    "pages": "8577-8581",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "This paper proposes a novel linear prediction coding-based data aug-mentation method for children's low and zero resource dialect ASR. The data augmentation procedure consists of perturbing the formant peaks of the LPC spectrum during LPC analysis and reconstruction. The method is evaluated on two novel children's speech datasets with one containing California English from the Southern CaliforniaArea and the other containing a mix of Southern American English and African American English from the Atlanta, Georgia area. We test the proposed method in training both an HMM-DNN system and an end-to-end system to show model-robustness and demonstrate that the algorithm improves ASR performance, especially for zero resource dialect children's task, as compared to common data augmentation methods such as VTLP, Speed Perturbation, and SpecAugment.",
    "publication_date": "2022-05-23T00:00:00",
    "citation_count": 12,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2202.09529",
    "docname": "johnson2022lpcaugmentan",
    "dockey": "c02106dda3811734",
    "citation": "Alexander Johnson, Ruchao Fan, Robin Morris, and Abeer Alwan. Lpc augment: an lpc-based asr data augmentation algorithm for low and zero-resource children’s dialects. ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8577-8581, May 2022. URL: https://doi.org/10.1109/icassp43922.2022.9746281, doi:10.1109/icassp43922.2022.9746281.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "johnson2022lpcaugmentan",
    "bibtex": "@article{johnson2022lpcaugmentan,\n    author = \"Johnson, Alexander and Fan, Ruchao and Morris, Robin and Alwan, Abeer\",\n    title = \"LPC Augment: an LPC-based ASR Data Augmentation Algorithm for Low and Zero-Resource Children’s Dialects\",\n    year = \"2022\",\n    journal = \"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"8577-8581\",\n    month = \"May\",\n    doi = \"10.1109/icassp43922.2022.9746281\",\n    url = \"https://doi.org/10.1109/icassp43922.2022.9746281\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp43922.2022.9746281",
    "doc_id": "c02106dda3811734",
    "formatted_citation": "Alexander Johnson, Ruchao Fan, Robin Morris, and Abeer Alwan. Lpc augment: an lpc-based asr data augmentation algorithm for low and zero-resource children’s dialects. ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8577-8581, May 2022. URL: https://doi.org/10.1109/icassp43922.2022.9746281, doi:10.1109/icassp43922.2022.9746281. This article has 12 citations."
  },
  "COLLECTING AND ANNOTATING NATURAL CHILD SPEECH DATA – CHALLENGES AND INTERDISCIPLINARY PERSPECTIVES": {
    "title": "Technological and methodological challenges in creating, annotating and sharing a learner corpus of spoken German",
    "authors": [
      "H. Hedeland",
      "Thomas C. Schmidt"
    ],
    "doi": "10.1075/hsm.14.04hed",
    "url": "https://doi.org/10.1075/hsm.14.04hed",
    "journal": "Hamburg Studies on Multilingualism",
    "year": 2012,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "25-46",
    "issue": null,
    "volume": "",
    "publisher": "John Benjamins Publishing Company",
    "abstract": "In this paper we share experiences on collecting and annotating child speech data from our speech language therapy background and the TALC-project (Tools for Analyzing Language and Communication) where we explore the application of machine learning models (focus ASR) for linguistic and speech therapy purposes in an interdisciplinary team. We will reﬂect on the importance of collecting natural speech data for ASR model training and will summarize recommended methods for eliciting such spontaneous child speech at different ages. For annotating recorded data such as transcribing them and marking relevant parts for subsequent analysis, we will focus on possible ways to ensure communication between different researchers. Throughout, we will elaborate on the interdisciplinary collaboration in our project in order to ensure that requirements of model developers and end-users are met.",
    "publication_date": "2012-10-22T00:00:00",
    "citation_count": 5,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "hedeland2012technologicalandmethodological",
    "dockey": "2819bdc445a34bdd",
    "citation": "H. Hedeland and Thomas C. Schmidt. Technological and methodological challenges in creating, annotating and sharing a learner corpus of spoken german. Hamburg Studies on Multilingualism, pages 25-46, Oct 2012. URL: https://doi.org/10.1075/hsm.14.04hed, doi:10.1075/hsm.14.04hed.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "hedeland2012technologicalandmethodological",
    "bibtex": "@article{hedeland2012technologicalandmethodological,\n    author = \"Hedeland, H. and Schmidt, Thomas C.\",\n    title = \"Technological and methodological challenges in creating, annotating and sharing a learner corpus of spoken German\",\n    year = \"2012\",\n    journal = \"Hamburg Studies on Multilingualism\",\n    pages = \"25-46\",\n    month = \"Oct\",\n    doi = \"10.1075/hsm.14.04hed\",\n    url = \"https://doi.org/10.1075/hsm.14.04hed\",\n    publisher = \"John Benjamins Publishing Company\",\n    issn = \"1571-4934\"\n}\n",
    "issn": "1571-4934",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1075/hsm.14.04hed",
    "doc_id": "2819bdc445a34bdd",
    "formatted_citation": "H. Hedeland and Thomas C. Schmidt. Technological and methodological challenges in creating, annotating and sharing a learner corpus of spoken german. Hamburg Studies on Multilingualism, pages 25-46, Oct 2012. URL: https://doi.org/10.1075/hsm.14.04hed, doi:10.1075/hsm.14.04hed. This article has 5 citations and is from a peer-reviewed journal."
  },
  "Advances in Low Resource ASR: A Deep Learning Perspective": {
    "title": "Advances in Low Resource ASR: A Deep Learning Perspective",
    "authors": [
      "Hardik B. Sailor",
      "Ankur T. Patil",
      "H. Patil"
    ],
    "doi": "10.21437/sltu.2018-4",
    "url": "https://www.isca-archive.org/sltu_2018/sailor18b_sltu.html",
    "journal": "6th Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU 2018)",
    "year": 2018,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.isca-archive.org/sltu_2018/sailor18b_sltu.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "15-19",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Recently, developing Automatic Speech Recognition (ASR) systems for Low Resource (LR) languages is an active research area. The research in ASR is signiﬁcantly advanced using deep learning approaches producing state-of-the-art results compared to the conventional approaches. However, it is still challenging to use such approaches for LR languages since it requires a huge amount of training data. Recently, data augmentation, multilingual and cross-lingual approaches, transfer learning, etc. enable training deep learning architectures. This paper presents an overview of deep learning-based approaches for building ASR for LR languages. Recent projects and events organized to support the development of ASR and related applications in this direction are also discussed. This paper could be a good motivation for the researchers interested to work towards low resource ASR using deep learning techniques. The approaches described here could be useful in other related applications, such as audio search.",
    "publication_date": "2018-08-29T00:00:00",
    "citation_count": 8,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "sailor2018advancesinlow",
    "dockey": "69c09b218392116c",
    "citation": "Hardik B. Sailor, Ankur T. Patil, and H. Patil. Advances in low resource asr: a deep learning perspective. 6th Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU 2018), pages 15-19, Aug 2018. URL: https://doi.org/10.21437/sltu.2018-4, doi:10.21437/sltu.2018-4.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "sailor2018advancesinlow",
    "bibtex": "@article{sailor2018advancesinlow,\n    author = \"Sailor, Hardik B. and Patil, Ankur T. and Patil, H.\",\n    title = \"Advances in Low Resource ASR: A Deep Learning Perspective\",\n    year = \"2018\",\n    journal = \"6th Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU 2018)\",\n    pages = \"15-19\",\n    month = \"Aug\",\n    doi = \"10.21437/sltu.2018-4\",\n    url = \"https://doi.org/10.21437/sltu.2018-4\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/sltu.2018-4",
    "doc_id": "69c09b218392116c",
    "formatted_citation": "Hardik B. Sailor, Ankur T. Patil, and H. Patil. Advances in low resource asr: a deep learning perspective. 6th Workshop on Spoken Language Technologies for Under-Resourced Languages (SLTU 2018), pages 15-19, Aug 2018. URL: https://doi.org/10.21437/sltu.2018-4, doi:10.21437/sltu.2018-4. This article has 8 citations."
  },
  "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data": {
    "title": "Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data",
    "authors": [
      "Manasa Prasad",
      "D. Esch",
      "Sandy Ritchie",
      "J. F. Mortensen"
    ],
    "doi": "10.21437/interspeech.2019-1775",
    "url": "https://www.isca-archive.org/interspeech_2019/prasad19_interspeech.html",
    "journal": "Interspeech 2019",
    "year": 2019,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.isca-archive.org/interspeech_2019/prasad19_interspeech.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "271-275",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "When building automatic speech recognition (ASR) systems, typically some amount of audio and text data in the target language is needed. While text data can be obtained relatively easily across many languages, transcribed audio data is challenging to obtain. This presents a barrier to making voice technologies available in more languages of the world. In this paper, we present a way to build an ASR system system for a language even in the absence of any audio training data in that language at all. We do this by simply re-using an existing acoustic model from a phonologically similar language, without any kind of modiﬁcation or adaptation towards the target language. The basic insight is that, if two languages are sufﬁciently similar in terms of their phonological system, an acoustic model should hold up relatively well when used for another language. We describe how we tailor our pronunciation models to enable such re-use, and show experimental results across a number of languages from various language families. We also provide a theoretical analysis of situations in which this approach is likely to work. Our results show that it is possible to achieve less than 20% word error rate (WER) using this method.",
    "publication_date": "2019-09-13T00:00:00",
    "citation_count": 14,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "prasad2019buildinglargevocabularyasr",
    "dockey": "ceb186780a72183e",
    "citation": "Manasa Prasad, D. Esch, Sandy Ritchie, and J. F. Mortensen. Building large-vocabulary asr systems for languages without any audio training data. Interspeech 2019, pages 271-275, Sep 2019. URL: https://doi.org/10.21437/interspeech.2019-1775, doi:10.21437/interspeech.2019-1775.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "prasad2019buildinglargevocabularyasr",
    "bibtex": "@article{prasad2019buildinglargevocabularyasr,\n    author = \"Prasad, Manasa and Esch, D. and Ritchie, Sandy and Mortensen, J. F.\",\n    title = \"Building Large-Vocabulary ASR Systems for Languages Without Any Audio Training Data\",\n    year = \"2019\",\n    journal = \"Interspeech 2019\",\n    pages = \"271-275\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2019-1775\",\n    url = \"https://doi.org/10.21437/interspeech.2019-1775\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2019-1775",
    "doc_id": "ceb186780a72183e",
    "formatted_citation": "Manasa Prasad, D. Esch, Sandy Ritchie, and J. F. Mortensen. Building large-vocabulary asr systems for languages without any audio training data. Interspeech 2019, pages 271-275, Sep 2019. URL: https://doi.org/10.21437/interspeech.2019-1775, doi:10.21437/interspeech.2019-1775. This article has 14 citations."
  },
  "TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline": {
    "title": "TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline",
    "authors": [
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None"
    ],
    "doi": "10.21437/interspeech.2022-877",
    "url": "https://www.isca-archive.org/interspeech_2022/li22j_interspeech.html",
    "journal": "Interspeech 2022",
    "year": 2022,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://arxiv.org/pdf/2206.13135"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1741-1745",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "This paper introduces a new corpus of Mandarin-English code-switching speech recognition—TALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license1. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable.",
    "publication_date": "2022-09-16T00:00:00",
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "none2022talcsanopensource",
    "dockey": "841659dcb72aa668",
    "citation": "None None, None None, None None, None None, None None, None None, and None None. Talcs: an open-source mandarin-english code-switching corpus and a speech recognition baseline. Interspeech 2022, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-877, doi:10.21437/interspeech.2022-877.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "none2022talcsanopensource",
    "bibtex": "@article{none2022talcsanopensource,\n    author = \"None, None and None, None and None, None and None, None and None, None and None, None and None, None\",\n    title = \"TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline\",\n    year = \"2022\",\n    journal = \"Interspeech 2022\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2022-877\",\n    url = \"https://doi.org/10.21437/interspeech.2022-877\",\n    publisher = \"ISCA\"\n}\n",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2022-877",
    "doc_id": "841659dcb72aa668",
    "formatted_citation": "None None, None None, None None, None None, None None, None None, and None None. Talcs: an open-source mandarin-english code-switching corpus and a speech recognition baseline. Interspeech 2022, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-877, doi:10.21437/interspeech.2022-877."
  },
  "The PF_STAR children's speech corpus": {
    "title": "The PF_STAR children's speech corpus",
    "authors": [
      "A. Batliner",
      "M. Blomberg",
      "S. D'Arcy",
      "Daniel Elenius",
      "D. Giuliani",
      "M. Gerosa",
      "C. Hacker",
      "M. Russell",
      "S. Steidl",
      "Michael Wong"
    ],
    "doi": "10.21437/interspeech.2005-705",
    "url": "https://www.isca-archive.org/interspeech_2005/batliner05b_interspeech.html",
    "journal": "Interspeech 2005",
    "year": 2005,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.isca-archive.org/interspeech_2005/batliner05b_interspeech.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2761-2764",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "This paper describes the corpus of recordings of children’s speech which was collected as part of the EU FP5 PF STAR project. The corpus contains more than 60 hours of speech, including read and imitated native-language speech in British English, German and Swedish, read and imitated non-nativelanguage English speech from German, Italian and Swedish children, and native-language spontaneous and emotional speech in English and German.",
    "publication_date": null,
    "citation_count": 151,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null,
    "docname": "batliner2005thepfstarchildrens",
    "dockey": "51ce75238824bf0e",
    "citation": "A. Batliner, M. Blomberg, S. D'Arcy, Daniel Elenius, D. Giuliani, M. Gerosa, C. Hacker, M. Russell, S. Steidl, and Michael Wong. The pf_star children's speech corpus. Unknown journal, pages 2761-2764, 2005. URL: https://doi.org/10.21437/interspeech.2005-705, doi:10.21437/interspeech.2005-705.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "batliner2005thepfstarchildrens",
    "bibtex": "@article{batliner2005thepfstarchildrens,\n    author = \"Batliner, A. and Blomberg, M. and D'Arcy, S. and Elenius, Daniel and Giuliani, D. and Gerosa, M. and Hacker, C. and Russell, M. and Steidl, S. and Wong, Michael\",\n    title = \"The PF\\_STAR children's speech corpus\",\n    year = \"2005\",\n    journal = \"Unknown journal\",\n    pages = \"2761-2764\",\n    doi = \"10.21437/interspeech.2005-705\",\n    url = \"https://doi.org/10.21437/interspeech.2005-705\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2005-705",
    "doc_id": "51ce75238824bf0e",
    "formatted_citation": "A. Batliner, M. Blomberg, S. D'Arcy, Daniel Elenius, D. Giuliani, M. Gerosa, C. Hacker, M. Russell, S. Steidl, and Michael Wong. The pf_star children's speech corpus. Unknown journal, pages 2761-2764, 2005. URL: https://doi.org/10.21437/interspeech.2005-705, doi:10.21437/interspeech.2005-705."
  },
  "childes-db: a flexible and reproducible interface to the Child Language Data Exchange System": {
    "title": "childes-db: a flexible and reproducible interface to the Child Language Data Exchange System",
    "authors": [
      "Alessandro Sanchez",
      "Stephan Meylan",
      "Mika Braginsky",
      "Kyle Earl MacDonald",
      "Daniel Yurovsky",
      "Michael C. Frank"
    ],
    "doi": "10.31234/osf.io/93mwx",
    "url": "https://doi.org/10.31234/osf.io/93mwx",
    "journal": null,
    "year": 2018,
    "genre": "preprint",
    "link_attachments": [
      "https://psyarxiv.com/93mwx/download"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "Center for Open Science",
    "abstract": "The Child Language Data Exchange System (CHILDES) has played a critical role in research on child language development, particularly in characterizing the early language learning environment. Access to these data can be both complex for novices and difficult to automate for advanced users, however. To address these issues, we introduce childes-db, a database-formatted mirror of CHILDES that improves data accessibility and usability by offering novel interfaces, including browsable web applications and an R application programming interface (API). Along with versioned infrastructure that facilitates reproducibility of past analyses, these interfaces lower barriers to analyzing naturalistic parent-child language, allowing for a wider range of researchers in language and cognitive development to easily leverage CHILDES in their work.",
    "publication_date": "2018-04-23T00:00:00",
    "citation_count": 5,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null,
    "docname": "sanchez2018childesdbaflexible",
    "dockey": "11fa5b2ff1c3d7fc",
    "citation": "Alessandro Sanchez, Stephan Meylan, Mika Braginsky, Kyle Earl MacDonald, Daniel Yurovsky, and Michael C. Frank. Childes-db: a flexible and reproducible interface to the child language data exchange system. Unknown journal, Apr 2018. URL: https://doi.org/10.31234/osf.io/93mwx, doi:10.31234/osf.io/93mwx.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "sanchez2018childesdbaflexible",
    "bibtex": "@article{sanchez2018childesdbaflexible,\n    author = \"Sanchez, Alessandro and Meylan, Stephan and Braginsky, Mika and MacDonald, Kyle Earl and Yurovsky, Daniel and Frank, Michael C.\",\n    title = \"childes-db: a flexible and reproducible interface to the Child Language Data Exchange System\",\n    year = \"2018\",\n    journal = \"Unknown journal\",\n    month = \"Apr\",\n    doi = \"10.31234/osf.io/93mwx\",\n    url = \"https://doi.org/10.31234/osf.io/93mwx\",\n    publisher = \"Center for Open Science\"\n}\n",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.31234/osf.io/93mwx",
    "doc_id": "11fa5b2ff1c3d7fc",
    "formatted_citation": "Alessandro Sanchez, Stephan Meylan, Mika Braginsky, Kyle Earl MacDonald, Daniel Yurovsky, and Michael C. Frank. Childes-db: a flexible and reproducible interface to the child language data exchange system. Unknown journal, Apr 2018. URL: https://doi.org/10.31234/osf.io/93mwx, doi:10.31234/osf.io/93mwx."
  },
  "Automatic Multilingual Speech Recognition": {
    "title": "Efficient Weight Factorization for Multilingual Speech Recognition",
    "authors": [
      "Ngoc-Quan Pham",
      "Tuan-Nam Nguyen",
      "Sebastian Stüker",
      "Alex Waibel"
    ],
    "doi": "10.21437/interspeech.2021-216",
    "url": "https://doi.org/10.21437/interspeech.2021-216",
    "journal": "Interspeech 2021",
    "year": 2021,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.ijeas.org/download_data/IJEAS0705013.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2421-2425",
    "issue": "5",
    "volume": "abs/2105.03010",
    "publisher": "ISCA",
    "abstract": "Automatic Speech Recognition (ASR) for multi-languages is currently attracting more and more attention; however, development is still hampered by the need for language experts. End-toEnd ASR simplifies their work by directly predicting the output character based on the acoustic input. This study presents the improvement of LIS-Net model for End-to-End Vietnamese and Chinese ASR system. In this study, an efficient yet accurate end-to-end multilingual multi-speaker ASR model has developed, allowing direct conversion of raw speech audio signals into text of multiple languages. This study proposes a new method of coding labels specifically for multiple languages by pagination labels by language. The results of this study are significantly improved compared to that of baseline models.",
    "publication_date": "2021-08-30T00:00:00",
    "citation_count": 19,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "pham2021efficientweightfactorization",
    "dockey": "479e10ddd5c072cd",
    "citation": "Ngoc-Quan Pham, Tuan-Nam Nguyen, Sebastian Stüker, and Alex Waibel. Efficient weight factorization for multilingual speech recognition. Interspeech 2021, abs/2105.03010:2421-2425, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-216, doi:10.21437/interspeech.2021-216.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "pham2021efficientweightfactorization",
    "bibtex": "@article{pham2021efficientweightfactorization,\n    author = \"Pham, Ngoc-Quan and Nguyen, Tuan-Nam and Stüker, Sebastian and Waibel, Alex\",\n    title = \"Efficient Weight Factorization for Multilingual Speech Recognition\",\n    year = \"2021\",\n    journal = \"Interspeech 2021\",\n    volume = \"abs/2105.03010\",\n    pages = \"2421-2425\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2021-216\",\n    url = \"https://doi.org/10.21437/interspeech.2021-216\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2021-216",
    "doc_id": "479e10ddd5c072cd",
    "formatted_citation": "Ngoc-Quan Pham, Tuan-Nam Nguyen, Sebastian Stüker, and Alex Waibel. Efficient weight factorization for multilingual speech recognition. Interspeech 2021, abs/2105.03010:2421-2425, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-216, doi:10.21437/interspeech.2021-216. This article has 19 citations."
  },
  "Cross-lingual and Multilingual Automatic Speech Recognition for Scandinavian Languages": {
    "title": "A speech recognition enabled ambulance call report system for paramedics",
    "authors": [
      "Rafal Zydowicz"
    ],
    "doi": "10.22215/etd/2011-09074",
    "url": "https://doi.org/10.22215/etd/2011-09074",
    "journal": null,
    "year": null,
    "genre": "journalArticle",
    "link_attachments": [
      "https://uu.diva-portal.org/smash/get/diva2:1675877/FULLTEXT01.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "Carleton University",
    "abstract": null,
    "publication_date": null,
    "citation_count": 0,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null,
    "docname": "zydowiczUnknownyearaspeechrecognition",
    "dockey": "bea44d4134cabd8b",
    "citation": "Rafal Zydowicz. A speech recognition enabled ambulance call report system for paramedics. Unknown journal, Unknown year. URL: https://doi.org/10.22215/etd/2011-09074, doi:10.22215/etd/2011-09074.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "zydowiczUnknownyearaspeechrecognition",
    "bibtex": "@article{zydowiczUnknownyearaspeechrecognition,\n    author = \"Zydowicz, Rafal\",\n    title = \"A speech recognition enabled ambulance call report system for paramedics\",\n    year = \"Unknown year\",\n    journal = \"Unknown journal\",\n    doi = \"10.22215/etd/2011-09074\",\n    url = \"https://doi.org/10.22215/etd/2011-09074\",\n    publisher = \"Carleton University\"\n}\n",
    "bibtex_type": "phdthesis",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.22215/etd/2011-09074",
    "doc_id": "bea44d4134cabd8b",
    "formatted_citation": "Rafal Zydowicz. A speech recognition enabled ambulance call report system for paramedics. Unknown journal, Unknown year. URL: https://doi.org/10.22215/etd/2011-09074, doi:10.22215/etd/2011-09074."
  },
  "How Multilingual is Multilingual BERT?": {
    "title": "How Multilingual is Multilingual BERT?",
    "authors": [
      "Telmo Pires",
      "Eva Schlinger",
      "Dan Garrette"
    ],
    "doi": "10.48550/arxiv.1906.01502",
    "url": "https://aclanthology.org/P19-1493",
    "journal": "ArXiv",
    "year": 2019,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://aclanthology.org/P19-1493.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "4996–5001",
    "issue": null,
    "volume": "abs/1906.01502",
    "publisher": "Association for Computational Linguistics",
    "abstract": "In this paper, we show that Multilingual BERT (M-BERT), released by Devlin et al. (2018) as a single language model pre-trained from monolingual corpora in 104 languages, is surprisingly good at zero-shot cross-lingual model transfer, in which task-specific annotations in one language are used to fine-tune the model for evaluation in another language. To understand why, we present a large number of probing experiments, showing that transfer is possible even to languages in different scripts, that transfer works best between typologically similar languages, that monolingual corpora can train models for code-switching, and that the model can find translation pairs. From these results, we can conclude that M-BERT does create multilingual representations, but that these representations exhibit systematic deficiencies affecting certain language pairs.",
    "publication_date": null,
    "citation_count": 1394,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "pires2019howmultilingualis",
    "dockey": "1b345f22117a2102",
    "citation": "Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? ArXiv, 2019. URL: https://doi.org/10.48550/arxiv.1906.01502, doi:10.48550/arxiv.1906.01502.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "pires2019howmultilingualis",
    "bibtex": "@article{pires2019howmultilingualis,\n    author = \"Pires, Telmo and Schlinger, Eva and Garrette, Dan\",\n    title = \"How Multilingual is Multilingual BERT?\",\n    year = \"2019\",\n    journal = \"ArXiv\",\n    volume = \"abs/1906.01502\",\n    doi = \"10.48550/arxiv.1906.01502\",\n    url = \"https://doi.org/10.48550/arxiv.1906.01502\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.1906.01502",
    "doc_id": "1b345f22117a2102",
    "formatted_citation": "Telmo Pires, Eva Schlinger, and Dan Garrette. How multilingual is multilingual bert? ArXiv, 2019. URL: https://doi.org/10.48550/arxiv.1906.01502, doi:10.48550/arxiv.1906.01502. This article has 1394 citations."
  },
  "Book Reviews: Statistical Methods for Speech Recognition": {
    "title": "Book Reviews",
    "authors": [
      "Eric Laws"
    ],
    "doi": "10.1080/02642069100000021",
    "url": "https://doi.org/10.1080/02642069100000021",
    "journal": "Service Industries Journal",
    "year": 1991,
    "genre": "journalArticle",
    "link_attachments": [
      "https://aclanthology.org/J99-2009.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "125-126",
    "issue": "1",
    "volume": "11",
    "publisher": "Informa UK Limited",
    "abstract": null,
    "publication_date": "1991-01-01T00:00:00",
    "citation_count": 0,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "laws1991bookreviews",
    "dockey": "bd3e14c1909f467e",
    "citation": "Eric Laws. Book reviews. Service Industries Journal, 11:125-126, Jan 1991. URL: https://doi.org/10.1080/02642069100000021, doi:10.1080/02642069100000021.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "laws1991bookreviews",
    "bibtex": "@article{laws1991bookreviews,\n    author = \"Laws, Eric\",\n    title = \"Book Reviews\",\n    year = \"1991\",\n    journal = \"Service Industries Journal\",\n    volume = \"11\",\n    pages = \"125-126\",\n    month = \"Jan\",\n    doi = \"10.1080/02642069100000021\",\n    url = \"https://doi.org/10.1080/02642069100000021\",\n    publisher = \"Informa UK Limited\",\n    issue = \"1\",\n    issn = \"0264-2069\"\n}\n",
    "issn": "0264-2069",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1080/02642069100000021",
    "doc_id": "bd3e14c1909f467e",
    "formatted_citation": "Eric Laws. Book reviews. Service Industries Journal, 11:125-126, Jan 1991. URL: https://doi.org/10.1080/02642069100000021, doi:10.1080/02642069100000021. This article has 0 citations and is from a peer-reviewed journal."
  },
  "Unsupervised Cross-Lingual Representation Learning for Speech Recognition": {
    "title": "Unsupervised Cross-Lingual Representation Learning for Speech Recognition",
    "authors": [
      "Alexis Conneau",
      "Alexei Baevski",
      "R. Collobert",
      "Abdel-rahman Mohamed",
      "Michael Auli"
    ],
    "doi": "10.21437/interspeech.2021-329",
    "url": "https://www.isca-archive.org/interspeech_2021/conneau21_interspeech.html",
    "journal": "Interspeech 2021",
    "year": 2021,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://arxiv.org/pdf/2006.13979"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2426-2430",
    "issue": null,
    "volume": "abs/2006.13979",
    "publisher": "ISCA",
    "abstract": null,
    "publication_date": "2021-08-27T00:00:00",
    "citation_count": 772,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "conneau2021unsupervisedcrosslingualrepresentation",
    "dockey": "6829d1c927bb65d6",
    "citation": "Alexis Conneau, Alexei Baevski, R. Collobert, Abdel-rahman Mohamed, and Michael Auli. Unsupervised cross-lingual representation learning for speech recognition. Interspeech 2021, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-329, doi:10.21437/interspeech.2021-329.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "conneau2021unsupervisedcrosslingualrepresentation",
    "bibtex": "@article{conneau2021unsupervisedcrosslingualrepresentation,\n    author = \"Conneau, Alexis and Baevski, Alexei and Collobert, R. and Mohamed, Abdel-rahman and Auli, Michael\",\n    title = \"Unsupervised Cross-Lingual Representation Learning for Speech Recognition\",\n    year = \"2021\",\n    journal = \"Interspeech 2021\",\n    volume = \"abs/2006.13979\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2021-329\",\n    url = \"https://doi.org/10.21437/interspeech.2021-329\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2021-329",
    "doc_id": "6829d1c927bb65d6",
    "formatted_citation": "Alexis Conneau, Alexei Baevski, R. Collobert, Abdel-rahman Mohamed, and Michael Auli. Unsupervised cross-lingual representation learning for speech recognition. Interspeech 2021, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-329, doi:10.21437/interspeech.2021-329. This article has 772 citations."
  },
  "Language-agnostic multilingual modeling": {
    "title": "Language-Agnostic Multilingual Modeling",
    "authors": [
      "Arindrima Datta",
      "Bhuvana Ramabhadran",
      "Jesse Emond",
      "Anjuli Kannan",
      "Brian Roark"
    ],
    "doi": "10.1109/icassp40776.2020.9053443",
    "url": "https://doi.org/10.1109/icassp40776.2020.9053443",
    "journal": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2020,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://arxiv.org/pdf/2004.09571"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "8239-8243",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2020-05-01T00:00:00",
    "citation_count": 35,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "datta2020languageagnosticmultilingualmodeling",
    "dockey": "6a151204da3e0d46",
    "citation": "Arindrima Datta, Bhuvana Ramabhadran, Jesse Emond, Anjuli Kannan, and Brian Roark. Language-agnostic multilingual modeling. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8239-8243, May 2020. URL: https://doi.org/10.1109/icassp40776.2020.9053443, doi:10.1109/icassp40776.2020.9053443.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "datta2020languageagnosticmultilingualmodeling",
    "bibtex": "@article{datta2020languageagnosticmultilingualmodeling,\n    author = \"Datta, Arindrima and Ramabhadran, Bhuvana and Emond, Jesse and Kannan, Anjuli and Roark, Brian\",\n    title = \"Language-Agnostic Multilingual Modeling\",\n    year = \"2020\",\n    journal = \"ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"8239-8243\",\n    month = \"May\",\n    doi = \"10.1109/icassp40776.2020.9053443\",\n    url = \"https://doi.org/10.1109/icassp40776.2020.9053443\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp40776.2020.9053443",
    "doc_id": "6a151204da3e0d46",
    "formatted_citation": "Arindrima Datta, Bhuvana Ramabhadran, Jesse Emond, Anjuli Kannan, and Brian Roark. Language-agnostic multilingual modeling. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8239-8243, May 2020. URL: https://doi.org/10.1109/icassp40776.2020.9053443, doi:10.1109/icassp40776.2020.9053443. This article has 35 citations."
  },
  "Comprehensive literature review on children automatic speech recognition system, acoustic linguistic mismatch approaches and challenges": {
    "title": "Comprehensive literature review on children automatic speech recognition system, acoustic linguistic mismatch approaches and challenges",
    "authors": [
      "Rajni Sobti",
      "Kalpna Guleria",
      "Virender Kadyan"
    ],
    "doi": "10.1007/s11042-024-18753-4",
    "url": "https://doi.org/10.1007/s11042-024-18753-4",
    "journal": "Multimedia Tools and Applications",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [
      "https://link.springer.com/content/pdf/10.1007%2Fs11042-024-18753-4.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Acoustic and Linguistic Variations",
      "Applications of Child ASRs",
      "Automatic Speech Recognition",
      "Data Augmentation",
      "Low Resource Language",
      "Mismatch ASR"
    ],
    "pages": "81933-81995",
    "issue": "35",
    "volume": "83",
    "publisher": "Springer Science and Business Media LLC",
    "abstract": null,
    "publication_date": "2024-03-11T00:00:00",
    "citation_count": 4,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "sobti2024comprehensiveliteraturereview",
    "dockey": "d71650fd2cbc5148",
    "citation": "Rajni Sobti, Kalpna Guleria, and Virender Kadyan. Comprehensive literature review on children automatic speech recognition system, acoustic linguistic mismatch approaches and challenges. Multimedia Tools and Applications, 83:81933-81995, Mar 2024. URL: https://doi.org/10.1007/s11042-024-18753-4, doi:10.1007/s11042-024-18753-4.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "sobti2024comprehensiveliteraturereview",
    "bibtex": "@article{sobti2024comprehensiveliteraturereview,\n    author = \"Sobti, Rajni and Guleria, Kalpna and Kadyan, Virender\",\n    title = \"Comprehensive literature review on children automatic speech recognition system, acoustic linguistic mismatch approaches and challenges\",\n    year = \"2024\",\n    journal = \"Multimedia Tools and Applications\",\n    volume = \"83\",\n    pages = \"81933-81995\",\n    month = \"Mar\",\n    doi = \"10.1007/s11042-024-18753-4\",\n    url = \"https://doi.org/10.1007/s11042-024-18753-4\",\n    publisher = \"Springer Science and Business Media LLC\",\n    issue = \"35\",\n    issn = \"1573-7721\"\n}\n",
    "issn": "1573-7721",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1007/s11042-024-18753-4",
    "doc_id": "d71650fd2cbc5148",
    "formatted_citation": "Rajni Sobti, Kalpna Guleria, and Virender Kadyan. Comprehensive literature review on children automatic speech recognition system, acoustic linguistic mismatch approaches and challenges. Multimedia Tools and Applications, 83:81933-81995, Mar 2024. URL: https://doi.org/10.1007/s11042-024-18753-4, doi:10.1007/s11042-024-18753-4. This article has 4 citations and is from a peer-reviewed journal."
  },
  "Challenges for Designing of Children Speech Corpora: A State-of-the-Art Review": {
    "title": "Challenges for Designing of Children Speech Corpora: A State-of-the-Art Review",
    "authors": [
      "Rajni Sobti",
      "Virender Kadyan",
      "Kalpna Guleria"
    ],
    "doi": "10.1149/10701.9053ecst",
    "url": "https://doi.org/10.1149/10701.9053ecst",
    "journal": "ECS Transactions",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [
      "https://iopscience.iop.org/article/10.1149/10701.9053ecst/pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "9053-9064",
    "issue": "1",
    "volume": "107",
    "publisher": "The Electrochemical Society",
    "abstract": "Challenges for Designing of Children Speech Corpora: A State-of-the-Art Review, Rajni Sobti, Virender Kadyan, Kalpna Guleria",
    "publication_date": "2022-04-24T00:00:00",
    "citation_count": 3,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "sobti2022challengesfordesigning",
    "dockey": "0c8bd1f62e8269c0",
    "citation": "Rajni Sobti, Virender Kadyan, and Kalpna Guleria. Challenges for designing of children speech corpora: a state-of-the-art review. ECS Transactions, 107:9053-9064, Apr 2022. URL: https://doi.org/10.1149/10701.9053ecst, doi:10.1149/10701.9053ecst.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "sobti2022challengesfordesigning",
    "bibtex": "@article{sobti2022challengesfordesigning,\n    author = \"Sobti, Rajni and Kadyan, Virender and Guleria, Kalpna\",\n    title = \"Challenges for Designing of Children Speech Corpora: A State-of-the-Art Review\",\n    year = \"2022\",\n    journal = \"ECS Transactions\",\n    volume = \"107\",\n    pages = \"9053-9064\",\n    month = \"Apr\",\n    doi = \"10.1149/10701.9053ecst\",\n    url = \"https://doi.org/10.1149/10701.9053ecst\",\n    publisher = \"The Electrochemical Society\",\n    issue = \"1\",\n    issn = \"1938-5862\"\n}\n",
    "issn": "1938-5862",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1149/10701.9053ecst",
    "doc_id": "0c8bd1f62e8269c0",
    "formatted_citation": "Rajni Sobti, Virender Kadyan, and Kalpna Guleria. Challenges for designing of children speech corpora: a state-of-the-art review. ECS Transactions, 107:9053-9064, Apr 2022. URL: https://doi.org/10.1149/10701.9053ecst, doi:10.1149/10701.9053ecst. This article has 3 citations and is from a peer-reviewed journal."
  },
  "A systematic review on language identification of code-mixed text: techniques, data availability, challenges, and framework development": {
    "title": "A Systematic Review on Language Identification of Code-Mixed Text: Techniques, Data Availability, Challenges, and Framework Development",
    "authors": [
      "Ahmad Fathan Hidayatullah",
      "Atika Qazi",
      "Daphne Teck Ching Lai",
      "Rosyzie Anna Apong"
    ],
    "doi": "10.1109/access.2022.3223703",
    "url": "https://doi.org/10.1109/access.2022.3223703",
    "journal": "IEEE Access",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "122812-122831",
    "issue": null,
    "volume": "10",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "abstract": null,
    "publication_date": "2022-01-01T00:00:00",
    "citation_count": 19,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "hidayatullah2022asystematicreview",
    "dockey": "ca282f2d6262603b",
    "citation": "Ahmad Fathan Hidayatullah, Atika Qazi, Daphne Teck Ching Lai, and Rosyzie Anna Apong. A systematic review on language identification of code-mixed text: techniques, data availability, challenges, and framework development. IEEE Access, 10:122812-122831, Jan 2022. URL: https://doi.org/10.1109/access.2022.3223703, doi:10.1109/access.2022.3223703.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "hidayatullah2022asystematicreview",
    "bibtex": "@article{hidayatullah2022asystematicreview,\n    author = \"Hidayatullah, Ahmad Fathan and Qazi, Atika and Lai, Daphne Teck Ching and Apong, Rosyzie Anna\",\n    title = \"A Systematic Review on Language Identification of Code-Mixed Text: Techniques, Data Availability, Challenges, and Framework Development\",\n    year = \"2022\",\n    journal = \"IEEE Access\",\n    volume = \"10\",\n    pages = \"122812-122831\",\n    month = \"Jan\",\n    doi = \"10.1109/access.2022.3223703\",\n    url = \"https://doi.org/10.1109/access.2022.3223703\",\n    publisher = \"Institute of Electrical and Electronics Engineers (IEEE)\",\n    issn = \"2169-3536\"\n}\n",
    "issn": "2169-3536",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/access.2022.3223703",
    "doc_id": "ca282f2d6262603b",
    "formatted_citation": "Ahmad Fathan Hidayatullah, Atika Qazi, Daphne Teck Ching Lai, and Rosyzie Anna Apong. A systematic review on language identification of code-mixed text: techniques, data availability, challenges, and framework development. IEEE Access, 10:122812-122831, Jan 2022. URL: https://doi.org/10.1109/access.2022.3223703, doi:10.1109/access.2022.3223703. This article has 19 citations and is from a peer-reviewed journal."
  },
  "A Survey of Code-switched Speech and Language Processing": {
    "title": "Word Embeddings for Code-Mixed Language Processing",
    "authors": [
      "Adithya Pratapa",
      "Monojit Choudhury",
      "Sunayana Sitaram"
    ],
    "doi": "10.18653/v1/d18-1344",
    "url": "https://doi.org/10.18653/v1/d18-1344",
    "journal": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    "year": 2018,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/1904.00784.pdf",
      "https://arxiv.org/abs/1904.00784"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Computation and Language",
      "Computer Science - Machine Learning",
      "Statistics - Machine Learning"
    ],
    "pages": "3067-3072",
    "issue": null,
    "volume": null,
    "publisher": "Association for Computational Linguistics",
    "abstract": "Code-switching, the alternation of languages within a conversation or utterance, is a common communicative phenomenon that occurs in multilingual communities across the world. This survey reviews computational approaches for code-switched Speech and Natural Language Processing. We motivate why processing code-switched text and speech is essential for building intelligent agents and systems that interact with users in multilingual communities. As code-switching data and resources are scarce, we list what is available in various code-switched language pairs with the language processing tasks they can be used for. We review code-switching research in various Speech and NLP applications, including language processing tools and end-to-end systems. We conclude with future directions and open problems in the field.",
    "publication_date": "2018-01-01T00:00:00",
    "citation_count": 58,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "1904.00784",
    "docname": "pratapa2018wordembeddingsfor",
    "dockey": "5c64ad35019c074e",
    "citation": "Adithya Pratapa, Monojit Choudhury, and Sunayana Sitaram. Word embeddings for code-mixed language processing. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3067-3072, Jan 2018. URL: https://doi.org/10.18653/v1/d18-1344, doi:10.18653/v1/d18-1344.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "pratapa2018wordembeddingsfor",
    "bibtex": "@article{pratapa2018wordembeddingsfor,\n    author = \"Pratapa, Adithya and Choudhury, Monojit and Sitaram, Sunayana\",\n    title = \"Word Embeddings for Code-Mixed Language Processing\",\n    year = \"2018\",\n    journal = \"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\",\n    pages = \"3067-3072\",\n    month = \"Jan\",\n    doi = \"10.18653/v1/d18-1344\",\n    url = \"https://doi.org/10.18653/v1/d18-1344\",\n    publisher = \"Association for Computational Linguistics\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.18653/v1/d18-1344",
    "doc_id": "5c64ad35019c074e",
    "formatted_citation": "Adithya Pratapa, Monojit Choudhury, and Sunayana Sitaram. Word embeddings for code-mixed language processing. Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3067-3072, Jan 2018. URL: https://doi.org/10.18653/v1/d18-1344, doi:10.18653/v1/d18-1344. This article has 58 citations."
  },
  "A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss": {
    "title": "A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss",
    "authors": [
      "Kathryn Crowe",
      "Sharynne McLeod"
    ],
    "doi": "10.1080/13670050.2012.758686",
    "url": "https://doi.org/10.1080/13670050.2012.758686",
    "journal": "International Journal of Bilingual Education and Bilingualism",
    "year": 2014,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "287-309",
    "issue": "3",
    "volume": "17",
    "publisher": "Informa UK Limited",
    "abstract": null,
    "publication_date": "2013-02-08T00:00:00",
    "citation_count": 20,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 2,
    "arxiv_id": null,
    "docname": "crowe2014asystematicreview",
    "dockey": "922f5ab34a964a59",
    "citation": "Kathryn Crowe and Sharynne McLeod. A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss. International Journal of Bilingual Education and Bilingualism, 17:287-309, Feb 2014. URL: https://doi.org/10.1080/13670050.2012.758686, doi:10.1080/13670050.2012.758686.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "crowe2014asystematicreview",
    "bibtex": "@article{crowe2014asystematicreview,\n    author = \"Crowe, Kathryn and McLeod, Sharynne\",\n    title = \"A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss\",\n    year = \"2014\",\n    journal = \"International Journal of Bilingual Education and Bilingualism\",\n    volume = \"17\",\n    pages = \"287-309\",\n    month = \"Feb\",\n    doi = \"10.1080/13670050.2012.758686\",\n    url = \"https://doi.org/10.1080/13670050.2012.758686\",\n    publisher = \"Informa UK Limited\",\n    issue = \"3\",\n    issn = \"1367-0050\"\n}\n",
    "issn": "1367-0050",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1080/13670050.2012.758686",
    "doc_id": "922f5ab34a964a59",
    "formatted_citation": "Kathryn Crowe and Sharynne McLeod. A systematic review of cross-linguistic and multilingual speech and language outcomes for children with hearing loss. International Journal of Bilingual Education and Bilingualism, 17:287-309, Feb 2014. URL: https://doi.org/10.1080/13670050.2012.758686, doi:10.1080/13670050.2012.758686. This article has 20 citations and is from a domain leading peer-reviewed journal."
  },
  "Bilingualism/Multilingualism and Second‐Language Acquisition": {
    "title": "Bilingualism/Multilingualism and Second‐Language Acquisition",
    "authors": [
      "Yuko Goto Butler"
    ],
    "doi": "10.1002/9781118332382.ch5",
    "url": "https://doi.org/10.1002/9781118332382.ch5",
    "journal": "The Handbook of Bilingualism and Multilingualism",
    "year": 2012,
    "genre": "bookSection",
    "link_attachments": [
      "https://www.researchgate.net/profile/Yuko-Butler/publication/287446419_BilingualismMultilingualism_and_Second-Language_Acquisition/links/5ad4e2feaca272fdaf7c02f0/Bilingualism-Multilingualism-and-Second-Language-Acquisition.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "109-136",
    "issue": null,
    "volume": "",
    "publisher": "Wiley",
    "abstract": null,
    "publication_date": "2012-10-03T00:00:00",
    "citation_count": 62,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "butler2012bilingualismmultilingualismandsecond‐language",
    "dockey": "a53d53b5510d7971",
    "citation": "Yuko Goto Butler. Bilingualism/multilingualism and second‐language acquisition. The Handbook of Bilingualism and Multilingualism, pages 109-136, Oct 2012. URL: https://doi.org/10.1002/9781118332382.ch5, doi:10.1002/9781118332382.ch5.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "butler2012bilingualismmultilingualismandsecond‐language",
    "bibtex": "@article{butler2012bilingualismmultilingualismandsecond‐language,\n    author = \"Butler, Yuko Goto\",\n    title = \"Bilingualism/Multilingualism and Second‐Language Acquisition\",\n    year = \"2012\",\n    journal = \"The Handbook of Bilingualism and Multilingualism\",\n    pages = \"109-136\",\n    month = \"Oct\",\n    doi = \"10.1002/9781118332382.ch5\",\n    url = \"https://doi.org/10.1002/9781118332382.ch5\",\n    publisher = \"Wiley\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1002/9781118332382.ch5",
    "doc_id": "a53d53b5510d7971",
    "formatted_citation": "Yuko Goto Butler. Bilingualism/multilingualism and second‐language acquisition. The Handbook of Bilingualism and Multilingualism, pages 109-136, Oct 2012. URL: https://doi.org/10.1002/9781118332382.ch5, doi:10.1002/9781118332382.ch5. This article has 62 citations."
  },
  "Bilingual Language Acquisition": {
    "title": "Bilingual Language Acquisition",
    "authors": [
      "Annick De Houwer"
    ],
    "doi": "10.1111/b.9780631203124.1996.00009.x",
    "url": "https://doi.org/10.1111/b.9780631203124.1996.00009.x",
    "journal": "The Handbook of Child Language",
    "year": 2019,
    "genre": "bookSection",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "219-250",
    "issue": null,
    "volume": null,
    "publisher": "Wiley",
    "abstract": null,
    "publication_date": "1996-12-01T00:00:00",
    "citation_count": 47,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "houwer2019bilinguallanguageacquisition",
    "dockey": "da35f29f70baeb09",
    "citation": "Annick De Houwer. Bilingual language acquisition. The Handbook of Child Language, pages 219-250, Dec 2019. URL: https://doi.org/10.1111/b.9780631203124.1996.00009.x, doi:10.1111/b.9780631203124.1996.00009.x.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "houwer2019bilinguallanguageacquisition",
    "bibtex": "@article{houwer2019bilinguallanguageacquisition,\n    author = \"Houwer, Annick De\",\n    title = \"Bilingual Language Acquisition\",\n    year = \"2019\",\n    journal = \"The Handbook of Child Language\",\n    pages = \"219-250\",\n    month = \"Dec\",\n    doi = \"10.1111/b.9780631203124.1996.00009.x\",\n    url = \"https://doi.org/10.1111/b.9780631203124.1996.00009.x\",\n    publisher = \"Wiley\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1111/b.9780631203124.1996.00009.x",
    "doc_id": "da35f29f70baeb09",
    "formatted_citation": "Annick De Houwer. Bilingual language acquisition. The Handbook of Child Language, pages 219-250, Dec 2019. URL: https://doi.org/10.1111/b.9780631203124.1996.00009.x, doi:10.1111/b.9780631203124.1996.00009.x. This article has 47 citations."
  },
  "Multilingual and code-switching ASR challenges for low resource Indian languages": {
    "title": "Multilingual and code-switching ASR challenges for low resource Indian languages",
    "authors": [
      "Anuj Diwan",
      "Rakesh Vaideeswaran",
      "Sanket Shah",
      "Ankita Singh",
      "Srinivasa Raghavan",
      "Shreya Khare",
      "Vinit Unni",
      "Saurabh Vyas",
      "Akash Rajpuria",
      "Chiranjeevi Yarra",
      "Ashish R. Mittal",
      "P. Ghosh",
      "P. Jyothi",
      "Kalika Bali",
      "Vivek Seshadri",
      "Sunayana Sitaram",
      "Samarth Bharadwaj",
      "Jai Nanavati",
      "Raoul Nanavati",
      "Karthik Sankaranarayanan",
      "Tejaswi Seeram",
      "Basil Abraham"
    ],
    "doi": "10.48550/arxiv.2104.00235",
    "url": "http://arxiv.org/abs/2104.00235",
    "journal": "ArXiv",
    "year": 2021,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://arxiv.org/pdf/2104.00235.pdf",
      "https://arxiv.org/abs/2104.00235"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": "2446-2450",
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": "Recently, there is increasing interest in multilingual automatic speech recognition (ASR) where a speech recognition system caters to multiple low resource languages by taking advantage of low amounts of labeled corpora in multiple languages. With multilingualism becoming common in today's world, there has been increasing interest in code-switching ASR as well. In code-switching, multiple languages are freely interchanged within a single sentence or between sentences. The success of low-resource multilingual and code-switching ASR often depends on the variety of languages in terms of their acoustics, linguistic characteristics as well as the amount of data available and how these are carefully considered in building the ASR system. In this challenge, we would like to focus on building multilingual and code-switching ASR systems through two different subtasks related to a total of seven Indian languages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati and Bengali. For this purpose, we provide a total of ~600 hours of transcribed speech data, comprising train and test sets, in these languages including two code-switched language pairs, Hindi-English and Bengali-English. We also provide a baseline recipe for both the tasks with a WER of 30.73% and 32.45% on the test sets of multilingual and code-switching subtasks, respectively.",
    "publication_date": null,
    "citation_count": 83,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2104.00235",
    "docname": "diwan2021multilingualandcodeswitching",
    "dockey": "d956e4727b28c5e3",
    "citation": "Anuj Diwan, Rakesh Vaideeswaran, Sanket Shah, Ankita Singh, Srinivasa Raghavan, Shreya Khare, Vinit Unni, Saurabh Vyas, Akash Rajpuria, Chiranjeevi Yarra, Ashish R. Mittal, P. Ghosh, P. Jyothi, Kalika Bali, Vivek Seshadri, Sunayana Sitaram, Samarth Bharadwaj, Jai Nanavati, Raoul Nanavati, Karthik Sankaranarayanan, Tejaswi Seeram, and Basil Abraham. Multilingual and code-switching asr challenges for low resource indian languages. ArXiv, pages 2446-2450, 2021. URL: https://doi.org/10.48550/arxiv.2104.00235, doi:10.48550/arxiv.2104.00235.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "diwan2021multilingualandcodeswitching",
    "bibtex": "@article{diwan2021multilingualandcodeswitching,\n    author = \"Diwan, Anuj and Vaideeswaran, Rakesh and Shah, Sanket and Singh, Ankita and Raghavan, Srinivasa and Khare, Shreya and Unni, Vinit and Vyas, Saurabh and Rajpuria, Akash and Yarra, Chiranjeevi and Mittal, Ashish R. and Ghosh, P. and Jyothi, P. and Bali, Kalika and Seshadri, Vivek and Sitaram, Sunayana and Bharadwaj, Samarth and Nanavati, Jai and Nanavati, Raoul and Sankaranarayanan, Karthik and Seeram, Tejaswi and Abraham, Basil\",\n    title = \"Multilingual and code-switching ASR challenges for low resource Indian languages\",\n    year = \"2021\",\n    journal = \"ArXiv\",\n    pages = \"2446-2450\",\n    doi = \"10.48550/arxiv.2104.00235\",\n    url = \"https://doi.org/10.48550/arxiv.2104.00235\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2104.00235",
    "doc_id": "d956e4727b28c5e3",
    "formatted_citation": "Anuj Diwan, Rakesh Vaideeswaran, Sanket Shah, Ankita Singh, Srinivasa Raghavan, Shreya Khare, Vinit Unni, Saurabh Vyas, Akash Rajpuria, Chiranjeevi Yarra, Ashish R. Mittal, P. Ghosh, P. Jyothi, Kalika Bali, Vivek Seshadri, Sunayana Sitaram, Samarth Bharadwaj, Jai Nanavati, Raoul Nanavati, Karthik Sankaranarayanan, Tejaswi Seeram, and Basil Abraham. Multilingual and code-switching asr challenges for low resource indian languages. ArXiv, pages 2446-2450, 2021. URL: https://doi.org/10.48550/arxiv.2104.00235, doi:10.48550/arxiv.2104.00235. This article has 83 citations."
  },
  "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition": {
    "title": "SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition",
    "authors": [
      "Daniel S. Park",
      "William Chan",
      "Yu Zhang",
      "Chung-Cheng Chiu",
      "Barret Zoph",
      "E. D. Cubuk",
      "Quoc V. Le"
    ],
    "doi": "10.48550/arxiv.1904.08779",
    "url": "http://arxiv.org/abs/1904.08779",
    "journal": "ArXiv",
    "year": 2019,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://arxiv.org/pdf/1904.08779.pdf",
      "https://arxiv.org/pdf/1904.08779.pdf",
      "https://arxiv.org/abs/1904.08779",
      "https://arxiv.org/abs/1904.08779"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language",
      "Computer Science - Machine Learning",
      "Statistics - Machine Learning"
    ],
    "pages": "2613-2617",
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": "We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8% WER on test-other without the use of a language model, and 5.8% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5% WER. For Switchboard, we achieve 7.2%/14.6% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8%/14.1% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3%/17.3% WER.",
    "publication_date": null,
    "citation_count": 3437,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "1904.08779",
    "docname": "park2019specaugmentasimple",
    "dockey": "0d147d6e60a15a0a",
    "citation": "Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, E. D. Cubuk, and Quoc V. Le. Specaugment: a simple data augmentation method for automatic speech recognition. ArXiv, pages 2613-2617, 2019. URL: https://doi.org/10.48550/arxiv.1904.08779, doi:10.48550/arxiv.1904.08779.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "park2019specaugmentasimple",
    "bibtex": "@article{park2019specaugmentasimple,\n    author = \"Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, E. D. and Le, Quoc V.\",\n    title = \"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition\",\n    year = \"2019\",\n    journal = \"ArXiv\",\n    pages = \"2613-2617\",\n    doi = \"10.48550/arxiv.1904.08779\",\n    url = \"https://doi.org/10.48550/arxiv.1904.08779\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.1904.08779",
    "doc_id": "0d147d6e60a15a0a",
    "formatted_citation": "Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, E. D. Cubuk, and Quoc V. Le. Specaugment: a simple data augmentation method for automatic speech recognition. ArXiv, pages 2613-2617, 2019. URL: https://doi.org/10.48550/arxiv.1904.08779, doi:10.48550/arxiv.1904.08779. This article has 3437 citations."
  },
  "Self-Supervised Learning for Speech Processing": {
    "title": "CITISEN: A Deep Learning-Based Speech Signal-Processing Mobile Application",
    "authors": [
      "Yu-Wen Chen",
      "Kuo-Hsuan Hung",
      "You-Jin Li",
      "Alexander Chao-Fu Kang",
      "Ya-Hsin Lai",
      "Kai-Chun Liu",
      "Szu-Wei Fu",
      "Syu-Siang Wang",
      "Yu Tsao"
    ],
    "doi": "10.1109/access.2022.3153469",
    "url": "https://doi.org/10.1109/access.2022.3153469",
    "journal": "IEEE Access",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "46082-46099",
    "issue": null,
    "volume": "10",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "abstract": "Deep neural networks trained with supervised learning algorithms on large amounts of labeled speech data have achieved remarkable performance on various spoken language processing applications, often being the state of the arts on the corresponding leaderboards. However, the fact that training these systems relies on large amounts of annotated speech poses a scalability bottleneck for the continued advancement of state-of-the-art performance, and an even more fundamental barrier for deployment of deep neural networks in speech domains where labeled data are intrinsically rare, costly, or time-consuming to collect.",
    "publication_date": "2022-01-01T00:00:00",
    "citation_count": 7,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "chen2022citisenadeep",
    "dockey": "83c0886c3f9a1e5b",
    "citation": "Yu-Wen Chen, Kuo-Hsuan Hung, You-Jin Li, Alexander Chao-Fu Kang, Ya-Hsin Lai, Kai-Chun Liu, Szu-Wei Fu, Syu-Siang Wang, and Yu Tsao. Citisen: a deep learning-based speech signal-processing mobile application. IEEE Access, 10:46082-46099, Jan 2022. URL: https://doi.org/10.1109/access.2022.3153469, doi:10.1109/access.2022.3153469.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "chen2022citisenadeep",
    "bibtex": "@article{chen2022citisenadeep,\n    author = \"Chen, Yu-Wen and Hung, Kuo-Hsuan and Li, You-Jin and Kang, Alexander Chao-Fu and Lai, Ya-Hsin and Liu, Kai-Chun and Fu, Szu-Wei and Wang, Syu-Siang and Tsao, Yu\",\n    title = \"CITISEN: A Deep Learning-Based Speech Signal-Processing Mobile Application\",\n    year = \"2022\",\n    journal = \"IEEE Access\",\n    volume = \"10\",\n    pages = \"46082-46099\",\n    month = \"Jan\",\n    doi = \"10.1109/access.2022.3153469\",\n    url = \"https://doi.org/10.1109/access.2022.3153469\",\n    publisher = \"Institute of Electrical and Electronics Engineers (IEEE)\",\n    issn = \"2169-3536\"\n}\n",
    "issn": "2169-3536",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/access.2022.3153469",
    "doc_id": "83c0886c3f9a1e5b",
    "formatted_citation": "Yu-Wen Chen, Kuo-Hsuan Hung, You-Jin Li, Alexander Chao-Fu Kang, Ya-Hsin Lai, Kai-Chun Liu, Szu-Wei Fu, Syu-Siang Wang, and Yu Tsao. Citisen: a deep learning-based speech signal-processing mobile application. IEEE Access, 10:46082-46099, Jan 2022. URL: https://doi.org/10.1109/access.2022.3153469, doi:10.1109/access.2022.3153469. This article has 7 citations and is from a peer-reviewed journal."
  },
  "SERENGETI: Massively Multilingual Language Models for Africa": {
    "title": "SERENGETI: Massively Multilingual Language Models for Africa",
    "authors": [
      "Ife Adebara",
      "AbdelRahim Elmadany",
      "Muhammad Abdul-Mageed",
      "Alcides Alcoba Inciarte"
    ],
    "doi": "10.18653/v1/2023.findings-acl.97",
    "url": "https://doi.org/10.18653/v1/2023.findings-acl.97",
    "journal": "Findings of the Association for Computational Linguistics: ACL 2023",
    "year": 2023,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2212.10785.pdf",
      "https://arxiv.org/abs/2212.10785"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Artificial Intelligence",
      "Computer Science - Computation and Language"
    ],
    "pages": "1498-1537",
    "issue": null,
    "volume": null,
    "publisher": "Association for Computational Linguistics",
    "abstract": "Multilingual pretrained language models (mPLMs) acquire valuable, generalizable linguistic information during pretraining and have advanced the state of the art on task-specific finetuning. To date, only ~31 out of ~2,000 African languages are covered in existing language models. We ameliorate this limitation by developing SERENGETI, a massively multilingual language model that covers 517 African languages and language varieties. We evaluate our novel models on eight natural language understanding tasks across 20 datasets, comparing to 4 mPLMs that cover 4-23 African languages. SERENGETI outperforms other models on 11 datasets across the eights tasks, achieving 82.27 average F_1. We also perform analyses of errors from our models, which allows us to investigate the influence of language genealogy and linguistic similarity when the models are applied under zero-shot settings. We will publicly release our models for research.\\footnote{\\href{https://github.com/UBC-NLP/serengeti}{https://github.com/UBC-NLP/serengeti}}",
    "publication_date": "2023-01-01T00:00:00",
    "citation_count": 1,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2212.10785",
    "docname": "adebara2023serengetimassivelymultilingual",
    "dockey": "d0f38fb647fa7046",
    "citation": "Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba Inciarte. Serengeti: massively multilingual language models for africa. Findings of the Association for Computational Linguistics: ACL 2023, pages 1498-1537, Jan 2023. URL: https://doi.org/10.18653/v1/2023.findings-acl.97, doi:10.18653/v1/2023.findings-acl.97.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "adebara2023serengetimassivelymultilingual",
    "bibtex": "@article{adebara2023serengetimassivelymultilingual,\n    author = \"Adebara, Ife and Elmadany, AbdelRahim and Abdul-Mageed, Muhammad and Inciarte, Alcides Alcoba\",\n    title = \"SERENGETI: Massively Multilingual Language Models for Africa\",\n    year = \"2023\",\n    journal = \"Findings of the Association for Computational Linguistics: ACL 2023\",\n    pages = \"1498-1537\",\n    month = \"Jan\",\n    doi = \"10.18653/v1/2023.findings-acl.97\",\n    url = \"https://doi.org/10.18653/v1/2023.findings-acl.97\",\n    publisher = \"Association for Computational Linguistics\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.18653/v1/2023.findings-acl.97",
    "doc_id": "d0f38fb647fa7046",
    "formatted_citation": "Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba Inciarte. Serengeti: massively multilingual language models for africa. Findings of the Association for Computational Linguistics: ACL 2023, pages 1498-1537, Jan 2023. URL: https://doi.org/10.18653/v1/2023.findings-acl.97, doi:10.18653/v1/2023.findings-acl.97. This article has 1 citations."
  },
  "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques": {
    "title": "Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques",
    "authors": [
      "Robert C. Gale",
      "Liu Chen",
      "Jill K. Dolata",
      "J. V. Santen",
      "Meysam Asgari"
    ],
    "doi": "10.21437/interspeech.2019-3161",
    "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575194/",
    "journal": "Interspeech",
    "year": 2019,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575194/pdf/nihms-1052986.pdf",
      "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7575194/"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "\n11-15\n",
    "issue": null,
    "volume": "2019",
    "publisher": null,
    "abstract": "This study explores building and improving an automatic speech recognition (ASR) system for children aged 6–9 years and diagnosed with autism spectrum disorder (ASD), language impairment (LI), or both. Working with only 1.5 hours of target data in which children perform the Clinical Evaluation of Language Fundamentals Recalling Sentences task, we apply deep neural network (DNN) weight transfer techniques to adapt a large DNN model trained on the LibriSpeech corpus of adult speech. To begin, we aim to find the best proportional training rates of the DNN layers. Our best configuration yields a 29.38% word error rate (WER). Using this configuration, we explore the effects of quantity and similarity of data augmentation in transfer learning. We augment our training with portions of the OGI Kids’ Corpus, adding 4.6 hours of typically developing speakers aged kindergarten through 3rd grade. We find that 2nd grade data alone — approximately the mean age of the target data — outperforms other grades and all the sets combined. Doubling the data for 1st, 2nd, and 3rd grade, we again compare each grade as well as pairs of grades. We find the combination of 1st and 2nd grade performs best at a 26.21% WER.",
    "publication_date": null,
    "citation_count": 30,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "gale2019improvingasrsystems",
    "dockey": "5e683ed53079f5c4",
    "citation": "Robert C. Gale, Liu Chen, Jill K. Dolata, J. V. Santen, and Meysam Asgari. Improving asr systems for children with autism and language impairment using domain-focused dnn transfer techniques. Interspeech, 2019:11-15, 2019. URL: https://doi.org/10.21437/interspeech.2019-3161, doi:10.21437/interspeech.2019-3161.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "gale2019improvingasrsystems",
    "bibtex": "@article{gale2019improvingasrsystems,\n    author = \"Gale, Robert C. and Chen, Liu and Dolata, Jill K. and Santen, J. V. and Asgari, Meysam\",\n    title = \"Improving ASR Systems for Children with Autism and Language Impairment Using Domain-Focused DNN Transfer Techniques\",\n    year = \"2019\",\n    journal = \"Interspeech\",\n    volume = \"2019\",\n    pages = \"\n11-15\n\",\n    doi = \"10.21437/interspeech.2019-3161\",\n    url = \"https://doi.org/10.21437/interspeech.2019-3161\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2019-3161",
    "doc_id": "5e683ed53079f5c4",
    "formatted_citation": "Robert C. Gale, Liu Chen, Jill K. Dolata, J. V. Santen, and Meysam Asgari. Improving asr systems for children with autism and language impairment using domain-focused dnn transfer techniques. Interspeech, 2019:11-15, 2019. URL: https://doi.org/10.21437/interspeech.2019-3161, doi:10.21437/interspeech.2019-3161. This article has 30 citations and is from a peer-reviewed journal."
  },
  "NITK Kids’ Speech Corpus": {
    "title": "NITK Kids’ Speech Corpus",
    "authors": [
      "Pravin Bhaskar Ramteke",
      "Sujata Supanekar",
      "Pradyoth Hegde",
      "Hanna Nelson",
      "Venkataraja Aithal",
      "Shashidhar G. Koolagudi"
    ],
    "doi": "10.21437/interspeech.2019-2061",
    "url": "https://doi.org/10.21437/interspeech.2019-2061",
    "journal": "Interspeech 2019",
    "year": 2019,
    "genre": "book",
    "link_attachments": [
      "https://www.researchgate.net/profile/Pradyoth-Hegde/publication/335829087_NITK_Kids'_Speech_Corpus/links/5db84cdba6fdcc2128eb86a3/NITK-Kids-Speech-Corpus.pdf",
      "https://www.researchgate.net/publication/335829087_NITK_Kids%27_Speech_Corpus"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "331-335",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": null,
    "publication_date": "2019-09-15T00:00:00",
    "citation_count": 8,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "ramteke2019nitkkids’speech",
    "dockey": "fd598290b953b1f2",
    "citation": "Pravin Bhaskar Ramteke, Sujata Supanekar, Pradyoth Hegde, Hanna Nelson, Venkataraja Aithal, and Shashidhar G. Koolagudi. Nitk kids’ speech corpus. Interspeech 2019, pages 331-335, Sep 2019. URL: https://doi.org/10.21437/interspeech.2019-2061, doi:10.21437/interspeech.2019-2061.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "ramteke2019nitkkids’speech",
    "bibtex": "@article{ramteke2019nitkkids’speech,\n    author = \"Ramteke, Pravin Bhaskar and Supanekar, Sujata and Hegde, Pradyoth and Nelson, Hanna and Aithal, Venkataraja and Koolagudi, Shashidhar G.\",\n    title = \"NITK Kids’ Speech Corpus\",\n    year = \"2019\",\n    journal = \"Interspeech 2019\",\n    pages = \"331-335\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2019-2061\",\n    url = \"https://doi.org/10.21437/interspeech.2019-2061\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2019-2061",
    "doc_id": "fd598290b953b1f2",
    "formatted_citation": "Pravin Bhaskar Ramteke, Sujata Supanekar, Pradyoth Hegde, Hanna Nelson, Venkataraja Aithal, and Shashidhar G. Koolagudi. Nitk kids’ speech corpus. Interspeech 2019, pages 331-335, Sep 2019. URL: https://doi.org/10.21437/interspeech.2019-2061, doi:10.21437/interspeech.2019-2061. This article has 8 citations."
  },
  "Design and development a children’s speech database": {
    "title": "160. Urban Speech (Czech) / Stadtsprache (tschechisch)",
    "authors": [
      "Radoslava Brabcová"
    ],
    "doi": "10.1515/9783110215472.2135",
    "url": "https://doi.org/10.1515/9783110215472.2135",
    "journal": "Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science (HSK) 32/2",
    "year": 2014,
    "genre": "journalArticle",
    "link_attachments": [
      "https://arxiv.org/pdf/1605.07735"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2135-2144",
    "issue": null,
    "volume": null,
    "publisher": "DE GRUYTER",
    "abstract": "The report presents the process of planning, designing and the development of a database of spoken children’s speech whose native language is Bulgarian. The proposed model is designed for children between the age of 4 and 6 without speech disorders, and reflects their specific capabilities. At this age most children cannot read, there is no sustained concentration, they are emotional, etc. The aim is to unite all the media information accompanying the recording and processing of spoken speech, thereby to facilitate the work of researchers in the field of speech recognition. This database will be used for the development of systems for children’s speech recognition, children's speech synthesis systems, games which allow voice control, etc. As a result of the proposed model a prototype system for speech recognition is presented.",
    "publication_date": "2014-12-31T00:00:00",
    "citation_count": 0,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "brabcova2014160.urbanspeech",
    "dockey": "66a524351ae1cdde",
    "citation": "Radoslava Brabcová. 160. urban speech (czech) / stadtsprache (tschechisch). Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science (HSK) 32/2, pages 2135-2144, Dec 2014. URL: https://doi.org/10.1515/9783110215472.2135, doi:10.1515/9783110215472.2135.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "brabcova2014160.urbanspeech",
    "bibtex": "@article{brabcova2014160.urbanspeech,\n    author = \"Brabcová, Radoslava\",\n    title = \"160. Urban Speech (Czech) / Stadtsprache (tschechisch)\",\n    year = \"2014\",\n    journal = \"Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science (HSK) 32/2\",\n    pages = \"2135-2144\",\n    month = \"Dec\",\n    doi = \"10.1515/9783110215472.2135\",\n    url = \"https://doi.org/10.1515/9783110215472.2135\",\n    publisher = \"DE GRUYTER\"\n}\n",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1515/9783110215472.2135",
    "doc_id": "66a524351ae1cdde",
    "formatted_citation": "Radoslava Brabcová. 160. urban speech (czech) / stadtsprache (tschechisch). Handbücher zur Sprach- und Kommunikationswissenschaft / Handbooks of Linguistics and Communication Science (HSK) 32/2, pages 2135-2144, Dec 2014. URL: https://doi.org/10.1515/9783110215472.2135, doi:10.1515/9783110215472.2135. This article has 0 citations."
  },
  "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning": {
    "title": "Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning",
    "authors": [
      "Runxin Xu",
      "Fuli Luo",
      "Zhiyuan Zhang",
      "Chuanqi Tan",
      "Baobao Chang",
      "Songfang Huang",
      "Fei Huang"
    ],
    "doi": "10.18653/v1/2021.emnlp-main.749",
    "url": "https://doi.org/10.18653/v1/2021.emnlp-main.749",
    "journal": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    "year": 2021,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://aclanthology.org/2021.emnlp-main.749.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "9514–9528",
    "issue": null,
    "volume": "abs/2109.05687",
    "publisher": "Association for Computational Linguistics",
    "abstract": "Recent pretrained language models extend from millions to billions of parameters. Thus the need to fine-tune an extremely large pretrained model with a limited training corpus arises in various downstream tasks. In this paper, we propose a straightforward yet effective fine-tuning technique, Child-Tuning, which updates a subset of parameters (called child network) of large pretrained models via strategically masking out the gradients of the non-child network during the backward process. Experiments on various downstream tasks in GLUE benchmark show that Child-Tuning consistently outperforms the vanilla fine-tuning by 1.5 8.6 average score among four different pretrained models, and surpasses the prior fine-tuning techniques by 0.6 1.3 points. Furthermore, empirical results on domain transfer and task transfer show that Child-Tuning can obtain better generalization performance by large margins.",
    "publication_date": "2021-01-01T00:00:00",
    "citation_count": 184,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "xu2021raiseachild",
    "dockey": "e97cae07d157a054",
    "citation": "Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise a child in large language model: towards effective and generalizable fine-tuning. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Jan 2021. URL: https://doi.org/10.18653/v1/2021.emnlp-main.749, doi:10.18653/v1/2021.emnlp-main.749.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "xu2021raiseachild",
    "bibtex": "@article{xu2021raiseachild,\n    author = \"Xu, Runxin and Luo, Fuli and Zhang, Zhiyuan and Tan, Chuanqi and Chang, Baobao and Huang, Songfang and Huang, Fei\",\n    title = \"Raise a Child in Large Language Model: Towards Effective and Generalizable Fine-tuning\",\n    year = \"2021\",\n    journal = \"Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing\",\n    volume = \"abs/2109.05687\",\n    month = \"Jan\",\n    doi = \"10.18653/v1/2021.emnlp-main.749\",\n    url = \"https://doi.org/10.18653/v1/2021.emnlp-main.749\",\n    publisher = \"Association for Computational Linguistics\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.18653/v1/2021.emnlp-main.749",
    "doc_id": "e97cae07d157a054",
    "formatted_citation": "Runxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan, Baobao Chang, Songfang Huang, and Fei Huang. Raise a child in large language model: towards effective and generalizable fine-tuning. Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, Jan 2021. URL: https://doi.org/10.18653/v1/2021.emnlp-main.749, doi:10.18653/v1/2021.emnlp-main.749. This article has 184 citations."
  },
  "Automatic Speech Recognition Tuned for Child Speech in the Classroom": {
    "title": "Automatic Speech Recognition Tuned for Child Speech in the Classroom",
    "authors": [
      "Rosy Southwell",
      "Wayne Ward",
      "V. Trinh",
      "Charis Clevenger",
      "Clay Clevenger",
      "Emily Watts",
      "Jason G. Reitman",
      "Sidney D'Mello",
      "Jacob Whitehill"
    ],
    "doi": "10.1109/icassp48485.2024.10447428",
    "url": "https://ieeexplore.ieee.org/document/10447428/",
    "journal": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2024,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.colorado.edu/research/ai-institute/sites/default/files/attached-files/childasr_icassp24_camera-ready_0.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "12291-12295",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2024-03-18T00:00:00",
    "citation_count": 8,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "southwell2024automaticspeechrecognition",
    "dockey": "62090d760752a31c",
    "citation": "Rosy Southwell, Wayne Ward, V. Trinh, Charis Clevenger, Clay Clevenger, Emily Watts, Jason G. Reitman, Sidney D'Mello, and Jacob Whitehill. Automatic speech recognition tuned for child speech in the classroom. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12291-12295, Mar 2024. URL: https://doi.org/10.1109/icassp48485.2024.10447428, doi:10.1109/icassp48485.2024.10447428.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "southwell2024automaticspeechrecognition",
    "bibtex": "@article{southwell2024automaticspeechrecognition,\n    author = \"Southwell, Rosy and Ward, Wayne and Trinh, V. and Clevenger, Charis and Clevenger, Clay and Watts, Emily and Reitman, Jason G. and D'Mello, Sidney and Whitehill, Jacob\",\n    title = \"Automatic Speech Recognition Tuned for Child Speech in the Classroom\",\n    year = \"2024\",\n    journal = \"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"12291-12295\",\n    month = \"Mar\",\n    doi = \"10.1109/icassp48485.2024.10447428\",\n    url = \"https://doi.org/10.1109/icassp48485.2024.10447428\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp48485.2024.10447428",
    "doc_id": "62090d760752a31c",
    "formatted_citation": "Rosy Southwell, Wayne Ward, V. Trinh, Charis Clevenger, Clay Clevenger, Emily Watts, Jason G. Reitman, Sidney D'Mello, and Jacob Whitehill. Automatic speech recognition tuned for child speech in the classroom. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 12291-12295, Mar 2024. URL: https://doi.org/10.1109/icassp48485.2024.10447428, doi:10.1109/icassp48485.2024.10447428. This article has 8 citations."
  },
  "Learning Robust and Multilingual Speech Representations": {
    "title": "Learning Robust and Multilingual Speech Representations",
    "authors": [
      "Kazuya Kawakami",
      "Luyu Wang",
      "Chris Dyer",
      "Phil Blunsom",
      "Aaron van den Oord"
    ],
    "doi": "10.18653/v1/2020.findings-emnlp.106",
    "url": "https://doi.org/10.18653/v1/2020.findings-emnlp.106",
    "journal": "Findings of the Association for Computational Linguistics: EMNLP 2020",
    "year": 2020,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2001.11128.pdf",
      "https://arxiv.org/abs/2001.11128"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language",
      "Computer Science - Machine Learning"
    ],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "Association for Computational Linguistics",
    "abstract": "Unsupervised speech representation learning has shown remarkable success at finding representations that correlate with phonetic structures and improve downstream speech recognition performance. However, most research has been focused on evaluating the representations in terms of their ability to improve the performance of speech recognition systems on read English (e.g. Wall Street Journal and LibriSpeech). This evaluation methodology overlooks two important desiderata that speech representations should have: robustness to domain shifts and transferability to other languages. In this paper we learn representations from up to 8000 hours of diverse and noisy speech data and evaluate the representations by looking at their robustness to domain shifts and their ability to improve recognition performance in many languages. We find that our representations confer significant robustness advantages to the resulting recognition systems: we see significant improvements in out-of-domain transfer relative to baseline feature sets and the features likewise provide improvements in 25 phonetically diverse languages including tonal languages and low-resource languages.",
    "publication_date": "2020-01-01T00:00:00",
    "citation_count": 37,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2001.11128",
    "docname": "kawakami2020learningrobustand",
    "dockey": "96ebbaa876c5cb04",
    "citation": "Kazuya Kawakami, Luyu Wang, Chris Dyer, Phil Blunsom, and Aaron van den Oord. Learning robust and multilingual speech representations. Findings of the Association for Computational Linguistics: EMNLP 2020, Jan 2020. URL: https://doi.org/10.18653/v1/2020.findings-emnlp.106, doi:10.18653/v1/2020.findings-emnlp.106.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "kawakami2020learningrobustand",
    "bibtex": "@article{kawakami2020learningrobustand,\n    author = \"Kawakami, Kazuya and Wang, Luyu and Dyer, Chris and Blunsom, Phil and van den Oord, Aaron\",\n    title = \"Learning Robust and Multilingual Speech Representations\",\n    year = \"2020\",\n    journal = \"Findings of the Association for Computational Linguistics: EMNLP 2020\",\n    month = \"Jan\",\n    doi = \"10.18653/v1/2020.findings-emnlp.106\",\n    url = \"https://doi.org/10.18653/v1/2020.findings-emnlp.106\",\n    publisher = \"Association for Computational Linguistics\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.18653/v1/2020.findings-emnlp.106",
    "doc_id": "96ebbaa876c5cb04",
    "formatted_citation": "Kazuya Kawakami, Luyu Wang, Chris Dyer, Phil Blunsom, and Aaron van den Oord. Learning robust and multilingual speech representations. Findings of the Association for Computational Linguistics: EMNLP 2020, Jan 2020. URL: https://doi.org/10.18653/v1/2020.findings-emnlp.106, doi:10.18653/v1/2020.findings-emnlp.106. This article has 37 citations."
  },
  "Self-Supervised Speech Representation Learning: A Review": {
    "title": "Self-Supervised Speech Representation Learning: A Review",
    "authors": [
      "Abdel-rahman Mohamed",
      "Hung-yi Lee",
      "Lasse Borgholt",
      "Jakob Drachmann Havtorn",
      "Joakim Edin",
      "C. Igel",
      "K. Kirchhoff",
      "Shang-Wen Li",
      "Karen Livescu",
      "Lars Maaløe",
      "Tara N. Sainath",
      "Shinji Watanabe"
    ],
    "doi": "10.48550/arxiv.2205.10643",
    "url": "http://arxiv.org/abs/2205.10643",
    "journal": "IEEE Journal of Selected Topics in Signal Processing",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [
      "https://arxiv.org/pdf/2205.10643.pdf",
      "https://arxiv.org/abs/2205.10643"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": "1179-1210",
    "issue": "6",
    "volume": "16",
    "publisher": null,
    "abstract": "Although supervised deep learning has revolutionized speech and audio processing, it has necessitated the building of specialist models for individual tasks and application scenarios. It is likewise difficult to apply this to dialects and languages for which only limited labeled data is available. Self-supervised representation learning methods promise a single universal model that would benefit a wide variety of tasks and domains. Such methods have shown success in natural language processing and computer vision domains, achieving new levels of performance while reducing the number of labels required for many downstream scenarios. Speech representation learning is experiencing similar progress in three main categories: generative, contrastive, and predictive methods. Other approaches rely on multi-modal data for pre-training, mixing text or visual data streams with speech. Although self-supervised speech representation is still a nascent research area, it is closely related to acoustic word embedding and learning with zero lexical resources, both of which have seen active research for many years. This review presents approaches for self-supervised speech representation learning and their connection to other research areas. Since many current methods focus solely on automatic speech recognition as a downstream task, we review recent efforts on benchmarking learned representations to extend the application beyond speech recognition.",
    "publication_date": null,
    "citation_count": 357,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 3,
    "arxiv_id": "2205.10643",
    "docname": "mohamed2022selfsupervisedspeechrepresentation",
    "dockey": "3a1431bdb1635448",
    "citation": "Abdel-rahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob Drachmann Havtorn, Joakim Edin, C. Igel, K. Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maaløe, Tara N. Sainath, and Shinji Watanabe. Self-supervised speech representation learning: a review. IEEE Journal of Selected Topics in Signal Processing, 16:1179-1210, 2022. URL: https://doi.org/10.48550/arxiv.2205.10643, doi:10.48550/arxiv.2205.10643.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "mohamed2022selfsupervisedspeechrepresentation",
    "bibtex": "@article{mohamed2022selfsupervisedspeechrepresentation,\n    author = \"Mohamed, Abdel-rahman and Lee, Hung-yi and Borgholt, Lasse and Havtorn, Jakob Drachmann and Edin, Joakim and Igel, C. and Kirchhoff, K. and Li, Shang-Wen and Livescu, Karen and Maaløe, Lars and Sainath, Tara N. and Watanabe, Shinji\",\n    title = \"Self-Supervised Speech Representation Learning: A Review\",\n    year = \"2022\",\n    journal = \"IEEE Journal of Selected Topics in Signal Processing\",\n    volume = \"16\",\n    pages = \"1179-1210\",\n    doi = \"10.48550/arxiv.2205.10643\",\n    url = \"https://doi.org/10.48550/arxiv.2205.10643\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2205.10643",
    "doc_id": "3a1431bdb1635448",
    "formatted_citation": "Abdel-rahman Mohamed, Hung-yi Lee, Lasse Borgholt, Jakob Drachmann Havtorn, Joakim Edin, C. Igel, K. Kirchhoff, Shang-Wen Li, Karen Livescu, Lars Maaløe, Tara N. Sainath, and Shinji Watanabe. Self-supervised speech representation learning: a review. IEEE Journal of Selected Topics in Signal Processing, 16:1179-1210, 2022. URL: https://doi.org/10.48550/arxiv.2205.10643, doi:10.48550/arxiv.2205.10643. This article has 357 citations and is from a highest quality peer-reviewed journal."
  },
  "“You don’t understand me!”: Comparing ASR results for L1 and L2 speakers of Swedish": {
    "title": "Do You Know Me? Decomposing Identifiability",
    "authors": [
      "Ronald E. Leenes"
    ],
    "doi": "10.2139/ssrn.1084878",
    "url": "https://doi.org/10.2139/ssrn.1084878",
    "journal": "SSRN Electronic Journal",
    "year": 2008,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.diva-portal.org/smash/get/diva2:1663482/FULLTEXT01.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "96–100",
    "issue": null,
    "volume": "",
    "publisher": "Elsevier BV",
    "abstract": null,
    "publication_date": "2008-01-01T00:00:00",
    "citation_count": 10,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "leenes2008doyouknow",
    "dockey": "0539de06645114b1",
    "citation": "Ronald E. Leenes. Do you know me? decomposing identifiability. SSRN Electronic Journal, Jan 2008. URL: https://doi.org/10.2139/ssrn.1084878, doi:10.2139/ssrn.1084878.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "leenes2008doyouknow",
    "bibtex": "@article{leenes2008doyouknow,\n    author = \"Leenes, Ronald E.\",\n    title = \"Do You Know Me? Decomposing Identifiability\",\n    year = \"2008\",\n    journal = \"SSRN Electronic Journal\",\n    month = \"Jan\",\n    doi = \"10.2139/ssrn.1084878\",\n    url = \"https://doi.org/10.2139/ssrn.1084878\",\n    publisher = \"Elsevier BV\",\n    issn = \"1556-5068\"\n}\n",
    "issn": "1556-5068",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.2139/ssrn.1084878",
    "doc_id": "0539de06645114b1",
    "formatted_citation": "Ronald E. Leenes. Do you know me? decomposing identifiability. SSRN Electronic Journal, Jan 2008. URL: https://doi.org/10.2139/ssrn.1084878, doi:10.2139/ssrn.1084878. This article has 10 citations."
  },
  "Multidisciplinary Perspectives on Automatic Analysis of Children’s Language Samples: Where Do We Go from Here?": {
    "title": "Multidisciplinary Perspectives on Automatic Analysis of Children’s Language Samples: Where Do We Go from Here?",
    "authors": [
      "Ulrike Lüdtke",
      "Juan Bornman",
      "Febe de Wet",
      "Ulrich Heid",
      "Jörn Ostermann",
      "Lars Rumberg",
      "Jeannie van der Linde",
      "Hanna Ehlert"
    ],
    "doi": "10.1159/000527427",
    "url": "https://doi.org/10.1159/000527427",
    "journal": "Folia Phoniatrica et Logopaedica",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [
      "https://repository.up.ac.za/bitstream/handle/2263/92336/Ludtke_Multidisciplinary_2023.pdf?sequence=1"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1-12",
    "issue": "1",
    "volume": "75",
    "publisher": "S. Karger AG",
    "abstract": null,
    "publication_date": "2022-10-07T00:00:00",
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "ludtke2022multidisciplinaryperspectiveson",
    "dockey": "d9f1298cff33c469",
    "citation": "Ulrike Lüdtke, Juan Bornman, Febe de Wet, Ulrich Heid, Jörn Ostermann, Lars Rumberg, Jeannie van der Linde, and Hanna Ehlert. Multidisciplinary perspectives on automatic analysis of children’s language samples: where do we go from here? Folia Phoniatrica et Logopaedica, 75:1-12, Oct 2022. URL: https://doi.org/10.1159/000527427, doi:10.1159/000527427.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "ludtke2022multidisciplinaryperspectiveson",
    "bibtex": "@article{ludtke2022multidisciplinaryperspectiveson,\n    author = \"Lüdtke, Ulrike and Bornman, Juan and de Wet, Febe and Heid, Ulrich and Ostermann, Jörn and Rumberg, Lars and van der Linde, Jeannie and Ehlert, Hanna\",\n    title = \"Multidisciplinary Perspectives on Automatic Analysis of Children’s Language Samples: Where Do We Go from Here?\",\n    year = \"2022\",\n    journal = \"Folia Phoniatrica et Logopaedica\",\n    volume = \"75\",\n    pages = \"1-12\",\n    month = \"Oct\",\n    doi = \"10.1159/000527427\",\n    url = \"https://doi.org/10.1159/000527427\",\n    publisher = \"S. Karger AG\",\n    issue = \"1\",\n    issn = \"1021-7762\"\n}\n",
    "issn": "1021-7762",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1159/000527427",
    "doc_id": "d9f1298cff33c469",
    "formatted_citation": "Ulrike Lüdtke, Juan Bornman, Febe de Wet, Ulrich Heid, Jörn Ostermann, Lars Rumberg, Jeannie van der Linde, and Hanna Ehlert. Multidisciplinary perspectives on automatic analysis of children’s language samples: where do we go from here? Folia Phoniatrica et Logopaedica, 75:1-12, Oct 2022. URL: https://doi.org/10.1159/000527427, doi:10.1159/000527427. This article has 2 citations and is from a peer-reviewed journal."
  },
  "The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech.": {
    "title": "The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech",
    "authors": [
      "Gaopeng Xu",
      "Song Yang",
      "Lu Ma",
      "Chengfei Li",
      "Zhongqin Wu"
    ],
    "doi": "10.21437/interspeech.2021-1104",
    "url": "https://doi.org/10.21437/interspeech.2021-1104",
    "journal": "Interspeech 2021",
    "year": 2021,
    "genre": "conferencePaper",
    "link_attachments": [
      "https://www.isca-archive.org/interspeech_2021/xu21c_interspeech.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1294-1298",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": null,
    "publication_date": "2021-08-30T00:00:00",
    "citation_count": 4,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "xu2021thetalsystem",
    "dockey": "f669153014d89966",
    "citation": "Gaopeng Xu, Song Yang, Lu Ma, Chengfei Li, and Zhongqin Wu. The tal system for the interspeech2021 shared task on automatic speech recognition for non-native childrens speech. Interspeech 2021, pages 1294-1298, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-1104, doi:10.21437/interspeech.2021-1104.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "xu2021thetalsystem",
    "bibtex": "@article{xu2021thetalsystem,\n    author = \"Xu, Gaopeng and Yang, Song and Ma, Lu and Li, Chengfei and Wu, Zhongqin\",\n    title = \"The TAL System for the INTERSPEECH2021 Shared Task on Automatic Speech Recognition for Non-Native Childrens Speech\",\n    year = \"2021\",\n    journal = \"Interspeech 2021\",\n    pages = \"1294-1298\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2021-1104\",\n    url = \"https://doi.org/10.21437/interspeech.2021-1104\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2021-1104",
    "doc_id": "f669153014d89966",
    "formatted_citation": "Gaopeng Xu, Song Yang, Lu Ma, Chengfei Li, and Zhongqin Wu. The tal system for the interspeech2021 shared task on automatic speech recognition for non-native childrens speech. Interspeech 2021, pages 1294-1298, Aug 2021. URL: https://doi.org/10.21437/interspeech.2021-1104, doi:10.21437/interspeech.2021-1104. This article has 4 citations."
  },
  "Understanding Multilingual Language Models: Training, Representation and Architecture": {
    "title": "Are Multilingual Neural Machine Translation Models Better at Capturing Linguistic Features?",
    "authors": [
      "David Mareček",
      "Hande Celikkanat",
      "Miikka Silfverberg",
      "Vinit Ravishankar",
      "Jörg Tiedemann"
    ],
    "doi": "10.14712/00326585.009",
    "url": "https://doi.org/10.14712/00326585.009",
    "journal": "Prague Bulletin of Mathematical Linguistics",
    "year": 2020,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.duo.uio.no/bitstream/handle/10852/102833/PhD-Ravishankar-2023.pdf?sequence=1"
    ],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "143-162",
    "issue": "1",
    "volume": "115",
    "publisher": "Charles University in Prague, Karolinum Press",
    "abstract": null,
    "publication_date": "2020-10-01T00:00:00",
    "citation_count": 9,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "marecek2020aremultilingualneural",
    "dockey": "30e7987fe58ca876",
    "citation": "David Mareček, Hande Celikkanat, Miikka Silfverberg, Vinit Ravishankar, and Jörg Tiedemann. Are multilingual neural machine translation models better at capturing linguistic features? Prague Bulletin of Mathematical Linguistics, 115:143-162, Oct 2020. URL: https://doi.org/10.14712/00326585.009, doi:10.14712/00326585.009.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "marecek2020aremultilingualneural",
    "bibtex": "@article{marecek2020aremultilingualneural,\n    author = \"Mareček, David and Celikkanat, Hande and Silfverberg, Miikka and Ravishankar, Vinit and Tiedemann, Jörg\",\n    title = \"Are Multilingual Neural Machine Translation Models Better at Capturing Linguistic Features?\",\n    year = \"2020\",\n    journal = \"Prague Bulletin of Mathematical Linguistics\",\n    volume = \"115\",\n    pages = \"143-162\",\n    month = \"Oct\",\n    doi = \"10.14712/00326585.009\",\n    url = \"https://doi.org/10.14712/00326585.009\",\n    publisher = \"Charles University in Prague, Karolinum Press\",\n    issue = \"1\",\n    issn = \"1804-0462\"\n}\n",
    "issn": "1804-0462",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.14712/00326585.009",
    "doc_id": "30e7987fe58ca876",
    "formatted_citation": "David Mareček, Hande Celikkanat, Miikka Silfverberg, Vinit Ravishankar, and Jörg Tiedemann. Are multilingual neural machine translation models better at capturing linguistic features? Prague Bulletin of Mathematical Linguistics, 115:143-162, Oct 2020. URL: https://doi.org/10.14712/00326585.009, doi:10.14712/00326585.009. This article has 9 citations and is from a peer-reviewed journal."
  },
  "Code-switching in automatic speech recognition: The issues and future directions": {
    "title": "Automatic speech recognition system for Malay speaking children",
    "authors": [
      "Feisal Dani Rahman",
      "Noraini Mohamed",
      "Mumtaz Begum Mustafa",
      "Siti Salwah Salim"
    ],
    "doi": "10.1109/ict-ispc.2014.6923222",
    "url": "https://doi.org/10.1109/ict-ispc.2014.6923222",
    "journal": "2014 Third ICT International Student Project Conference (ICT-ISPC)",
    "year": 2014,
    "genre": "journalArticle",
    "link_attachments": [
      "https://www.mdpi.com/2076-3417/12/19/9541",
      "https://figshare.cardiffmet.ac.uk/articles/journal_contribution/Code-Switching_in_Automatic_Speech_Recognition_The_Issues_and_Future_Directions/21511053/1/files/38126151.pdf"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "multilingual speech recognition",
      "automatic speech recognition system",
      "bilingual speech recognition",
      "code-switching",
      "evaluation metrics",
      "language and acoustic models"
    ],
    "pages": "79-82",
    "issue": "19",
    "volume": "12",
    "publisher": "IEEE",
    "abstract": null,
    "publication_date": "2014-03-01T00:00:00",
    "citation_count": 15,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "rahman2014automaticspeechrecognition",
    "dockey": "17fda9587a8c1f44",
    "citation": "Feisal Dani Rahman, Noraini Mohamed, Mumtaz Begum Mustafa, and Siti Salwah Salim. Automatic speech recognition system for malay speaking children. 2014 Third ICT International Student Project Conference (ICT-ISPC), pages 79-82, Mar 2014. URL: https://doi.org/10.1109/ict-ispc.2014.6923222, doi:10.1109/ict-ispc.2014.6923222.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "rahman2014automaticspeechrecognition",
    "bibtex": "@article{rahman2014automaticspeechrecognition,\n    author = \"Rahman, Feisal Dani and Mohamed, Noraini and Mustafa, Mumtaz Begum and Salim, Siti Salwah\",\n    title = \"Automatic speech recognition system for Malay speaking children\",\n    year = \"2014\",\n    journal = \"2014 Third ICT International Student Project Conference (ICT-ISPC)\",\n    pages = \"79-82\",\n    month = \"Mar\",\n    doi = \"10.1109/ict-ispc.2014.6923222\",\n    url = \"https://doi.org/10.1109/ict-ispc.2014.6923222\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/ict-ispc.2014.6923222",
    "doc_id": "17fda9587a8c1f44",
    "formatted_citation": "Feisal Dani Rahman, Noraini Mohamed, Mumtaz Begum Mustafa, and Siti Salwah Salim. Automatic speech recognition system for malay speaking children. 2014 Third ICT International Student Project Conference (ICT-ISPC), pages 79-82, Mar 2014. URL: https://doi.org/10.1109/ict-ispc.2014.6923222, doi:10.1109/ict-ispc.2014.6923222. This article has 15 citations."
  },
  "Massively Multilingual Text Translation For Low-Resource Languages": {
    "title": "Paraphrases as Foreign Languages in Multilingual Neural Machine Translation",
    "authors": [
      "Zhong Zhou",
      "Matthias Sperber",
      "Alexander Waibel"
    ],
    "doi": "10.18653/v1/p19-2015",
    "url": "https://doi.org/10.18653/v1/p19-2015",
    "journal": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop",
    "year": 2019,
    "genre": "preprint",
    "link_attachments": [
      "https://arxiv.org/pdf/2401.16582.pdf",
      "https://arxiv.org/abs/2401.16582"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Computation and Language"
    ],
    "pages": "113-122",
    "issue": null,
    "volume": "abs/1808.08438",
    "publisher": "Association for Computational Linguistics",
    "abstract": "Translation into severely low-resource languages has both the cultural goal of saving and reviving those languages and the humanitarian goal of assisting the everyday needs of local communities that are accelerated by the recent COVID-19 pandemic. In many humanitarian efforts, translation into severely low-resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, low-resource languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich-resource languages to efficiently produce best possible translation quality for well known texts, which are available in multiple languages, in a new, low-resource language. To reach this goal, we argue that in translating a closed text into low-resource languages, generalization to out-of-domain texts is not necessary, but generalization to new languages is. Performance gain comes from massive source parallelism by careful choice of close-by language families, style-consistent corpus-level paraphrases within the same language and strategic adaptation of existing large pretrained multilingual models to the domain first and then to the language. Such performance gain makes it possible for machine translation systems to collaborate with human translators to expedite the translation process into new, low-resource languages.",
    "publication_date": "2019-01-01T00:00:00",
    "citation_count": 19,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2401.16582",
    "docname": "zhou2019paraphrasesasforeign",
    "dockey": "7371bed28f534790",
    "citation": "Zhong Zhou, Matthias Sperber, and Alexander Waibel. Paraphrases as foreign languages in multilingual neural machine translation. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, abs/1808.08438:113-122, Jan 2019. URL: https://doi.org/10.18653/v1/p19-2015, doi:10.18653/v1/p19-2015.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "zhou2019paraphrasesasforeign",
    "bibtex": "@article{zhou2019paraphrasesasforeign,\n    author = \"Zhou, Zhong and Sperber, Matthias and Waibel, Alexander\",\n    title = \"Paraphrases as Foreign Languages in Multilingual Neural Machine Translation\",\n    year = \"2019\",\n    journal = \"Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop\",\n    volume = \"abs/1808.08438\",\n    pages = \"113-122\",\n    month = \"Jan\",\n    doi = \"10.18653/v1/p19-2015\",\n    url = \"https://doi.org/10.18653/v1/p19-2015\",\n    publisher = \"Association for Computational Linguistics\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.18653/v1/p19-2015",
    "doc_id": "7371bed28f534790",
    "formatted_citation": "Zhong Zhou, Matthias Sperber, and Alexander Waibel. Paraphrases as foreign languages in multilingual neural machine translation. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, abs/1808.08438:113-122, Jan 2019. URL: https://doi.org/10.18653/v1/p19-2015, doi:10.18653/v1/p19-2015. This article has 19 citations."
  },
  "Measuring the Accuracy of Automatic Speech Recognition Solutions": {
    "title": "Measuring the Accuracy of Automatic Speech Recognition Solutions",
    "authors": [
      "Korbinian Kuhn",
      "Verena Kersken",
      "Benedikt Reuter",
      "Niklas Egger",
      "Gottfried Zimmermann"
    ],
    "doi": "10.1145/3636513",
    "url": "https://doi.org/10.1145/3636513",
    "journal": "ACM Transactions on Accessible Computing",
    "year": 2023,
    "genre": "journalArticle",
    "link_attachments": [
      "https://dl.acm.org/doi/pdf/10.1145/3636513"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "captions",
      "real time",
      "subtitles",
      "Transcription"
    ],
    "pages": "1-23",
    "issue": "4",
    "volume": "16",
    "publisher": "Association for Computing Machinery (ACM)",
    "abstract": "For d/Deaf and hard of hearing (DHH) people, captioning is an essential accessibility tool. Significant developments in artificial intelligence mean that automatic speech recognition (ASR) is now a part of many popular applications. This makes creating captions easy and broadly available—but transcription needs high levels of accuracy to be accessible. Scientific publications and industry report very low error rates, claiming that artificial intelligence has reached human parity or even outperforms manual transcription. At the same time, the DHH community reports serious issues with the accuracy and reliability of ASR. There seems to be a mismatch between technical innovations and the real-life experience for people who depend on transcription. Independent and comprehensive data is needed to capture the state of ASR. We measured the performance of 11 common ASR services with recordings of Higher Education lectures. We evaluated the influence of technical conditions like streaming, the use of vocabularies, and differences between languages. Our results show that accuracy ranges widely between vendors and for the individual audio samples. We also measured a significant lower quality for streaming ASR, which is used for live events. Our study shows that despite the recent improvements of ASR, common services lack reliability in accuracy.",
    "publication_date": "2023-12-31T00:00:00",
    "citation_count": 20,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "kuhn2023measuringtheaccuracy",
    "dockey": "73d04ece18f27a91",
    "citation": "Korbinian Kuhn, Verena Kersken, Benedikt Reuter, Niklas Egger, and Gottfried Zimmermann. Measuring the accuracy of automatic speech recognition solutions. ACM Transactions on Accessible Computing, 16:1-23, Dec 2023. URL: https://doi.org/10.1145/3636513, doi:10.1145/3636513.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "kuhn2023measuringtheaccuracy",
    "bibtex": "@article{kuhn2023measuringtheaccuracy,\n    author = \"Kuhn, Korbinian and Kersken, Verena and Reuter, Benedikt and Egger, Niklas and Zimmermann, Gottfried\",\n    title = \"Measuring the Accuracy of Automatic Speech Recognition Solutions\",\n    year = \"2023\",\n    journal = \"ACM Transactions on Accessible Computing\",\n    volume = \"16\",\n    pages = \"1-23\",\n    month = \"Dec\",\n    doi = \"10.1145/3636513\",\n    url = \"https://doi.org/10.1145/3636513\",\n    publisher = \"Association for Computing Machinery (ACM)\",\n    issue = \"4\",\n    issn = \"1936-7228\"\n}\n",
    "issn": "1936-7228",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1145/3636513",
    "doc_id": "73d04ece18f27a91",
    "formatted_citation": "Korbinian Kuhn, Verena Kersken, Benedikt Reuter, Niklas Egger, and Gottfried Zimmermann. Measuring the accuracy of automatic speech recognition solutions. ACM Transactions on Accessible Computing, 16:1-23, Dec 2023. URL: https://doi.org/10.1145/3636513, doi:10.1145/3636513. This article has 20 citations and is from a peer-reviewed journal."
  },
  "Bilingual first language acquisition": {
    "title": "Bilingual First Language Acquisition",
    "authors": [
      "Fred Genesee",
      "Elena Nicoladis"
    ],
    "doi": "10.1002/9780470757833.ch16",
    "url": "https://doi.org/10.1002/9780470757833.ch16",
    "journal": "Blackwell Handbook of Language Development",
    "year": 2008,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "324-342",
    "issue": null,
    "volume": "",
    "publisher": "Blackwell Publishing Ltd",
    "abstract": null,
    "publication_date": "2008-01-28T00:00:00",
    "citation_count": 92,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "genesee2008bilingualfirstlanguage",
    "dockey": "6b362c963c0142e0",
    "citation": "Fred Genesee and Elena Nicoladis. Bilingual first language acquisition. Blackwell Handbook of Language Development, pages 324-342, Jan 2008. URL: https://doi.org/10.1002/9780470757833.ch16, doi:10.1002/9780470757833.ch16.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "genesee2008bilingualfirstlanguage",
    "bibtex": "@article{genesee2008bilingualfirstlanguage,\n    author = \"Genesee, Fred and Nicoladis, Elena\",\n    title = \"Bilingual First Language Acquisition\",\n    year = \"2008\",\n    journal = \"Blackwell Handbook of Language Development\",\n    pages = \"324-342\",\n    month = \"Jan\",\n    doi = \"10.1002/9780470757833.ch16\",\n    url = \"https://doi.org/10.1002/9780470757833.ch16\",\n    publisher = \"Blackwell Publishing Ltd\"\n}\n",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1002/9780470757833.ch16",
    "doc_id": "6b362c963c0142e0",
    "formatted_citation": "Fred Genesee and Elena Nicoladis. Bilingual first language acquisition. Blackwell Handbook of Language Development, pages 324-342, Jan 2008. URL: https://doi.org/10.1002/9780470757833.ch16, doi:10.1002/9780470757833.ch16. This article has 92 citations."
  },
  "A Wav2vec2-Based Experimental Study on Self-Supervised Learning Methods to Improve Child Speech Recognition": {
    "title": "Can Self-Supervised Learning solve the problem of child speech recognition?",
    "authors": [
      "Rishabh Jain",
      "Mariam Yiwere",
      "Dan Bigioi",
      "Peter Corcoran"
    ],
    "doi": "10.48550/arxiv.2204.05419",
    "url": "https://arxiv.org/abs/2204.05419",
    "journal": "ArXiv",
    "year": 2022,
    "genre": "journalArticle",
    "link_attachments": [
      "https://ieeexplore.ieee.org/ielx7/6287639/10005208/10122501.pdf",
      "https://www.semanticscholar.org/paper/A-WAV2VEC2-Based-Experimental-Study-on-Learning-to-Jain-Barcovschi/1784f193a4e15dd21ff4ae40334266bb0c3af7a9",
      "https://www.semanticscholar.org/paper/A-WAV2VEC2-Based-Experimental-Study-on-Learning-to-Jain-Barcovschi/1784f193a4e15dd21ff4ae40334266bb0c3af7a9",
      "https://www.semanticscholar.org/paper/Can-Self-Supervised-Learning-solve-the-problem-of-Jain-Yiwere/52e7b2074278477b276cdf11433d08335e25fb9e"
    ],
    "manual_tags": [],
    "automatic_tags": [
      "Audio and Speech Processing (eess.AS)",
      "FOS: Computer and information sciences",
      "FOS: Electrical engineering, electronic engineering, information engineering",
      "Sound (cs.SD)"
    ],
    "pages": null,
    "issue": null,
    "volume": "abs/2204.05419",
    "publisher": null,
    "abstract": "Despite recent advancements in deep learning technologies, Child Speech Recognition remains a challenging task. Current Automatic Speech Recognition (ASR) models require substantial amounts of annotated data for training, which is scarce. In this work, we explore using the ASR model, wav2vec2, with different pretraining and finetuning configurations for self-supervised learning (SSL) toward improving automatic child speech recognition. The pretrained wav2vec2 models were finetuned using different amounts of child speech training data, adult speech data, and a combination of both, to discover the optimum amount of data required to finetune the model for the task of child ASR. Our trained model achieves the best Word Error Rate (WER) of 7.42 on the MyST child speech dataset, 2.99 on the PFSTAR dataset and 12.47 on the CMU KIDS dataset as compared to any other previous methods. Our models outperformed the wav2vec2 BASE 960 on child speech which is considered a state-of-the-art ASR model on adult speech by just using 10 hours of child speech data in finetuning. The analysis of different types of training data and their effect on inference is also provided by using a combination of datasets in pretraining, finetuning and inference.",
    "publication_date": null,
    "citation_count": 1,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2204.05419",
    "docname": "jain2022canselfsupervisedlearning",
    "dockey": "2006e44a826263f7",
    "citation": "Rishabh Jain, Mariam Yiwere, Dan Bigioi, and Peter Corcoran. Can self-supervised learning solve the problem of child speech recognition? ArXiv, 2022. URL: https://doi.org/10.48550/arxiv.2204.05419, doi:10.48550/arxiv.2204.05419.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "jain2022canselfsupervisedlearning",
    "bibtex": "@article{jain2022canselfsupervisedlearning,\n    author = \"Jain, Rishabh and Yiwere, Mariam and Bigioi, Dan and Corcoran, Peter\",\n    title = \"Can Self-Supervised Learning solve the problem of child speech recognition?\",\n    year = \"2022\",\n    journal = \"ArXiv\",\n    volume = \"abs/2204.05419\",\n    doi = \"10.48550/arxiv.2204.05419\",\n    url = \"https://doi.org/10.48550/arxiv.2204.05419\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2204.05419",
    "doc_id": "2006e44a826263f7",
    "formatted_citation": "Rishabh Jain, Mariam Yiwere, Dan Bigioi, and Peter Corcoran. Can self-supervised learning solve the problem of child speech recognition? ArXiv, 2022. URL: https://doi.org/10.48550/arxiv.2204.05419, doi:10.48550/arxiv.2204.05419. This article has 1 citations."
  },
  "Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text": {
    "title": "Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text",
    "authors": [
      "Jinpeng Li",
      "Yu Pu",
      "Qi Sun",
      "Wei-Qiang Zhang"
    ],
    "doi": "10.48550/arxiv.2408.05554",
    "url": "https://www.isca-archive.org/interspeech_2024/li24ia_interspeech.html",
    "journal": "ArXiv",
    "year": 2024,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2514-2518",
    "issue": null,
    "volume": "abs/2408.05554",
    "publisher": "ISCA",
    "abstract": "Whisper and other large-scale automatic speech recognition models have made significant progress in performance. However, their performance on many low-resource languages, such as Kazakh, is not satisfactory. It is worth researching how to utilize low-cost data to improve the performance of Whisper on under-represented languages. In this study, we utilized easily accessible unpaired speech and text data and combined the language model GPT with Whisper on Kazakh. We implemented end of transcript (EOT) judgment modification and hallucination penalty to improve the performance of speech recognition. Further, we employed the decoding average token log probability as a criterion to select samples from unlabeled speech data and used pseudo-labeled data to fine-tune the model to further improve its performance. Ultimately, we achieved more than 10% absolute WER reduction in multiple experiments, and the whole process has the potential to be generalized to other underrepresented languages.",
    "publication_date": null,
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "li2024improvingwhispersrecognition",
    "dockey": "cb5167daf6c7e620",
    "citation": "Jinpeng Li, Yu Pu, Qi Sun, and Wei-Qiang Zhang. Improving whisper's recognition performance for under-represented language kazakh leveraging unpaired speech and text. ArXiv, 2024. URL: https://doi.org/10.48550/arxiv.2408.05554, doi:10.48550/arxiv.2408.05554.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "li2024improvingwhispersrecognition",
    "bibtex": "@article{li2024improvingwhispersrecognition,\n    author = \"Li, Jinpeng and Pu, Yu and Sun, Qi and Zhang, Wei-Qiang\",\n    title = \"Improving Whisper's Recognition Performance for Under-Represented Language Kazakh Leveraging Unpaired Speech and Text\",\n    year = \"2024\",\n    journal = \"ArXiv\",\n    volume = \"abs/2408.05554\",\n    doi = \"10.48550/arxiv.2408.05554\",\n    url = \"https://doi.org/10.48550/arxiv.2408.05554\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2408.05554",
    "doc_id": "cb5167daf6c7e620",
    "formatted_citation": "Jinpeng Li, Yu Pu, Qi Sun, and Wei-Qiang Zhang. Improving whisper's recognition performance for under-represented language kazakh leveraging unpaired speech and text. ArXiv, 2024. URL: https://doi.org/10.48550/arxiv.2408.05554, doi:10.48550/arxiv.2408.05554. This article has 2 citations."
  },
  "Multilingual DistilWhisper: Efficient Distillation of Multi-task Speech Models via Language-Specific Experts": {
    "title": "Multilingual Distilwhisper: Efficient Distillation of Multi-Task Speech Models Via Language-Specific Experts",
    "authors": [
      "Thomas Palmeira Ferraz",
      "Marcely Zanon Boito",
      "Caroline Brun",
      "Vassilina Nikoulina"
    ],
    "doi": "10.1109/icassp48485.2024.10447520",
    "url": "https://doi.org/10.1109/icassp48485.2024.10447520",
    "journal": "ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)",
    "year": 2024,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": "10716-10720",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "Whisper is a multitask and multilingual speech model covering 99 languages. It yields commendable automatic speech recognition (ASR) results in a subset of its covered languages, but the model still underperforms on a non-negligible number of under-represented languages, a problem exacerbated in smaller model versions. In this work, we propose DistilWhisper, an approach able to bridge the performance gap in ASR for these languages while retaining the advantages of multitask and multilingual capabilities. Our approach involves two key strategies: lightweight modular ASR fine-tuning of whisper-small using language-specific experts, and knowledge distillation from whisper-large-v2. This dual approach allows us to effectively boost ASR performance while keeping the robustness inherited from the multitask and multilingual pre-training. Results demonstrate that our approach is more effective than standard fine-tuning or LoRA adapters, boosting performance in the targeted languages for both in- and out-of-domain test sets, while introducing only a negligible parameter overhead at inference.",
    "publication_date": "2024-04-14T00:00:00",
    "citation_count": 12,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2311.01070",
    "docname": "ferraz2024multilingualdistilwhisperefficient",
    "dockey": "ef7536b0b8bc96a9",
    "citation": "Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, and Vassilina Nikoulina. Multilingual distilwhisper: efficient distillation of multi-task speech models via language-specific experts. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 10716-10720, Apr 2024. URL: https://doi.org/10.1109/icassp48485.2024.10447520, doi:10.1109/icassp48485.2024.10447520.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "ferraz2024multilingualdistilwhisperefficient",
    "bibtex": "@article{ferraz2024multilingualdistilwhisperefficient,\n    author = \"Ferraz, Thomas Palmeira and Boito, Marcely Zanon and Brun, Caroline and Nikoulina, Vassilina\",\n    title = \"Multilingual Distilwhisper: Efficient Distillation of Multi-Task Speech Models Via Language-Specific Experts\",\n    year = \"2024\",\n    journal = \"ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\",\n    pages = \"10716-10720\",\n    month = \"Apr\",\n    doi = \"10.1109/icassp48485.2024.10447520\",\n    url = \"https://doi.org/10.1109/icassp48485.2024.10447520\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icassp48485.2024.10447520",
    "doc_id": "ef7536b0b8bc96a9",
    "formatted_citation": "Thomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, and Vassilina Nikoulina. Multilingual distilwhisper: efficient distillation of multi-task speech models via language-specific experts. ICASSP 2024 - 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 10716-10720, Apr 2024. URL: https://doi.org/10.1109/icassp48485.2024.10447520, doi:10.1109/icassp48485.2024.10447520. This article has 12 citations."
  },
  "Data Augmentation For Children's Speech Recognition -- The \"Ethiopian\" System For The SLT 2021 Children Speech Recognition Challenge": {
    "title": "Advanced Recurrent Neural Networks for Automatic Speech Recognition",
    "authors": [
      "None None",
      "None None",
      "None None"
    ],
    "doi": "10.1007/978-3-319-64680-0_11",
    "url": "https://doi.org/10.1007/978-3-319-64680-0_11",
    "journal": "New Era for Robust Speech Recognition",
    "year": 2017,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing"
    ],
    "pages": "261-279",
    "issue": null,
    "volume": null,
    "publisher": "Springer International Publishing",
    "abstract": "This paper presents the “Ethiopian” system for the SLT 2021 Children Speech Recognition Challenge. Various data processing and augmentation techniques are proposed to tackle children’s speech recognition problem, especially the lack of the children’s speech recognition training data issue. Detailed experiments are designed and conducted to show the effectiveness of each technique, across different speech recognition toolkits and model architectures. Step by step, we explain how we come up with our ﬁnal system, which provides the state-of-the-art results in the SLT 2021 Children Speech Recognition Challenge, with 21.66% CER on the Track 1 evaluation set (4th place overall), and 16.53% CER on the Track 2 evaluation set (1st place overall). Post-challenge analysis shows that our system actually achieves 18.82% CER on the Track 1 evaluation set, but we submitted the wrong version to the challenge organizer for Track 1.",
    "publication_date": "2017-01-01T00:00:00",
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2011.04547",
    "docname": "none2017advancedrecurrentneural",
    "dockey": "a4cadaa04bca6605",
    "citation": "None None, None None, and None None. Advanced recurrent neural networks for automatic speech recognition. New Era for Robust Speech Recognition, pages 261-279, Jan 2017. URL: https://doi.org/10.1007/978-3-319-64680-0\\_11, doi:10.1007/978-3-319-64680-0\\_11.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "none2017advancedrecurrentneural",
    "bibtex": "@article{none2017advancedrecurrentneural,\n    author = \"None, None and None, None and None, None\",\n    title = \"Advanced Recurrent Neural Networks for Automatic Speech Recognition\",\n    year = \"2017\",\n    journal = \"New Era for Robust Speech Recognition\",\n    pages = \"261-279\",\n    month = \"Jan\",\n    doi = \"10.1007/978-3-319-64680-0\\_11\",\n    url = \"https://doi.org/10.1007/978-3-319-64680-0\\_11\",\n    publisher = \"Springer International Publishing\"\n}\n",
    "bibtex_type": "inbook",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1007/978-3-319-64680-0_11",
    "doc_id": "a4cadaa04bca6605",
    "formatted_citation": "None None, None None, and None None. Advanced recurrent neural networks for automatic speech recognition. New Era for Robust Speech Recognition, pages 261-279, Jan 2017. URL: https://doi.org/10.1007/978-3-319-64680-0\\_11, doi:10.1007/978-3-319-64680-0\\_11. This article has 2 citations."
  },
  "Language development in mono- and multilingual children: A longitudinal approach": {
    "title": "Language development in mono- and multilingual children: A longitudinal approach",
    "authors": [
      "Marina Trebbels",
      "J. Duarte"
    ],
    "doi": "10.3224/diskurs.v9i3.16624",
    "url": "https://www.academia.edu/80730786/Language_development_in_mono_and_multilingual_children_A_longitudinal_approach",
    "journal": "Journal of Childhood and Adolescence Research",
    "year": 2014,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "303-318",
    "issue": null,
    "volume": "9",
    "publisher": null,
    "abstract": "Both theoretical considerations and empirical results have emphasized the need for longitudinal data in order to gain a more fine-grained insight into the processes of language acquisition and development. Based on data from a two-wave study on the",
    "publication_date": null,
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "trebbels2014languagedevelopmentin",
    "dockey": "cb77f2b1c193e81b",
    "citation": "Marina Trebbels and J. Duarte. Language development in mono- and multilingual children: a longitudinal approach. Journal of Childhood and Adolescence Research, 9:303-318, 2014. URL: https://doi.org/10.3224/diskurs.v9i3.16624, doi:10.3224/diskurs.v9i3.16624.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "trebbels2014languagedevelopmentin",
    "bibtex": "@article{trebbels2014languagedevelopmentin,\n    author = \"Trebbels, Marina and Duarte, J.\",\n    title = \"Language development in mono- and multilingual children: A longitudinal approach\",\n    year = \"2014\",\n    journal = \"Journal of Childhood and Adolescence Research\",\n    volume = \"9\",\n    pages = \"303-318\",\n    doi = \"10.3224/diskurs.v9i3.16624\",\n    url = \"https://doi.org/10.3224/diskurs.v9i3.16624\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.3224/diskurs.v9i3.16624",
    "doc_id": "cb77f2b1c193e81b",
    "formatted_citation": "Marina Trebbels and J. Duarte. Language development in mono- and multilingual children: a longitudinal approach. Journal of Childhood and Adolescence Research, 9:303-318, 2014. URL: https://doi.org/10.3224/diskurs.v9i3.16624, doi:10.3224/diskurs.v9i3.16624. This article has 2 citations."
  },
  "Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults": {
    "title": "Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults",
    "authors": [
      "Ahmed Adel Attia",
      "Jing Liu",
      "Wei Ai",
      "Dorottya Demszky",
      "Carol Espy-Wilson"
    ],
    "doi": "10.1609/aies.v7i1.31618",
    "url": "https://doi.org/10.1609/aies.v7i1.31618",
    "journal": "Proceedings of the AAAI/ACM Conference on AI Ethics and Society",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "74-80",
    "issue": "1",
    "volume": "7",
    "publisher": "Association for the Advancement of Artificial Intelligence (AAAI)",
    "abstract": "Recent advancements in Automatic Speech Recognition (ASR) systems, exemplified by Whisper, have demonstrated the potential of these systems to approach human-level performance given sufficient data. However, this progress doesn’t readily extend to ASR for children due to the lim- ited availability of suitable child-specific databases and the distinct characteristics of children’s speech. A recent study investigated leveraging the My Science Tutor (MyST) chil- dren’s speech corpus to enhance Whisper’s performance in recognizing children’s speech. They were able to demon- strate some improvement on a limited testset. This paper builds on these findings by enhancing the utility of the MyST dataset through more efficient data preprocessing. We reduce the Word Error Rate (WER) on the MyST testset 13.93% to 9.11% with Whisper-Small and from 13.23% to 8.61% with Whisper-Medium and show that this improvement can be generalized to unseen datasets. We also highlight important challenges towards improving children’s ASR performance and the effect of fine-tuning in improving the transcription of disfluent speech.",
    "publication_date": "2024-10-16T00:00:00",
    "citation_count": 1,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "attia2024kidwhispertowardsbridging",
    "dockey": "07d3b7fcdc77c4c3",
    "citation": "Ahmed Adel Attia, Jing Liu, Wei Ai, Dorottya Demszky, and Carol Espy-Wilson. Kid-whisper: towards bridging the performance gap in automatic speech recognition for children vs. adults. Proceedings of the AAAI/ACM Conference on AI Ethics and Society, 7:74-80, Oct 2024. URL: https://doi.org/10.1609/aies.v7i1.31618, doi:10.1609/aies.v7i1.31618.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "attia2024kidwhispertowardsbridging",
    "bibtex": "@article{attia2024kidwhispertowardsbridging,\n    author = \"Attia, Ahmed Adel and Liu, Jing and Ai, Wei and Demszky, Dorottya and Espy-Wilson, Carol\",\n    title = \"Kid-Whisper: Towards Bridging the Performance Gap in Automatic Speech Recognition for Children VS. Adults\",\n    year = \"2024\",\n    journal = \"Proceedings of the AAAI/ACM Conference on AI Ethics and Society\",\n    volume = \"7\",\n    pages = \"74-80\",\n    month = \"Oct\",\n    doi = \"10.1609/aies.v7i1.31618\",\n    url = \"https://doi.org/10.1609/aies.v7i1.31618\",\n    publisher = \"Association for the Advancement of Artificial Intelligence (AAAI)\",\n    issn = \"3065-8365\"\n}\n",
    "issn": "3065-8365",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1609/aies.v7i1.31618",
    "doc_id": "07d3b7fcdc77c4c3",
    "formatted_citation": "Ahmed Adel Attia, Jing Liu, Wei Ai, Dorottya Demszky, and Carol Espy-Wilson. Kid-whisper: towards bridging the performance gap in automatic speech recognition for children vs. adults. Proceedings of the AAAI/ACM Conference on AI Ethics and Society, 7:74-80, Oct 2024. URL: https://doi.org/10.1609/aies.v7i1.31618, doi:10.1609/aies.v7i1.31618. This article has 1 citations."
  },
  "ASR corpus design for resource-scarce languages": {
    "title": "ASR corpus design for resource-scarce languages",
    "authors": [
      "E. Barnard",
      "Marelie Hattingh Davel",
      "C. V. Heerden"
    ],
    "doi": "10.21437/interspeech.2009-727",
    "url": "https://www.isca-archive.org/interspeech_2009/barnard09_interspeech.html",
    "journal": "Interspeech 2009",
    "year": 2009,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2847-2850",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "We investigate the number of speakers and the amount of data that is required for the development of useable speakerindependent speech-recognition systems in resource-scarce languages. Our experiments employ the Lwazi corpus, which contains speech in the eleven ofﬁcial languages of South Africa. We ﬁnd that a surprisingly small number of speakers (fewer than 50) and around 10 to 20 hours of speech per language are sufﬁcient for the purposes of acceptable phone-based recognition.",
    "publication_date": "2009-09-06T00:00:00",
    "citation_count": 68,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "barnard2009asrcorpusdesign",
    "dockey": "40d21eeb6f209e73",
    "citation": "E. Barnard, Marelie Hattingh Davel, and C. V. Heerden. Asr corpus design for resource-scarce languages. Interspeech 2009, pages 2847-2850, Sep 2009. URL: https://doi.org/10.21437/interspeech.2009-727, doi:10.21437/interspeech.2009-727.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "barnard2009asrcorpusdesign",
    "bibtex": "@article{barnard2009asrcorpusdesign,\n    author = \"Barnard, E. and Davel, Marelie Hattingh and Heerden, C. V.\",\n    title = \"ASR corpus design for resource-scarce languages\",\n    year = \"2009\",\n    journal = \"Interspeech 2009\",\n    pages = \"2847-2850\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2009-727\",\n    url = \"https://doi.org/10.21437/interspeech.2009-727\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2009-727",
    "doc_id": "40d21eeb6f209e73",
    "formatted_citation": "E. Barnard, Marelie Hattingh Davel, and C. V. Heerden. Asr corpus design for resource-scarce languages. Interspeech 2009, pages 2847-2850, Sep 2009. URL: https://doi.org/10.21437/interspeech.2009-727, doi:10.21437/interspeech.2009-727. This article has 68 citations."
  },
  "(PDF) Towards inclusive automatic speech recognition": {
    "title": "(PDF) Towards inclusive automatic speech recognition",
    "authors": null,
    "doi": null,
    "url": "https://www.researchgate.net/publication/374063854_Towards_inclusive_automatic_speech_recognition",
    "journal": "ResearchGate",
    "year": 2021,
    "genre": "webpage",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": "ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.",
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  },
  "Towards inclusive automatic speech recognition": {
    "title": "Towards inclusive automatic speech recognition",
    "authors": [
      "Siyuan Feng",
      "Bence Mark Halpern",
      "Olya Kudina",
      "Odette Scharenborg"
    ],
    "doi": "10.1016/j.csl.2023.101567",
    "url": "https://doi.org/10.1016/j.csl.2023.101567",
    "journal": "Computer Speech &amp; Language",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Accent",
      "Age",
      "Bias",
      "Gender",
      "Inclusive automatic speech recognition"
    ],
    "pages": "101567",
    "issue": null,
    "volume": "84",
    "publisher": "Elsevier BV",
    "abstract": "Practice and recent evidence show that state-of-the-art (SotA) automatic speech recognition (ASR) systems do not perform equally well for all speaker groups. Many factors can cause this bias against different speaker groups. This paper, for the first time, systematically quantifies and finds speech recognition bias against gender, age, regional accents and non-native accents, and investigates the origin of this bias by investigating bias cross-lingually (i.e., Dutch and Mandarin) and for two different SotA ASR architectures (a hybrid DNN-HMM and an attention based end-to-end (E2E) model) through a phoneme error analysis. The results show that only a fraction of the bias can be explained by pronunciation differences between speaker groups, and that in order to mitigate bias, language- and architecture specific solutions need to be found.",
    "publication_date": "2024-03-01T00:00:00",
    "citation_count": 39,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "feng2024towardsinclusiveautomatic",
    "dockey": "40efcbdf8d65924b",
    "citation": "Siyuan Feng, Bence Mark Halpern, Olya Kudina, and Odette Scharenborg. Towards inclusive automatic speech recognition. Computer Speech &amp; Language, 84:101567, Mar 2024. URL: https://doi.org/10.1016/j.csl.2023.101567, doi:10.1016/j.csl.2023.101567.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "feng2024towardsinclusiveautomatic",
    "bibtex": "@article{feng2024towardsinclusiveautomatic,\n    author = \"Feng, Siyuan and Halpern, Bence Mark and Kudina, Olya and Scharenborg, Odette\",\n    title = \"Towards inclusive automatic speech recognition\",\n    year = \"2024\",\n    journal = \"Computer Speech \\&amp; Language\",\n    volume = \"84\",\n    pages = \"101567\",\n    month = \"Mar\",\n    doi = \"10.1016/j.csl.2023.101567\",\n    url = \"https://doi.org/10.1016/j.csl.2023.101567\",\n    publisher = \"Elsevier BV\",\n    issn = \"0885-2308\"\n}\n",
    "issn": "0885-2308",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1016/j.csl.2023.101567",
    "doc_id": "40efcbdf8d65924b",
    "formatted_citation": "Siyuan Feng, Bence Mark Halpern, Olya Kudina, and Odette Scharenborg. Towards inclusive automatic speech recognition. Computer Speech &amp; Language, 84:101567, Mar 2024. URL: https://doi.org/10.1016/j.csl.2023.101567, doi:10.1016/j.csl.2023.101567. This article has 39 citations."
  },
  "Improving on the Limitations of the ASR Model in Low-Resourced Environments Using Parameter-Efficient Fine-Tuning": {
    "title": "Exploring Features for Membership Inference in Asr Model Auditing",
    "authors": [
      "Francisco Teixeira",
      "Karla Pizzi",
      "Raphael Olivier",
      "Alberto Abad",
      "Bhiksha Raj",
      "Isabel Trancoso"
    ],
    "doi": "10.2139/ssrn.4937232",
    "url": "https://doi.org/10.2139/ssrn.4937232",
    "journal": "Proceedings of the 21st International Conference on Natural Language Processing (ICON)",
    "year": 2024,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "408–415",
    "issue": null,
    "volume": null,
    "publisher": "Elsevier BV",
    "abstract": "Modern general-purpose speech recognition systems are more robust in languages with high resources. In contrast, achieving state-of-the-art accuracy for low-resource languages is still challenging. The fine-tuning of the pre-trained model is one of the highly popular practices which utilizes the existing information while efficiently learning from a small amount of data to enhance the precision and robustness of speech recognition tasks. This work attempts to diagnose the performance of a pre-trained model when transcribing the audio from the low-resource language. In this work, we apply an adapter-based iterative parameter-efficient fine-tuning strategy on a limited dataset aiming to improve the quality of the transcription of a previously fine-tuned model. For the experiment we used Whisper's multilingual pre-trained speech model and Nepali as a test language. Using this approach we achieved Word Error Rate of 27.9%,which is more than 19% improvement over pre-trained Whisper Large \\ensuremath- V2.",
    "publication_date": "2024-01-01T00:00:00",
    "citation_count": 1,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null,
    "docname": "teixeira2024exploringfeaturesfor",
    "dockey": "b9c7455a85c454fe",
    "citation": "Francisco Teixeira, Karla Pizzi, Raphael Olivier, Alberto Abad, Bhiksha Raj, and Isabel Trancoso. Exploring features for membership inference in asr model auditing. Unknown journal, Jan 2024. URL: https://doi.org/10.2139/ssrn.4937232, doi:10.2139/ssrn.4937232.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "teixeira2024exploringfeaturesfor",
    "bibtex": "@article{teixeira2024exploringfeaturesfor,\n    author = \"Teixeira, Francisco and Pizzi, Karla and Olivier, Raphael and Abad, Alberto and Raj, Bhiksha and Trancoso, Isabel\",\n    title = \"Exploring Features for Membership Inference in Asr Model Auditing\",\n    year = \"2024\",\n    journal = \"Unknown journal\",\n    month = \"Jan\",\n    doi = \"10.2139/ssrn.4937232\",\n    url = \"https://doi.org/10.2139/ssrn.4937232\",\n    publisher = \"Elsevier BV\"\n}\n",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.2139/ssrn.4937232",
    "doc_id": "b9c7455a85c454fe",
    "formatted_citation": "Francisco Teixeira, Karla Pizzi, Raphael Olivier, Alberto Abad, Bhiksha Raj, and Isabel Trancoso. Exploring features for membership inference in asr model auditing. Unknown journal, Jan 2024. URL: https://doi.org/10.2139/ssrn.4937232, doi:10.2139/ssrn.4937232."
  },
  "Mixed Children/Adult/Childrenized Fine-Tuning for Children’s ASR: How to Reduce Age Mismatch and Speaking Style Mismatch": {
    "title": "Mixed Children/Adult/Childrenized Fine-Tuning for Children’s ASR: How to Reduce Age Mismatch and Speaking Style Mismatch",
    "authors": [
      "Thomas Graave",
      "Zhengyang Li",
      "Timo Lohrenz",
      "Tim Fingscheidt"
    ],
    "doi": "10.21437/interspeech.2024-499",
    "url": "https://www.isca-archive.org/interspeech_2024/graave24_interspeech.html",
    "journal": "Interspeech 2024",
    "year": 2024,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "5188-5192",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Today’s end-to-end (E2E) ASR models achieve strong performance when applied to adult speech, but deteriorate on children’s speech. Most E2E ASR models are pre-trained on adult speech, which introduces an age mismatch that can be addressed by finetuning on child data. However, due to limited availability of child datasets, fine-tuning on children’s speech may introduce new domain shifts such as speaking style mismatch. In this work, we explore mixed fine-tuning on partially matched data, namely read adult speech and spontaneous children’s speech, to improve the performance of E2E ASR on read children’s speech. We isolate the individual impact of age mismatch and speaking style mismatch and investigate the use of childrenization of read adult speech. Our proposed method reduces the WER by up to 5% absolute (21% relative) compared to the pre-trained E2E ASR and by roughly 3% absolute (15% relative) compared to individual fine-tuning on partially matched datasets.",
    "publication_date": "2024-09-01T00:00:00",
    "citation_count": 2,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "graave2024mixedchildrenadultchildrenizedfinetuning",
    "dockey": "e5c366120362418d",
    "citation": "Thomas Graave, Zhengyang Li, Timo Lohrenz, and Tim Fingscheidt. Mixed children/adult/childrenized fine-tuning for children’s asr: how to reduce age mismatch and speaking style mismatch. Interspeech 2024, Sep 2024. URL: https://doi.org/10.21437/interspeech.2024-499, doi:10.21437/interspeech.2024-499.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "graave2024mixedchildrenadultchildrenizedfinetuning",
    "bibtex": "@article{graave2024mixedchildrenadultchildrenizedfinetuning,\n    author = \"Graave, Thomas and Li, Zhengyang and Lohrenz, Timo and Fingscheidt, Tim\",\n    title = \"Mixed Children/Adult/Childrenized Fine-Tuning for Children’s ASR: How to Reduce Age Mismatch and Speaking Style Mismatch\",\n    year = \"2024\",\n    journal = \"Interspeech 2024\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2024-499\",\n    url = \"https://doi.org/10.21437/interspeech.2024-499\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2024-499",
    "doc_id": "e5c366120362418d",
    "formatted_citation": "Thomas Graave, Zhengyang Li, Timo Lohrenz, and Tim Fingscheidt. Mixed children/adult/childrenized fine-tuning for children’s asr: how to reduce age mismatch and speaking style mismatch. Interspeech 2024, Sep 2024. URL: https://doi.org/10.21437/interspeech.2024-499, doi:10.21437/interspeech.2024-499. This article has 2 citations."
  },
  "Cross-Language Transfer Learning, Continuous Learning, and Domain Adaptation for End-to-End Automatic Speech Recognition": {
    "title": "Cross-Language Transfer Learning and Domain Adaptation for End-to-End Automatic Speech Recognition",
    "authors": [
      "Jian Luo",
      "Jianzong Wang",
      "Ning Cheng",
      "Edward Xiao",
      "Jing Xiao",
      "G. Kucsko",
      "Patrick K. O’Neill",
      "Jagadeesh Balam",
      "Slyne Deng",
      "Adriana B. Flores",
      "Boris Ginsburg",
      "Jocelyn Huang",
      "Oleksii Kuchaiev",
      "Vitaly Lavrukhin",
      "Jason Li"
    ],
    "doi": "10.1109/icme51207.2021.9428334",
    "url": "https://doi.org/10.1109/icme51207.2021.9428334",
    "journal": "2021 IEEE International Conference on Multimedia and Expo (ICME)",
    "year": 2021,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Electrical Engineering and Systems Science - Audio and Speech Processing"
    ],
    "pages": "1-6",
    "issue": null,
    "volume": null,
    "publisher": "IEEE",
    "abstract": "In this paper, we demonstrate the efficacy of transfer learning and continuous learning for various automatic speech recognition (ASR) tasks. We start with a pre-trained English ASR model and show that transfer learning can be effectively and easily performed on: (1) different English accents, (2) different languages (German, Spanish and Russian) and (3) application-specific domains. Our experiments demonstrate that in all three cases, transfer learning from a good base model has higher accuracy than a model trained from scratch. It is preferred to fine-tune large models than small pre-trained models, even if the dataset for fine-tuning is small. Moreover, transfer learning significantly speeds up convergence for both very small and very large target datasets.",
    "publication_date": "2021-07-05T00:00:00",
    "citation_count": 17,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2005.04290",
    "docname": "luo2021crosslanguagetransferlearning",
    "dockey": "f066db1d1f593640",
    "citation": "Jian Luo, Jianzong Wang, Ning Cheng, Edward Xiao, Jing Xiao, G. Kucsko, Patrick K. O’Neill, Jagadeesh Balam, Slyne Deng, Adriana B. Flores, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, and Jason Li. Cross-language transfer learning and domain adaptation for end-to-end automatic speech recognition. 2021 IEEE International Conference on Multimedia and Expo (ICME), pages 1-6, Jul 2021. URL: https://doi.org/10.1109/icme51207.2021.9428334, doi:10.1109/icme51207.2021.9428334.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "luo2021crosslanguagetransferlearning",
    "bibtex": "@article{luo2021crosslanguagetransferlearning,\n    author = \"Luo, Jian and Wang, Jianzong and Cheng, Ning and Xiao, Edward and Xiao, Jing and Kucsko, G. and O’Neill, Patrick K. and Balam, Jagadeesh and Deng, Slyne and Flores, Adriana B. and Ginsburg, Boris and Huang, Jocelyn and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Li, Jason\",\n    title = \"Cross-Language Transfer Learning and Domain Adaptation for End-to-End Automatic Speech Recognition\",\n    year = \"2021\",\n    journal = \"2021 IEEE International Conference on Multimedia and Expo (ICME)\",\n    pages = \"1-6\",\n    month = \"Jul\",\n    doi = \"10.1109/icme51207.2021.9428334\",\n    url = \"https://doi.org/10.1109/icme51207.2021.9428334\",\n    publisher = \"IEEE\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/icme51207.2021.9428334",
    "doc_id": "f066db1d1f593640",
    "formatted_citation": "Jian Luo, Jianzong Wang, Ning Cheng, Edward Xiao, Jing Xiao, G. Kucsko, Patrick K. O’Neill, Jagadeesh Balam, Slyne Deng, Adriana B. Flores, Boris Ginsburg, Jocelyn Huang, Oleksii Kuchaiev, Vitaly Lavrukhin, and Jason Li. Cross-language transfer learning and domain adaptation for end-to-end automatic speech recognition. 2021 IEEE International Conference on Multimedia and Expo (ICME), pages 1-6, Jul 2021. URL: https://doi.org/10.1109/icme51207.2021.9428334, doi:10.1109/icme51207.2021.9428334. This article has 17 citations."
  },
  "Adaptation of Whisper models to child speech recognition": {
    "title": "Adaptation of Whisper models to child speech recognition",
    "authors": [
      "None None",
      "None None",
      "None None",
      "None None",
      "None None"
    ],
    "doi": "10.21437/interspeech.2023-935",
    "url": "https://www.isca-archive.org/interspeech_2023/jain23_interspeech.html",
    "journal": "INTERSPEECH 2023",
    "year": 2023,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "5242-5246",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Automatic Speech Recognition (ASR) systems often struggle with transcribing child speech due to the lack of large child speech datasets required to accurately train child-friendly ASR models. However, there are huge amounts of annotated adult speech datasets which were used to create multilingual ASR models, such as Whisper. Our work aims to explore whether such models can be adapted to child speech to improve ASR for children. In addition, we compare Whisper child-adaptations with finetuned self-supervised models, such as wav2vec2. We demonstrate that finetuning Whisper on child speech yields significant improvements in ASR performance on child speech, compared to non-finetuned Whisper models. Additionally, utilizing self-supervised Wav2vec2 models that have been finetuned on child speech outperforms Whisper finetuning.",
    "publication_date": "2023-08-14T00:00:00",
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "none2023adaptationofwhisper",
    "dockey": "ac12cbef82889e75",
    "citation": "None None, None None, None None, None None, and None None. Adaptation of whisper models to child speech recognition. INTERSPEECH 2023, Aug 2023. URL: https://doi.org/10.21437/interspeech.2023-935, doi:10.21437/interspeech.2023-935.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "none2023adaptationofwhisper",
    "bibtex": "@article{none2023adaptationofwhisper,\n    author = \"None, None and None, None and None, None and None, None and None, None\",\n    title = \"Adaptation of Whisper models to child speech recognition\",\n    year = \"2023\",\n    journal = \"INTERSPEECH 2023\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2023-935\",\n    url = \"https://doi.org/10.21437/interspeech.2023-935\",\n    publisher = \"ISCA\"\n}\n",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2023-935",
    "doc_id": "ac12cbef82889e75",
    "formatted_citation": "None None, None None, None None, None None, and None None. Adaptation of whisper models to child speech recognition. INTERSPEECH 2023, Aug 2023. URL: https://doi.org/10.21437/interspeech.2023-935, doi:10.21437/interspeech.2023-935."
  },
  "Analysis of children's speech: duration, pitch and formants": {
    "title": "Analysis of children's speech: duration, pitch and formants",
    "authors": [
      "Sungbok Lee",
      "A. Potamianos",
      "Shrikanth S. Narayanan"
    ],
    "doi": "10.21437/eurospeech.1997-161",
    "url": "https://www.isca-archive.org/eurospeech_1997/lee97b_eurospeech.html",
    "journal": "5th European Conference on Speech Communication and Technology (Eurospeech 1997)",
    "year": 1997,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "473-476",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": null,
    "publication_date": "1997-09-22T00:00:00",
    "citation_count": 77,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "lee1997analysisofchildrens",
    "dockey": "86ed46232a4a45c4",
    "citation": "Sungbok Lee, A. Potamianos, and Shrikanth S. Narayanan. Analysis of children's speech: duration, pitch and formants. 5th European Conference on Speech Communication and Technology (Eurospeech 1997), pages 473-476, Sep 1997. URL: https://doi.org/10.21437/eurospeech.1997-161, doi:10.21437/eurospeech.1997-161.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "lee1997analysisofchildrens",
    "bibtex": "@article{lee1997analysisofchildrens,\n    author = \"Lee, Sungbok and Potamianos, A. and Narayanan, Shrikanth S.\",\n    title = \"Analysis of children's speech: duration, pitch and formants\",\n    year = \"1997\",\n    journal = \"5th European Conference on Speech Communication and Technology (Eurospeech 1997)\",\n    pages = \"473-476\",\n    month = \"Sep\",\n    doi = \"10.21437/eurospeech.1997-161\",\n    url = \"https://doi.org/10.21437/eurospeech.1997-161\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/eurospeech.1997-161",
    "doc_id": "86ed46232a4a45c4",
    "formatted_citation": "Sungbok Lee, A. Potamianos, and Shrikanth S. Narayanan. Analysis of children's speech: duration, pitch and formants. 5th European Conference on Speech Communication and Technology (Eurospeech 1997), pages 473-476, Sep 1997. URL: https://doi.org/10.21437/eurospeech.1997-161, doi:10.21437/eurospeech.1997-161. This article has 77 citations."
  },
  "Acoustics of children’s speech: Developmental changes of temporal and spectral parametersa)": {
    "title": "Acoustics of children’s speech: Developmental changes of temporal and spectral parametersa)",
    "authors": [
      "Lee, Sungbok",
      "Potamianos, Alexandros",
      "Narayanan, Shrikanth"
    ],
    "doi": null,
    "url": null,
    "journal": null,
    "year": null,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": null,
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  },
  "LRSpeech: Extremely Low-Resource Speech Synthesis and Recognition": {
    "title": "LRSpeech",
    "authors": [
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None",
      "None None"
    ],
    "doi": "10.1145/3394486.3403331",
    "url": "https://doi.org/10.1145/3394486.3403331",
    "journal": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining",
    "year": 2020,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2802-2812",
    "issue": null,
    "volume": null,
    "publisher": "ACM",
    "abstract": "Speech synthesis (text to speech, TTS) and recognition (automatic speech recognition, ASR) are important speech tasks, and require a large amount of text and speech pairs for model training. However, there are more than 6,000 languages in the world and most languages are lack of speech training data, which poses significant challenges when building TTS and ASR systems for extremely lowresource languages. In this paper, we develop LRSpeech, a TTS and ASR system under the extremely low-resource setting, which can support rare languages with low data cost. LRSpeech consists of three key techniques: 1) pre-training on rich-resource languages and fine-tuning on low-resource languages; 2) dual transformation between TTS and ASR to iteratively boost the accuracy of each other; 3) knowledge distillation to customize the TTS model on a high-quality target-speaker voice and improve the ASR model on multiple voices. We conduct experiments on an experimental language (English) and a truly low-resource language (Lithuanian) to verify the effectiveness of LRSpeech. Experimental results show that LRSpeech 1) achieves high quality for TTS in terms of both intelligibility (more than 98% intelligibility rate) and naturalness (above 3.5 mean opinion score (MOS)) of the synthesized speech, which satisfy the requirements for industrial deployment, 2) achieves promising recognition accuracy for ASR, and 3) last but not least, uses extremely low-resource training data. We also conduct comprehensive analyses on LRSpeech with different amounts of data resources, and provide valuable insights and guidances for industrial deployment. We are currently deploying LRSpeech into a commercialized cloud speech service to support TTS on more rare languages.",
    "publication_date": "2020-08-20T00:00:00",
    "citation_count": 90,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "none2020lrspeech",
    "dockey": "2f16f57b358606c2",
    "citation": "None None, None None, None None, None None, None None, None None, and None None. Lrspeech. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, Aug 2020. URL: https://doi.org/10.1145/3394486.3403331, doi:10.1145/3394486.3403331.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "none2020lrspeech",
    "bibtex": "@article{none2020lrspeech,\n    author = \"None, None and None, None and None, None and None, None and None, None and None, None and None, None\",\n    title = \"LRSpeech\",\n    year = \"2020\",\n    journal = \"Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\&amp; Data Mining\",\n    month = \"Aug\",\n    doi = \"10.1145/3394486.3403331\",\n    url = \"https://doi.org/10.1145/3394486.3403331\",\n    publisher = \"ACM\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1145/3394486.3403331",
    "doc_id": "2f16f57b358606c2",
    "formatted_citation": "None None, None None, None None, None None, None None, None None, and None None. Lrspeech. Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining, Aug 2020. URL: https://doi.org/10.1145/3394486.3403331, doi:10.1145/3394486.3403331. This article has 90 citations."
  },
  "Exploration of Whisper fine-tuning strategies for low-resource ASR": {
    "title": "Exploration of Whisper fine-tuning strategies for low-resource ASR",
    "authors": [
      "Yunpeng Liu",
      "Xukui Yang",
      "Dan Qu"
    ],
    "doi": "10.1186/s13636-024-00349-3",
    "url": "https://doi.org/10.1186/s13636-024-00349-3",
    "journal": "EURASIP Journal on Audio, Speech, and Music Processing",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "ASR",
      "Fine-tune",
      "Low-resource",
      "Whisper"
    ],
    "pages": "29",
    "issue": "1",
    "volume": "2024",
    "publisher": "Springer Science and Business Media LLC",
    "abstract": "Limited data availability remains a significant challenge for Whisper’s low-resource speech recognition performance, falling short of practical application requirements. While previous studies have successfully reduced the recognition error rates of target language speech through fine-tuning, a comprehensive exploration and analysis of Whisper’s fine-tuning capabilities and the advantages and disadvantages of various fine-tuning strategies are still lacking. This paper aims to fill this gap by conducting comprehensive experimental exploration for Whisper’s low-resource speech recognition performance using five fine-tuning strategies with limited supervised data from seven low-resource languages. The results and analysis demonstrate that all fine-tuning strategies explored in this paper significantly enhance Whisper’s performance. However, different strategies vary in their suitability and practical effectiveness, highlighting the need for careful selection based on specific use cases and resources available.",
    "publication_date": "2024-06-01T00:00:00",
    "citation_count": 12,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "liu2024explorationofwhisper",
    "dockey": "46aa76322d78f629",
    "citation": "Yunpeng Liu, Xukui Yang, and Dan Qu. Exploration of whisper fine-tuning strategies for low-resource asr. EURASIP Journal on Audio, Speech, and Music Processing, 2024:29, Jun 2024. URL: https://doi.org/10.1186/s13636-024-00349-3, doi:10.1186/s13636-024-00349-3.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "liu2024explorationofwhisper",
    "bibtex": "@article{liu2024explorationofwhisper,\n    author = \"Liu, Yunpeng and Yang, Xukui and Qu, Dan\",\n    title = \"Exploration of Whisper fine-tuning strategies for low-resource ASR\",\n    year = \"2024\",\n    journal = \"EURASIP Journal on Audio, Speech, and Music Processing\",\n    volume = \"2024\",\n    pages = \"29\",\n    month = \"Jun\",\n    doi = \"10.1186/s13636-024-00349-3\",\n    url = \"https://doi.org/10.1186/s13636-024-00349-3\",\n    publisher = \"Springer Science and Business Media LLC\",\n    issue = \"1\",\n    issn = \"1687-4722\"\n}\n",
    "issn": "1687-4722",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1186/s13636-024-00349-3",
    "doc_id": "46aa76322d78f629",
    "formatted_citation": "Yunpeng Liu, Xukui Yang, and Dan Qu. Exploration of whisper fine-tuning strategies for low-resource asr. EURASIP Journal on Audio, Speech, and Music Processing, 2024:29, Jun 2024. URL: https://doi.org/10.1186/s13636-024-00349-3, doi:10.1186/s13636-024-00349-3. This article has 12 citations and is from a peer-reviewed journal."
  },
  "Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations": {
    "title": "Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations",
    "authors": [
      "Renée Lu",
      "M. Shahin",
      "Beena Ahmed"
    ],
    "doi": "10.48550/arxiv.2211.07769",
    "url": "http://arxiv.org/abs/2211.07769",
    "journal": "ArXiv",
    "year": 2022,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": null,
    "issue": null,
    "volume": "abs/2211.07769",
    "publisher": "arXiv",
    "abstract": "Children's speech recognition is a vital, yet largely overlooked domain when building inclusive speech technologies. The major challenge impeding progress in this domain is the lack of adequate child speech corpora; however, recent advances in self-supervised learning have created a new opportunity for overcoming this problem of data scarcity. In this paper, we leverage self-supervised adult speech representations and use three well-known child speech corpora to build models for children's speech recognition. We assess the performance of fine-tuning on both native and non-native children's speech, examine the effect of cross-domain child corpora, and investigate the minimum amount of child speech required to fine-tune a model which outperforms a state-of-the-art adult model. We also analyze speech recognition performance across children's ages. Our results demonstrate that fine-tuning with cross-domain child corpora leads to relative improvements of up to 46.08% and 45.53% for native and non-native child speech respectively, and absolute improvements of 14.70% and 31.10%. We also show that with as little as 5 hours of transcribed children's speech, it is possible to fine-tune a children's speech recognition system that outperforms a state-of-the-art adult model fine-tuned on 960 hours of adult speech.",
    "publication_date": null,
    "citation_count": 4,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2211.07769",
    "docname": "lu2022improvingchildrensspeech",
    "dockey": "a4ad69071ddb14b2",
    "citation": "Renée Lu, M. Shahin, and Beena Ahmed. Improving children's speech recognition by fine-tuning self-supervised adult speech representations. ArXiv, 2022. URL: https://doi.org/10.48550/arxiv.2211.07769, doi:10.48550/arxiv.2211.07769.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "lu2022improvingchildrensspeech",
    "bibtex": "@article{lu2022improvingchildrensspeech,\n    author = \"Lu, Renée and Shahin, M. and Ahmed, Beena\",\n    title = \"Improving Children's Speech Recognition by Fine-tuning Self-supervised Adult Speech Representations\",\n    year = \"2022\",\n    journal = \"ArXiv\",\n    volume = \"abs/2211.07769\",\n    doi = \"10.48550/arxiv.2211.07769\",\n    url = \"https://doi.org/10.48550/arxiv.2211.07769\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2211.07769",
    "doc_id": "a4ad69071ddb14b2",
    "formatted_citation": "Renée Lu, M. Shahin, and Beena Ahmed. Improving children's speech recognition by fine-tuning self-supervised adult speech representations. ArXiv, 2022. URL: https://doi.org/10.48550/arxiv.2211.07769, doi:10.48550/arxiv.2211.07769. This article has 4 citations."
  },
  "Causal Analysis of Asr Errors for Children: Quantifying the Impact of Physiological, Cognitive, and Extrinsic Factors": {
    "title": "Causal Analysis of Asr Errors for Children: Quantifying the Impact of Physiological, Cognitive, and Extrinsic Factors",
    "authors": [
      "Vishwanath Pratap Singh",
      "Md. Sahidullah",
      "Tomi Kinnunen"
    ],
    "doi": "10.2139/ssrn.5125557",
    "url": "https://doi.org/10.2139/ssrn.5125557",
    "journal": null,
    "year": 2025,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Causal Inference",
      "Children's ASR",
      "Cognition",
      "Physiology",
      "Pronunciation",
      "Speech Foundational Models"
    ],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": "Elsevier BV",
    "abstract": "The increasing use of children’s automatic speech recognition (ASR) systems has spurred research efforts to improve the accuracy of models designed for children’s speech in recent years. The current approach utilizes either open-source speech foundation models (SFMs) directly or fine-tuning them with children’s speech data. These SFMs, whether open-source or fine-tuned for children, often exhibit higher word error rates (WERs) compared to adult speech. However, there is a lack of systemic analysis of the cause of this degraded performance of SFMs. Understanding and addressing the reasons behind this performance disparity is crucial for improving the accuracy of SFMs for children’s speech. Our study addresses this gap by investigating the causes of accuracy degradation and the primary contributors to WER in children’s speech. In the first part of the study, we conduct a comprehensive benchmarking study on two self-supervised SFMs (Wav2Vec 2.0 and Hubert) and two weakly supervised SFMs (Whisper and MMS) across various age groups on two children speech corpora, establishing the raw data for the causal inference analysis in the second part. In the second part of the study, we analyze the impact of physiological factors (age, gender), cognitive factors (pronunciation ability), and external factors (vocabulary difficulty, background noise, and word count) on SFM accuracy in children’s speech using causal inference. The results indicate that physiology (age) and particular external factor (number of words in audio) have the highest impact on accuracy, followed by background noise and pronunciation ability. Fine-tuning SFMs on children’s speech reduces sensitivity to physiological and cognitive factors, while sensitivity to the number of words in audio persists.",
    "publication_date": "2025-01-01T00:00:00",
    "citation_count": 0,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null,
    "docname": "singh2025causalanalysisof",
    "dockey": "266691510861b093",
    "citation": "Vishwanath Pratap Singh, Md. Sahidullah, and Tomi Kinnunen. Causal analysis of asr errors for children: quantifying the impact of physiological, cognitive, and extrinsic factors. Unknown journal, Jan 2025. URL: https://doi.org/10.2139/ssrn.5125557, doi:10.2139/ssrn.5125557.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "singh2025causalanalysisof",
    "bibtex": "@article{singh2025causalanalysisof,\n    author = \"Singh, Vishwanath Pratap and Sahidullah, Md. and Kinnunen, Tomi\",\n    title = \"Causal Analysis of Asr Errors for Children: Quantifying the Impact of Physiological, Cognitive, and Extrinsic Factors\",\n    year = \"2025\",\n    journal = \"Unknown journal\",\n    month = \"Jan\",\n    doi = \"10.2139/ssrn.5125557\",\n    url = \"https://doi.org/10.2139/ssrn.5125557\",\n    publisher = \"Elsevier BV\"\n}\n",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.2139/ssrn.5125557",
    "doc_id": "266691510861b093",
    "formatted_citation": "Vishwanath Pratap Singh, Md. Sahidullah, and Tomi Kinnunen. Causal analysis of asr errors for children: quantifying the impact of physiological, cognitive, and extrinsic factors. Unknown journal, Jan 2025. URL: https://doi.org/10.2139/ssrn.5125557, doi:10.2139/ssrn.5125557."
  },
  "Selective Attention Merging for low resource tasks: A case study of Child ASR": {
    "title": "Selective Attention Merging for low resource tasks: A case study of Child ASR",
    "authors": [
      "Natarajan Balaji Shankar",
      "Zilai Wang",
      "Eray Eren",
      "Abeer Alwan"
    ],
    "doi": "10.48550/arxiv.2501.08468",
    "url": "http://arxiv.org/abs/2501.08468",
    "journal": "ArXiv",
    "year": 2025,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing",
      "Computer Science - Computation and Language"
    ],
    "pages": null,
    "issue": null,
    "volume": "abs/2501.08468",
    "publisher": "arXiv",
    "abstract": "While Speech Foundation Models (SFMs) excel in various speech tasks, their performance for low-resource tasks such as child Automatic Speech Recognition (ASR) is hampered by limited pretraining data. To address this, we explore different model merging techniques to leverage knowledge from models trained on larger, more diverse speech corpora. This paper also introduces Selective Attention (SA) Merge, a novel method that selectively merges task vectors from attention matrices to enhance SFM performance on low-resource tasks. Experiments on the MyST database show significant reductions in relative word error rate of up to 14%, outperforming existing model merging and data augmentation techniques. By combining data augmentation techniques with SA Merge, we achieve a new state-of-the-art WER of 8.69 on the MyST database for the Whisper-small model, highlighting the potential of SA Merge for improving low-resource ASR.",
    "publication_date": null,
    "citation_count": 1,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2501.08468",
    "docname": "shankar2025selectiveattentionmerging",
    "dockey": "94bf1d0ceba0a3e9",
    "citation": "Natarajan Balaji Shankar, Zilai Wang, Eray Eren, and Abeer Alwan. Selective attention merging for low resource tasks: a case study of child asr. ArXiv, 2025. URL: https://doi.org/10.48550/arxiv.2501.08468, doi:10.48550/arxiv.2501.08468.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shankar2025selectiveattentionmerging",
    "bibtex": "@article{shankar2025selectiveattentionmerging,\n    author = \"Shankar, Natarajan Balaji and Wang, Zilai and Eren, Eray and Alwan, Abeer\",\n    title = \"Selective Attention Merging for low resource tasks: A case study of Child ASR\",\n    year = \"2025\",\n    journal = \"ArXiv\",\n    volume = \"abs/2501.08468\",\n    doi = \"10.48550/arxiv.2501.08468\",\n    url = \"https://doi.org/10.48550/arxiv.2501.08468\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2501.08468",
    "doc_id": "94bf1d0ceba0a3e9",
    "formatted_citation": "Natarajan Balaji Shankar, Zilai Wang, Eray Eren, and Abeer Alwan. Selective attention merging for low resource tasks: a case study of child asr. ArXiv, 2025. URL: https://doi.org/10.48550/arxiv.2501.08468, doi:10.48550/arxiv.2501.08468. This article has 1 citations."
  },
  "Improving End-to-End Models for Children’s Speech Recognition": {
    "title": "Improving End-to-End Models for Children’s Speech Recognition",
    "authors": [
      "Tanvina Patel",
      "Odette Scharenborg"
    ],
    "doi": "10.3390/app14062353",
    "url": "https://doi.org/10.3390/app14062353",
    "journal": "Applied Sciences",
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2353",
    "issue": "6",
    "volume": "14",
    "publisher": "MDPI AG",
    "abstract": "Children’s Speech Recognition (CSR) is a challenging task due to the high variability in children’s speech patterns and limited amount of available annotated children’s speech data. We aim to improve CSR in the often-occurring scenario that no children’s speech data is available for training the Automatic Speech Recognition (ASR) systems. Traditionally, Vocal Tract Length Normalization (VTLN) has been widely used in hybrid ASR systems to address acoustic mismatch and variability in children’s speech when training models on adults’ speech. Meanwhile, End-to-End (E2E) systems often use data augmentation methods to create child-like speech from adults’ speech. For adult speech-trained ASRs, we investigate the effectiveness of augmentation methods; speed perturbations and spectral augmentation, along with VTLN, in an E2E framework for the CSR task, comparing these across Dutch, German, and Mandarin. We applied VTLN at different stages (training/test) of the ASR and conducted age and gender analyses. Our experiments showed highly similar patterns across the languages: Speed Perturbations and Spectral Augmentation yield significant performance improvements, while VTLN provided further improvements while maintaining recognition performance on adults’ speech (depending on when it is applied). Additionally, VTLN showed performance improvement for both male and female speakers and was particularly effective for younger children.",
    "publication_date": "2024-03-11T00:00:00",
    "citation_count": 1,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "patel2024improvingendtoendmodels",
    "dockey": "854dcc22aa7b2d26",
    "citation": "Tanvina Patel and Odette Scharenborg. Improving end-to-end models for children’s speech recognition. Applied Sciences, 14:2353, Mar 2024. URL: https://doi.org/10.3390/app14062353, doi:10.3390/app14062353.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "patel2024improvingendtoendmodels",
    "bibtex": "@article{patel2024improvingendtoendmodels,\n    author = \"Patel, Tanvina and Scharenborg, Odette\",\n    title = \"Improving End-to-End Models for Children’s Speech Recognition\",\n    year = \"2024\",\n    journal = \"Applied Sciences\",\n    volume = \"14\",\n    pages = \"2353\",\n    month = \"Mar\",\n    doi = \"10.3390/app14062353\",\n    url = \"https://doi.org/10.3390/app14062353\",\n    publisher = \"MDPI AG\",\n    issue = \"6\",\n    issn = \"2076-3417\"\n}\n",
    "issn": "2076-3417",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.3390/app14062353",
    "doc_id": "854dcc22aa7b2d26",
    "formatted_citation": "Tanvina Patel and Odette Scharenborg. Improving end-to-end models for children’s speech recognition. Applied Sciences, 14:2353, Mar 2024. URL: https://doi.org/10.3390/app14062353, doi:10.3390/app14062353. This article has 1 citations and is from a peer-reviewed journal."
  },
  "Automatic speech recognition for children": {
    "title": "Automatic speech recognition for children",
    "authors": [
      "A. Potamianos",
      "Shrikanth S. Narayanan",
      "Sungbok Lee"
    ],
    "doi": "10.21437/eurospeech.1997-623",
    "url": "https://www.isca-archive.org/eurospeech_1997/potamianos97b_eurospeech.html",
    "journal": "5th European Conference on Speech Communication and Technology (Eurospeech 1997)",
    "year": 1997,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2371-2374",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "In this paper, the acoustic and linguistic characteristics of children speech are investigated in the context of automatic speech recognition. Acoustic variability is identi ed as a major hurdle in building high performance ASR applications for children. A simple speaker normalization algorithm combining frequency warping and spectral shaping introduced in [5] is shown to reduce acoustic variability and signi cantly improve recognition performance for children speakers (by 25{ 45%). Age-dependent acoustic modeling further reduces word error rate by 10%. Piece-wise linear and phoneme-dependent frequency warping algorithms are proposed for reducing acoustic mismatch between the children and adult acoustic spaces.",
    "publication_date": "1997-09-22T00:00:00",
    "citation_count": 124,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "potamianos1997automaticspeechrecognition",
    "dockey": "3645c4213daf1954",
    "citation": "A. Potamianos, Shrikanth S. Narayanan, and Sungbok Lee. Automatic speech recognition for children. 5th European Conference on Speech Communication and Technology (Eurospeech 1997), pages 2371-2374, Sep 1997. URL: https://doi.org/10.21437/eurospeech.1997-623, doi:10.21437/eurospeech.1997-623.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "potamianos1997automaticspeechrecognition",
    "bibtex": "@article{potamianos1997automaticspeechrecognition,\n    author = \"Potamianos, A. and Narayanan, Shrikanth S. and Lee, Sungbok\",\n    title = \"Automatic speech recognition for children\",\n    year = \"1997\",\n    journal = \"5th European Conference on Speech Communication and Technology (Eurospeech 1997)\",\n    pages = \"2371-2374\",\n    month = \"Sep\",\n    doi = \"10.21437/eurospeech.1997-623\",\n    url = \"https://doi.org/10.21437/eurospeech.1997-623\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/eurospeech.1997-623",
    "doc_id": "3645c4213daf1954",
    "formatted_citation": "A. Potamianos, Shrikanth S. Narayanan, and Sungbok Lee. Automatic speech recognition for children. 5th European Conference on Speech Communication and Technology (Eurospeech 1997), pages 2371-2374, Sep 1997. URL: https://doi.org/10.21437/eurospeech.1997-623, doi:10.21437/eurospeech.1997-623. This article has 124 citations."
  },
  "Improving Speech Recognition for Children using Acoustic Adaptation and Pronunciation Modeling": {
    "title": "Learning from past mistakes: improving automatic speech recognition output via noisy-clean phrase context modeling",
    "authors": [
      "Prashanth Gurunath Shivakumar",
      "Haoqi Li",
      "Kevin Knight",
      "Panayiotis Georgiou"
    ],
    "doi": "10.1017/atsip.2018.31",
    "url": "https://doi.org/10.1017/atsip.2018.31",
    "journal": "APSIPA Transactions on Signal and Information Processing",
    "year": 2019,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": "1",
    "volume": "8",
    "publisher": "Now Publishers",
    "abstract": "Developing a robust Automatic Speech Recognition (ASR) system for children is a challenging task because of increased variability in acoustic and linguistic correlates as function of young age. The acoustic variability is mainly due to the developmental changes associated with vocal tract growth. On the linguistic side, the variability is associated with limited knowledge of vocabulary, pronunciations and other linguistic constructs. This paper presents a preliminary study towards better acoustic modeling, pronunciation modeling and front-end processing for children’s speech. Results are presented as a function of age. Speaker adaptation signiﬁcantly reduces mismatch and variability improving recognition results across age groups. In addition, introduction of pronunciation modeling shows promising performance improvements.",
    "publication_date": "2019-01-01T00:00:00",
    "citation_count": 27,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 1,
    "arxiv_id": null,
    "docname": "shivakumar2019learningfrompast",
    "dockey": "63bb064ad0fb82af",
    "citation": "Prashanth Gurunath Shivakumar, Haoqi Li, Kevin Knight, and Panayiotis Georgiou. Learning from past mistakes: improving automatic speech recognition output via noisy-clean phrase context modeling. APSIPA Transactions on Signal and Information Processing, Jan 2019. URL: https://doi.org/10.1017/atsip.2018.31, doi:10.1017/atsip.2018.31.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shivakumar2019learningfrompast",
    "bibtex": "@article{shivakumar2019learningfrompast,\n    author = \"Shivakumar, Prashanth Gurunath and Li, Haoqi and Knight, Kevin and Georgiou, Panayiotis\",\n    title = \"Learning from past mistakes: improving automatic speech recognition output via noisy-clean phrase context modeling\",\n    year = \"2019\",\n    journal = \"APSIPA Transactions on Signal and Information Processing\",\n    volume = \"8\",\n    month = \"Jan\",\n    doi = \"10.1017/atsip.2018.31\",\n    url = \"https://doi.org/10.1017/atsip.2018.31\",\n    publisher = \"Now Publishers\",\n    issue = \"1\",\n    issn = \"2048-7703\"\n}\n",
    "issn": "2048-7703",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1017/atsip.2018.31",
    "doc_id": "63bb064ad0fb82af",
    "formatted_citation": "Prashanth Gurunath Shivakumar, Haoqi Li, Kevin Knight, and Panayiotis Georgiou. Learning from past mistakes: improving automatic speech recognition output via noisy-clean phrase context modeling. APSIPA Transactions on Signal and Information Processing, Jan 2019. URL: https://doi.org/10.1017/atsip.2018.31, doi:10.1017/atsip.2018.31. This article has 27 citations and is from a peer-reviewed journal."
  },
  "Robust recognition of children's speech": {
    "title": "Robust recognition of children's speech",
    "authors": [
      "A. Potamianos",
      "Shrikanth S. Narayanan"
    ],
    "doi": "10.1109/tsa.2003.818026",
    "url": "http://ieeexplore.ieee.org/document/1255448/",
    "journal": "IEEE Transactions on Speech and Audio Processing",
    "year": 2003,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "603-616",
    "issue": "6",
    "volume": "11",
    "publisher": "Institute of Electrical and Electronics Engineers (IEEE)",
    "abstract": "Developmental changes in speech production introduce age-dependent spectral and temporal variability in the speech signal produced by children. Such variabilities pose challenges for robust automatic recognition of children’s speech. Through an analysis of age-related acoustic characteristics of children’s speech in the context of automatic speech recognition (ASR), effects such as frequency scaling of spectral envelope parameters are demonstrated. Recognition experiments using acoustic models trained from adult speech and tested against speech from children of various ages clearly show performance degradation with decreasing age. On average, the word error rates are two to five times worse for children speech than for adult speech. Various techniques for improving ASR performance on children’s speech are reported. A speaker normalization algorithm that combines frequency warping and model transformation is shown to reduce acoustic variability and significantly improve ASR performance for children speakers (by 25–45% under various model training and testing conditions). The use of age-dependent acoustic models further reduces word error rate by 10%. The potential of using piece-wise linear and phoneme-dependent frequency warping algorithms for reducing the variability in the acoustic feature space of children is also investigated.",
    "publication_date": "2003-11-01T00:00:00",
    "citation_count": 224,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "potamianos2003robustrecognitionof",
    "dockey": "1818b56cb19313b0",
    "citation": "A. Potamianos and Shrikanth S. Narayanan. Robust recognition of children's speech. IEEE Transactions on Speech and Audio Processing, 11:603-616, Nov 2003. URL: https://doi.org/10.1109/tsa.2003.818026, doi:10.1109/tsa.2003.818026.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "potamianos2003robustrecognitionof",
    "bibtex": "@article{potamianos2003robustrecognitionof,\n    author = \"Potamianos, A. and Narayanan, Shrikanth S.\",\n    title = \"Robust recognition of children's speech\",\n    year = \"2003\",\n    journal = \"IEEE Transactions on Speech and Audio Processing\",\n    volume = \"11\",\n    pages = \"603-616\",\n    month = \"Nov\",\n    doi = \"10.1109/tsa.2003.818026\",\n    url = \"https://doi.org/10.1109/tsa.2003.818026\",\n    publisher = \"Institute of Electrical and Electronics Engineers (IEEE)\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1109/tsa.2003.818026",
    "doc_id": "1818b56cb19313b0",
    "formatted_citation": "A. Potamianos and Shrikanth S. Narayanan. Robust recognition of children's speech. IEEE Transactions on Speech and Audio Processing, 11:603-616, Nov 2003. URL: https://doi.org/10.1109/tsa.2003.818026, doi:10.1109/tsa.2003.818026. This article has 224 citations."
  },
  "Personalizing ASR for Dysarthric and Accented Speech with Limited Data": {
    "title": "Personalizing ASR for Dysarthric and Accented Speech with Limited Data",
    "authors": [
      "Joel Shor",
      "Dotan Emanuel",
      "Oran Lang",
      "Omry Tuval",
      "Michael P. Brenner",
      "Julie Cattiau",
      "Fernando Vieira",
      "Maeve McNally",
      "Taylor Charbonneau",
      "Melissa Nollstadt",
      "A. Hassidim",
      "Yossi Matias"
    ],
    "doi": "10.48550/arxiv.1907.13511",
    "url": "https://www.isca-archive.org/interspeech_2019/shor19_interspeech.html",
    "journal": "ArXiv",
    "year": 2019,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "784-788",
    "issue": null,
    "volume": "abs/1907.13511",
    "publisher": "ISCA",
    "abstract": "Automatic speech recognition (ASR) systems have dramatically improved over the last few years. ASR systems are most often trained from ‘typical’ speech, which means that underrepresented groups don’t experience the same level of improvement. In this paper, we present and evaluate ﬁnetuning techniques to improve ASR for users with non-standard speech. We focus on two types of non-standard speech: speech from people with amyotrophic lateral sclerosis (ALS) and accented speech. We train personalized models that achieve 62% and 35% relative WER improvement on these two groups, bringing the absolute WER for ALS speakers, on a test set of message bank phrases, down to 10% for mild dysarthria and 20% for more serious dysarthria. We show that 71% of the improvement comes from only 5 minutes of training data. Finetuning a particular subset of layers (with many fewer parameters) often gives better results than ﬁnetuning the entire model. This is the ﬁrst step towards building state of the art ASR models for dysarthric speech.",
    "publication_date": null,
    "citation_count": 105,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "shor2019personalizingasrfor",
    "dockey": "d114d526f03de147",
    "citation": "Joel Shor, Dotan Emanuel, Oran Lang, Omry Tuval, Michael P. Brenner, Julie Cattiau, Fernando Vieira, Maeve McNally, Taylor Charbonneau, Melissa Nollstadt, A. Hassidim, and Yossi Matias. Personalizing asr for dysarthric and accented speech with limited data. ArXiv, 2019. URL: https://doi.org/10.48550/arxiv.1907.13511, doi:10.48550/arxiv.1907.13511.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "shor2019personalizingasrfor",
    "bibtex": "@article{shor2019personalizingasrfor,\n    author = \"Shor, Joel and Emanuel, Dotan and Lang, Oran and Tuval, Omry and Brenner, Michael P. and Cattiau, Julie and Vieira, Fernando and McNally, Maeve and Charbonneau, Taylor and Nollstadt, Melissa and Hassidim, A. and Matias, Yossi\",\n    title = \"Personalizing ASR for Dysarthric and Accented Speech with Limited Data\",\n    year = \"2019\",\n    journal = \"ArXiv\",\n    volume = \"abs/1907.13511\",\n    doi = \"10.48550/arxiv.1907.13511\",\n    url = \"https://doi.org/10.48550/arxiv.1907.13511\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.1907.13511",
    "doc_id": "d114d526f03de147",
    "formatted_citation": "Joel Shor, Dotan Emanuel, Oran Lang, Omry Tuval, Michael P. Brenner, Julie Cattiau, Fernando Vieira, Maeve McNally, Taylor Charbonneau, Melissa Nollstadt, A. Hassidim, and Yossi Matias. Personalizing asr for dysarthric and accented speech with limited data. ArXiv, 2019. URL: https://doi.org/10.48550/arxiv.1907.13511, doi:10.48550/arxiv.1907.13511. This article has 105 citations."
  },
  "Data augmentation for children ASR and child-adult speaker classification using voice conversion methods": {
    "title": "Data augmentation for children ASR and child-adult speaker classification using voice conversion methods",
    "authors": [
      "Shuyang Zhao",
      "Mittul Singh",
      "Abraham Woubie",
      "Reima Karhila"
    ],
    "doi": "10.21437/interspeech.2023-702",
    "url": "https://www.isca-archive.org/interspeech_2023/zhao23c_interspeech.html",
    "journal": "INTERSPEECH 2023",
    "year": 2023,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "4593-4597",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Many young children prefer speech based interfaces over text, as they are relatively slow and error-prone with text input. However, children ASR can be challenging due to the lack of transcribed children speech corpora. In this paper, we investigate a voice conversion method based on WORLD vocoder to generate childlike speech for data augmentation. Since noise may lead to severe artifacts in converted speech, we also investigate using speech enhancement to improve the quality of converted speech. On a publicly available children speech corpus, we evaluated the performance of the proposed data augmentation method against existing data augmentation methods based on linear prediction coefficients. Our proposed data augmentation method substantially outperformed the prior work on children ASR. Additionally, on a task to classify the speaker, adult or child, data generated using our proposed method was shown to mimic real children better compared to the reference methods.",
    "publication_date": "2023-08-14T00:00:00",
    "citation_count": 5,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "zhao2023dataaugmentationfor",
    "dockey": "64c4328ddaf0b54d",
    "citation": "Shuyang Zhao, Mittul Singh, Abraham Woubie, and Reima Karhila. Data augmentation for children asr and child-adult speaker classification using voice conversion methods. INTERSPEECH 2023, pages 4593-4597, Aug 2023. URL: https://doi.org/10.21437/interspeech.2023-702, doi:10.21437/interspeech.2023-702.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "zhao2023dataaugmentationfor",
    "bibtex": "@article{zhao2023dataaugmentationfor,\n    author = \"Zhao, Shuyang and Singh, Mittul and Woubie, Abraham and Karhila, Reima\",\n    title = \"Data augmentation for children ASR and child-adult speaker classification using voice conversion methods\",\n    year = \"2023\",\n    journal = \"INTERSPEECH 2023\",\n    pages = \"4593-4597\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2023-702\",\n    url = \"https://doi.org/10.21437/interspeech.2023-702\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2023-702",
    "doc_id": "64c4328ddaf0b54d",
    "formatted_citation": "Shuyang Zhao, Mittul Singh, Abraham Woubie, and Reima Karhila. Data augmentation for children asr and child-adult speaker classification using voice conversion methods. INTERSPEECH 2023, pages 4593-4597, Aug 2023. URL: https://doi.org/10.21437/interspeech.2023-702, doi:10.21437/interspeech.2023-702. This article has 5 citations."
  },
  "Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping": {
    "title": "Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping",
    "authors": [
      "Jenthe Thienpondt",
      "Kris Demuynck"
    ],
    "doi": "10.21437/interspeech.2022-10964",
    "url": "https://doi.org/10.21437/interspeech.2022-10964",
    "journal": "Interspeech 2022",
    "year": 2022,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "2213-2217",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Automatic Speech Recognition (ASR) systems are known to exhibit difficulties when transcribing children’s speech. This can mainly be attributed to the absence of large children’s speech corpora to train robust ASR models and the resulting domain mismatch when decoding children’s speech with systems trained on adult data. In this paper, we propose multiple enhancements to alleviate these issues. First, we propose a data augmentation technique based on the source-filter model of speech to close the domain gap between adult and children’s speech. This enables us to leverage the data availability of adult speech corpora by making these samples perceptually similar to children’s speech. Second, using this augmentation strategy, we apply transfer learning on a Transformer model pre-trained on adult data. This model follows the recently introduced XLS-R architecture, a wav2vec 2.0 model pre-trained on several crosslingual adult speech corpora to learn general and robust acoustic frame-level representations. Adopting this model for the ASR task using adult data augmented with the proposed source-filter warping strategy and a limited amount of in-domain children’s speech significantly outperforms previous state-of-the-art results on the PF-STAR British English Children’s Speech corpus with a 4.86% WER on the official test set.",
    "publication_date": "2022-09-18T00:00:00",
    "citation_count": 5,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "thienpondt2022transferlearningfor",
    "dockey": "9c883c5b04730dce",
    "citation": "Jenthe Thienpondt and Kris Demuynck. Transfer learning for robust low-resource children's speech asr with transformers and source-filter warping. Interspeech 2022, pages 2213-2217, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-10964, doi:10.21437/interspeech.2022-10964.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "thienpondt2022transferlearningfor",
    "bibtex": "@article{thienpondt2022transferlearningfor,\n    author = \"Thienpondt, Jenthe and Demuynck, Kris\",\n    title = \"Transfer Learning for Robust Low-Resource Children's Speech ASR with Transformers and Source-Filter Warping\",\n    year = \"2022\",\n    journal = \"Interspeech 2022\",\n    pages = \"2213-2217\",\n    month = \"Sep\",\n    doi = \"10.21437/interspeech.2022-10964\",\n    url = \"https://doi.org/10.21437/interspeech.2022-10964\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "inproceedings",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2022-10964",
    "doc_id": "9c883c5b04730dce",
    "formatted_citation": "Jenthe Thienpondt and Kris Demuynck. Transfer learning for robust low-resource children's speech asr with transformers and source-filter warping. Interspeech 2022, pages 2213-2217, Sep 2022. URL: https://doi.org/10.21437/interspeech.2022-10964, doi:10.21437/interspeech.2022-10964. This article has 5 citations."
  },
  "ChildAugment: Data Augmentation Methods for Zero-Resource Children's Speaker Verification": {
    "title": "ChildAugment: Data Augmentation Methods for Zero-Resource Children's Speaker Verification",
    "authors": [
      "V. Singh",
      "Md. Sahidullah",
      "T. Kinnunen"
    ],
    "doi": "10.48550/arxiv.2402.15214",
    "url": "http://arxiv.org/abs/2402.15214",
    "journal": "The Journal of the Acoustical Society of America",
    "year": 2024,
    "genre": "preprint",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [
      "Computer Science - Sound",
      "Electrical Engineering and Systems Science - Audio and Speech Processing"
    ],
    "pages": "\n2221-2232\n",
    "issue": null,
    "volume": "155 3",
    "publisher": "arXiv",
    "abstract": "The accuracy of modern automatic speaker verification (ASV) systems, when trained exclusively on adult data, drops substantially when applied to children's speech. The scarcity of children's speech corpora hinders fine-tuning ASV systems for children's speech. Hence, there is a timely need to explore more effective ways of reusing adults' speech data. One promising approach is to align vocal-tract parameters between adults and children through children-specific data augmentation, referred here to as ChildAugment. Specifically, we modify the formant frequencies and formant bandwidths of adult speech to emulate children's speech. The modified spectra are used to train ECAPA-TDNN (emphasized channel attention, propagation, and aggregation in time-delay neural network) recognizer for children. We compare ChildAugment against various state-of-the-art data augmentation techniques for children's ASV. We also extensively compare different scoring methods, including cosine scoring, PLDA (probabilistic linear discriminant analysis), and NPLDA (neural PLDA). We also propose a low-complexity weighted cosine score for extremely low-resource children ASV. Our findings on the CSLU kids corpus indicate that ChildAugment holds promise as a simple, acoustics-motivated approach, for improving state-of-the-art deep learning based ASV for children. We achieve up to 12.45% (boys) and 11.96% (girls) relative improvement over the baseline.",
    "publication_date": null,
    "citation_count": 4,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": "2402.15214",
    "docname": "singh2024childaugmentdataaugmentation",
    "dockey": "957348ebe582c060",
    "citation": "V. Singh, Md. Sahidullah, and T. Kinnunen. Childaugment: data augmentation methods for zero-resource children's speaker verification. The Journal of the Acoustical Society of America, 155 3:2221-2232, 2024. URL: https://doi.org/10.48550/arxiv.2402.15214, doi:10.48550/arxiv.2402.15214.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "singh2024childaugmentdataaugmentation",
    "bibtex": "@article{singh2024childaugmentdataaugmentation,\n    author = \"Singh, V. and Sahidullah, Md. and Kinnunen, T.\",\n    title = \"ChildAugment: Data Augmentation Methods for Zero-Resource Children's Speaker Verification\",\n    year = \"2024\",\n    journal = \"The Journal of the Acoustical Society of America\",\n    volume = \"155 3\",\n    pages = \"\n2221-2232\n\",\n    doi = \"10.48550/arxiv.2402.15214\",\n    url = \"https://doi.org/10.48550/arxiv.2402.15214\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.48550/arxiv.2402.15214",
    "doc_id": "957348ebe582c060",
    "formatted_citation": "V. Singh, Md. Sahidullah, and T. Kinnunen. Childaugment: data augmentation methods for zero-resource children's speaker verification. The Journal of the Acoustical Society of America, 155 3:2221-2232, 2024. URL: https://doi.org/10.48550/arxiv.2402.15214, doi:10.48550/arxiv.2402.15214. This article has 4 citations."
  },
  "Fundamental frequency feature warping for frequency normalization and data augmentation in child automatic speech recognition": {
    "title": "Fundamental frequency feature warping for frequency normalization and data augmentation in child automatic speech recognition",
    "authors": [
      "Gary Yeung",
      "Ruchao Fan",
      "Abeer Alwan"
    ],
    "doi": "10.1016/j.specom.2021.08.002",
    "url": "https://doi.org/10.1016/j.specom.2021.08.002",
    "journal": "Speech Communication",
    "year": 2021,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1-10",
    "issue": null,
    "volume": "135",
    "publisher": "Elsevier BV",
    "abstract": "Effective child automatic speech recognition (ASR) systems have become increasingly important due to the growing use of interactive technology. Due to the lack of publicly available child speech databases, young child ASR systems often rely on older child or adult speech for training data. However, there is a large acoustic mismatch between child and adult speech. This study proposes a novel fundamental frequency (𝑓𝑜)based frequency warping technique for both frequency normalization and data augmentation to combat this acoustic mismatch and address the lack of available child speech training data. The technique is inspired by the tonotopic distances between formants and 𝑓𝑜, developed to model human vowel perception. The tonotopic distances are reformulated as a linear relationship between 𝑓𝑜 and vowel formants on the Mel scale. This reformulation is verified using 𝑓𝑜 and formant measurements from child utterances. The relationship is further generalized such that the frequency warping technique only relies on two parameters. The LibriSpeech ASR corpus is used for training, and both the OGI Kids’ Speech and CMU Kids Corpora are used for both training and testing. A single word ASR experiment and a continuous read speech ASR experiment are performed to evaluate the 𝑓𝑜-based frequency normalization and data augmentation techniques. In the single word experiment, the system using 𝑓𝑜-based frequency normalization significantly improved over the baseline system with no normalization, with a relative improvement of up to 22.3%, when the mismatch between training and testing data was large. In the continuous speech experiment, the combination of 𝑓𝑜-based frequency normalization and data augmentation resulted in a relative improvement of 19.3% over the baseline. Additionally, in all experiments, the 𝑓𝑜-based techniques outperformed other techniques such as vocal tract length normalization (VTLN) or vocal tract length perturbation (VTLP). Results were validated using Gaussian mixture model (GMM), deep neural network (DNN), and bidirectional long–short term memory (BLSTM) acoustic models.",
    "publication_date": "2021-12-01T00:00:00",
    "citation_count": 9,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": 2,
    "arxiv_id": null,
    "docname": "yeung2021fundamentalfrequencyfeature",
    "dockey": "6b0735ce60a31b42",
    "citation": "Gary Yeung, Ruchao Fan, and Abeer Alwan. Fundamental frequency feature warping for frequency normalization and data augmentation in child automatic speech recognition. Speech Communication, 135:1-10, Dec 2021. URL: https://doi.org/10.1016/j.specom.2021.08.002, doi:10.1016/j.specom.2021.08.002.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "yeung2021fundamentalfrequencyfeature",
    "bibtex": "@article{yeung2021fundamentalfrequencyfeature,\n    author = \"Yeung, Gary and Fan, Ruchao and Alwan, Abeer\",\n    title = \"Fundamental frequency feature warping for frequency normalization and data augmentation in child automatic speech recognition\",\n    year = \"2021\",\n    journal = \"Speech Communication\",\n    volume = \"135\",\n    pages = \"1-10\",\n    month = \"Dec\",\n    doi = \"10.1016/j.specom.2021.08.002\",\n    url = \"https://doi.org/10.1016/j.specom.2021.08.002\",\n    publisher = \"Elsevier BV\",\n    issn = \"0167-6393\"\n}\n",
    "issn": "0167-6393",
    "bibtex_type": "misc",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.1016/j.specom.2021.08.002",
    "doc_id": "6b0735ce60a31b42",
    "formatted_citation": "Gary Yeung, Ruchao Fan, and Abeer Alwan. Fundamental frequency feature warping for frequency normalization and data augmentation in child automatic speech recognition. Speech Communication, 135:1-10, Dec 2021. URL: https://doi.org/10.1016/j.specom.2021.08.002, doi:10.1016/j.specom.2021.08.002. This article has 9 citations and is from a domain leading peer-reviewed journal."
  },
  "On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children": {
    "title": "On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children",
    "authors": [
      "Gary Yeung",
      "A. Alwan"
    ],
    "doi": "10.21437/interspeech.2018-2297",
    "url": "https://www.isca-archive.org/interspeech_2018/yeung18_interspeech.html",
    "journal": "Interspeech 2018",
    "year": 2018,
    "genre": "conferencePaper",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": "1661-1665",
    "issue": null,
    "volume": null,
    "publisher": "ISCA",
    "abstract": "Automatic speech recognition (ASR) systems for children have lagged behind in performance when compared to adult ASR. The exact problems and evaluation methods for child ASR have not yet been fully investigated. Recent work from the robotics community suggests that ASR for kindergarten speech is especially difﬁcult, even though this age group may beneﬁt most from voice-based educational and diagnostic tools. Our study focused on ASR performance for speciﬁc grade levels (K-10) using a word identiﬁcation task. Grade-speciﬁc ASR systems were evaluated, with particular attention placed on the evaluation of kindergarten-aged children (5-6 years old). Experiments included investigation of grade-speciﬁc interactions with triphone models using feature space maximum likelihood linear regression (fMLLR), vocal tract length normalization (VTLN), and subglottal resonance (SGR) normalization. Our results indicate that kindergarten ASR performs dramatically worse than even 1st grade ASR, likely due to large speech variability at that age. As such, ASR systems may require targeted evaluations on kindergarten speech rather than being evaluated under the guise of “child ASR.” Additionally, results show that systems trained in matched conditions on kindergarten speech may be less suitable than mismatched-grade training with 1st grade speech. Finally, we analyzed the phonetic errors made by the kindergarten ASR.",
    "publication_date": "2018-08-28T00:00:00",
    "citation_count": 57,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": -1,
    "arxiv_id": null,
    "docname": "yeung2018onthedifficulties",
    "dockey": "7fde6d0e3d1f6a07",
    "citation": "Gary Yeung and A. Alwan. On the difficulties of automatic speech recognition for kindergarten-aged children. Interspeech 2018, pages 1661-1665, Aug 2018. URL: https://doi.org/10.21437/interspeech.2018-2297, doi:10.21437/interspeech.2018-2297.",
    "fields_to_overwrite_from_metadata": [
      "doc_id",
      "key",
      "citation",
      "docname",
      "dockey"
    ],
    "key": "yeung2018onthedifficulties",
    "bibtex": "@article{yeung2018onthedifficulties,\n    author = \"Yeung, Gary and Alwan, A.\",\n    title = \"On the Difficulties of Automatic Speech Recognition for Kindergarten-Aged Children\",\n    year = \"2018\",\n    journal = \"Interspeech 2018\",\n    pages = \"1661-1665\",\n    month = \"Aug\",\n    doi = \"10.21437/interspeech.2018-2297\",\n    url = \"https://doi.org/10.21437/interspeech.2018-2297\",\n    publisher = \"ISCA\"\n}\n",
    "bibtex_type": "article",
    "is_retracted": false,
    "doi_url": "https://doi.org/10.21437/interspeech.2018-2297",
    "doc_id": "7fde6d0e3d1f6a07",
    "formatted_citation": "Gary Yeung and A. Alwan. On the difficulties of automatic speech recognition for kindergarten-aged children. Interspeech 2018, pages 1661-1665, Aug 2018. URL: https://doi.org/10.21437/interspeech.2018-2297, doi:10.21437/interspeech.2018-2297. This article has 57 citations."
  },
  "Reducing Bias in State-of-the-Art ASR Systems for Child Speech": {
    "title": "Reducing Bias in State-of-the-Art ASR Systems for Child Speech",
    "authors": [
      "Zeisler, F. A."
    ],
    "doi": null,
    "url": "https://repository.tudelft.nl/record/uuid:86f166d9-e13f-4084-a5bf-7a7632604b52",
    "journal": null,
    "year": 2024,
    "genre": "journalArticle",
    "link_attachments": [],
    "manual_tags": [],
    "automatic_tags": [],
    "pages": null,
    "issue": null,
    "volume": null,
    "publisher": null,
    "abstract": "Automatic Speech Recognition (ASR) systems have transformed human-machine interaction, yet they often struggle with child speech due to the unique vocal characteristics. This thesis investigates age and gender biases, focusing on enhancing the performance of state-of-the-art ASR model Whisper on child speech. Initial experiments reveal significant disparities in recognition accuracy across age groups and genders within child speech, highlighting the critical need for targeted improvements. The study uses Low-Rank Adaptation (LoRA) to finetune the model using four child-specific datasets, aiming to simultaneously enhance recognition performance and mitigate biases. Results demonstrate substantial reductions in Word Error Rates (WER) and biases after finetuning, showcasing the effectiveness of transfer learning in addressing demographic inequality. Gender biases decreased by 32.77% relative to their initial values, and age biases also improved, with a relative decrease of 27.52% after finetuning. This research showcases the potential of tailored approaches to advance ASR technology for low-resource user demographics, with implications for improving educational and assistive technologies.<br/><br/><b>Index Terms</b>: Automatic Speech Recognition, Child speech, Whisper ASR model, Age and gender biases, Low-Rank Adaptation, Transfer learning, Demographic disparities",
    "publication_date": null,
    "citation_count": null,
    "reference_count": null,
    "is_open_access": null,
    "pdf_url": null,
    "container_title": null,
    "fields_of_study": null,
    "crossref_subjects": null,
    "subject": null,
    "bibtex_source": [],
    "source_quality": null,
    "arxiv_id": null
  }
}